{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETHotZQEjXda",
        "outputId": "237c69a1-ec27-4e13-aa9f-43980e3dc4b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStop\n",
        "\n",
        "class CRC:\n",
        "    def __init__(self, polynomial=0b10011011, order=7):\n",
        "        \"\"\"\n",
        "        CRC-7 Polynomial Implementation\n",
        "\n",
        "        Args:\n",
        "        - polynomial: CRC generator polynomial (default: 0b10011011)\n",
        "        - order: CRC polynomial order (default: 7)\n",
        "        \"\"\"\n",
        "        self.polynomial = polynomial\n",
        "        self.order = order\n",
        "\n",
        "    def generate_crc(self, data):\n",
        "        \"\"\"\n",
        "        Generate CRC checksum\n",
        "\n",
        "        Args:\n",
        "        - data: Input data bits\n",
        "\n",
        "        Returns:\n",
        "        - CRC checksum bits\n",
        "        \"\"\"\n",
        "        # Extend data with zeros for CRC calculation\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.order, dtype=int)])\n",
        "\n",
        "        # CRC calculation\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    data_with_zeros[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        # Return CRC bits\n",
        "        return data_with_zeros[-self.order:]\n",
        "\n",
        "    def verify_crc(self, data, received_crc):\n",
        "        \"\"\"\n",
        "        Verify CRC checksum\n",
        "\n",
        "        Args:\n",
        "        - data: Original data bits\n",
        "        - received_crc: Received CRC bits\n",
        "\n",
        "        Returns:\n",
        "        - Boolean indicating CRC validity\n",
        "        \"\"\"\n",
        "        # Combine data and received CRC\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "\n",
        "        # CRC verification\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    full_data[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        # Check if remainder is zero\n",
        "        return np.all(full_data[-self.order:] == 0)\n",
        "\n",
        "    def example_usage(self):\n",
        "        \"\"\"\n",
        "        Demonstration of CRC usage\n",
        "        \"\"\"\n",
        "        # Example data\n",
        "        data = np.random.randint(2, size=64)\n",
        "\n",
        "        # Generate CRC\n",
        "        crc_bits = self.generate_crc(data)\n",
        "\n",
        "        # Verify CRC\n",
        "        is_valid = self.verify_crc(data, crc_bits)\n",
        "\n",
        "        print(\"Original Data:\", data)\n",
        "        print(\"CRC Bits:\", crc_bits)\n",
        "        print(\"CRC Verification:\", \"Valid\" if is_valid else \"Invalid\")\n",
        "\n",
        "        # Simulate error injection\n",
        "        corrupted_crc = crc_bits.copy()\n",
        "        corrupted_crc[0] ^= 1  # Flip a bit\n",
        "\n",
        "        is_corrupted_valid = self.verify_crc(data, corrupted_crc)\n",
        "        print(\"Corrupted CRC Verification:\", \"Valid\" if is_corrupted_valid else \"Invalid\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    crc = CRC()\n",
        "    crc.example_usage()\n",
        "    import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class PolarCodeDataGenerator:\n",
        "    def __init__(self, N=128, K=64):\n",
        "        self.N = N  # Block length\n",
        "        self.K = K  # Information bits\n",
        "\n",
        "    def generate_synthetic_dataset(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate synthetic polar code dataset\n",
        "\n",
        "        Returns:\n",
        "        - Input features\n",
        "        - Information bits\n",
        "        \"\"\"\n",
        "        # Generate input features\n",
        "        incidence_angle = np.random.uniform(0, 180, num_samples)\n",
        "        phase_shift = np.random.uniform(0, 2*np.pi, num_samples)\n",
        "        snr = np.random.exponential(10, num_samples)\n",
        "        num_ris_elements = np.random.randint(16, 256, num_samples)\n",
        "        threshold_rate = np.random.uniform(1, 10, num_samples)\n",
        "\n",
        "        # Calculate synthetic outage probability\n",
        "        def calculate_outage_probability(angle, phase, snr_val, ris_elements, rate_th):\n",
        "            base_prob = 0.1\n",
        "            angle_factor = angle / 180\n",
        "            phase_factor = np.sin(phase)\n",
        "            ris_factor = ris_elements / 256\n",
        "            snr_factor = 1 / (1 + np.exp(-snr_val / 10))\n",
        "            rate_factor = 1 / (1 + np.exp(-rate_th))\n",
        "\n",
        "            outage_prob = base_prob * (1 - angle_factor) * (1 + phase_factor) * (1 / ris_factor) * snr_factor * rate_factor\n",
        "            return np.clip(outage_prob, 0, 1)\n",
        "\n",
        "        # Calculate outage probability\n",
        "        outage_probability = np.array([\n",
        "            calculate_outage_probability(angle, phase, snr_val, ris_elem, rate_th)\n",
        "            for angle, phase, snr_val, ris_elem, rate_th\n",
        "            in zip(incidence_angle, phase_shift, snr, num_ris_elements, threshold_rate)\n",
        "        ])\n",
        "\n",
        "        return {\n",
        "            'features': np.column_stack([\n",
        "                incidence_angle,\n",
        "                phase_shift,\n",
        "                snr,\n",
        "                num_ris_elements,\n",
        "                threshold_rate\n",
        "            ]),\n",
        "            'outage_probability': outage_probability\n",
        "        }\n",
        "\n",
        "        class FeatureEngineer:\n",
        "    @staticmethod\n",
        "    def engineer_features(dataset):\n",
        "        \"\"\"\n",
        "        Advanced feature engineering\n",
        "\n",
        "        Args:\n",
        "        - dataset: Input dataset dictionary\n",
        "\n",
        "        Returns:\n",
        "        - Engineered features\n",
        "        \"\"\"\n",
        "        features = dataset['features']\n",
        "\n",
        "        # Extract individual features\n",
        "        incidence_angle = features[:, 0]\n",
        "        phase_shift = features[:, 1]\n",
        "        snr = features[:, 2]\n",
        "        num_ris_elements = features[:, 3]\n",
        "        threshold_rate = features[:, 4]\n",
        "\n",
        "        # Advanced feature creation\n",
        "        engineered_features = {\n",
        "            # Original features\n",
        "            'incidence_angle': incidence_angle,\n",
        "            'phase_shift': phase_shift,\n",
        "            'snr': snr,\n",
        "            'num_ris_elements': num_ris_elements,\n",
        "            'threshold_rate': threshold_rate,\n",
        "\n",
        "            # Derived statistical features\n",
        "            'angle_entropy': np.array([stats.entropy(incidence_angle)]*len(incidence_angle)),\n",
        "            'phase_variance': np.array([np.var(phase_shift)]*len(phase_shift)),\n",
        "            'snr_kurtosis': np.array([stats.kurtosis(snr)]*len(snr)),\n",
        "\n",
        "            # Interaction features\n",
        "            'angle_snr_interaction': incidence_angle * snr,\n",
        "            'phase_ris_interaction': phase_shift * num_ris_elements,\n",
        "\n",
        "            # Non-linear transformations\n",
        "            'log_snr': np.log1p(snr),\n",
        "            'sin_phase': np.sin(phase_shift),\n",
        "            'cos_angle': np.cos(np.deg2rad(incidence_angle)),\n",
        "\n",
        "            # Polynomial features\n",
        "            'snr_squared': snr**2,\n",
        "            'ris_elements_normalized': num_ris_elements / np.max(num_ris_elements)\n",
        "        }\n",
        "\n",
        "        return np.column_stack(list(engineered_features.values()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Dkl1dXJKjjb1",
        "outputId": "f44de627-5c3f-4d52-ad3f-0843e8149148"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after class definition on line 154 (<ipython-input-54-4fdbc032223d>, line 155)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-4fdbc032223d>\"\u001b[0;36m, line \u001b[0;32m155\u001b[0m\n\u001b[0;31m    @staticmethod\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j0oy-f7h6x6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import sys\n",
        "# Increase recursion limit\n",
        "sys.setrecursionlimit(3000)  # Adjust as needed\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt  # Add this import\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Configure logging at the start of your script\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s'\n",
        ")\n",
        "\n",
        "\n",
        "        # ... rest of the initialization\n",
        "\n",
        "\n",
        "class CRC:\n",
        "    def __init__(self, polynomial=0b10011011, order=7):\n",
        "        self.polynomial = polynomial\n",
        "        self.order = order\n",
        "\n",
        "    def generate_crc(self, data):\n",
        "        \"\"\"\n",
        "        Generate CRC checksum\n",
        "\n",
        "        Args:\n",
        "        - data: Input data bits\n",
        "\n",
        "        Returns:\n",
        "        - CRC checksum bits\n",
        "        \"\"\"\n",
        "        # Extend data with zeros for CRC calculation\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.order, dtype=int)])\n",
        "\n",
        "        # CRC calculation\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    data_with_zeros[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        # Return CRC bits\n",
        "        return data_with_zeros[-self.order:]\n",
        "\n",
        "    def verify_crc(self, data, received_crc):\n",
        "        \"\"\"\n",
        "        Verify CRC checksum\n",
        "\n",
        "        Args:\n",
        "        - data: Original data bits\n",
        "        - received_crc: Received CRC bits\n",
        "\n",
        "        Returns:\n",
        "        - Boolean indicating CRC validity\n",
        "        \"\"\"\n",
        "        # Combine data and received CRC\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "\n",
        "        # CRC verification\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    full_data[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        # Check if remainder is zero\n",
        "        return np.all(full_data[-self.order:] == 0)\n",
        "\n",
        "\n",
        "class PolarCode:\n",
        "    def __init__(self, block_length, info_length, design_snr=0):\n",
        "        \"\"\"\n",
        "        Polar Code Constructor\n",
        "\n",
        "        Args:\n",
        "            block_length (int): Total code block length\n",
        "            info_length (int): Number of information bits\n",
        "            design_snr (float): Design Signal-to-Noise Ratio for channel polarization\n",
        "        \"\"\"\n",
        "        # Validate inputs\n",
        "        if block_length <= 0 or info_length <= 0:\n",
        "            raise ValueError(\"Block length and info length must be positive\")\n",
        "\n",
        "        if block_length < info_length:\n",
        "            raise ValueError(\"Block length must be greater than or equal to info length\")\n",
        "\n",
        "        self.block_length = block_length\n",
        "        self.info_length = info_length\n",
        "        self.design_snr = design_snr\n",
        "\n",
        "        # Channel polarization\n",
        "        self.channel_capacities = self._compute_channel_capacities()\n",
        "        self.info_bit_positions = self._select_info_bit_positions()\n",
        "\n",
        "    def _compute_channel_capacities(self):\n",
        "        \"\"\"\n",
        "        Compute channel capacities using Bhattacharyya parameter approximation\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Channel capacities for each bit position\n",
        "        \"\"\"\n",
        "        # Bhattacharyya parameter approximation\n",
        "        def bhattacharyya_parameter(design_snr):\n",
        "            return np.exp(-((10 ** (design_snr / 10)) / 2))\n",
        "\n",
        "        # Initialize channel capacities\n",
        "        capacities = np.zeros(self.block_length)\n",
        "\n",
        "        # Recursive polarization transformation\n",
        "        z = bhattacharyya_parameter(self.design_snr)\n",
        "        capacities[0] = z\n",
        "\n",
        "        for i in range(1, self.block_length):\n",
        "            j = i // 2\n",
        "            capacities[i] = capacities[j] ** 2 if i % 2 == 0 else 1 - (1 - capacities[j] ** 2) ** 2\n",
        "\n",
        "        return capacities\n",
        "\n",
        "    def _select_info_bit_positions(self):\n",
        "        \"\"\"\n",
        "        Select best bit positions for information transmission\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Indices of selected information bit positions\n",
        "        \"\"\"\n",
        "        # Sort channel capacities and select best positions\n",
        "        sorted_indices = np.argsort(self.channel_capacities)\n",
        "        info_positions = sorted_indices[-self.info_length:]\n",
        "\n",
        "        return np.sort(info_positions)\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Polar Code Encoder\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits to be encoded\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Validate input\n",
        "        if len(info_bits) != self.info_length:\n",
        "            raise ValueError(f\"Expected {self.info_length} info bits, got {len(info_bits)}\")\n",
        "\n",
        "        # Initialize codeword\n",
        "        codeword = np.zeros(self.block_length, dtype=int)\n",
        "\n",
        "        # Place information bits in selected positions\n",
        "        codeword[self.info_bit_positions] = info_bits\n",
        "\n",
        "        # Recursive encoding\n",
        "        for i in range(int(np.log2(self.block_length))):\n",
        "            step = 2 ** i\n",
        "            for j in range(0, self.block_length, 2 * step):\n",
        "                for k in range(j, j + step):\n",
        "                    codeword[k + step] ^= codeword[k]\n",
        "\n",
        "        return codeword\n",
        "\n",
        "    def decode(self, received_signal, decoding_method='SC'):\n",
        "        \"\"\"\n",
        "        Polar Code Decoder\n",
        "\n",
        "        Args:\n",
        "            received_signal (np.ndarray): Received noisy signal\n",
        "            decoding_method (str): Decoding algorithm (SC, SCL)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Decoded information bits\n",
        "        \"\"\"\n",
        "        if decoding_method == 'SC':\n",
        "            return self._successive_cancellation_decode(received_signal)\n",
        "        elif decoding_method == 'SCL':\n",
        "            return self._successive_cancellation_list_decode(received_signal)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported decoding method: {decoding_method}\")\n",
        "\n",
        "    def _successive_cancellation_decode(self, received_signal):\n",
        "        \"\"\"\n",
        "        Successive Cancellation (SC) Decoding\n",
        "\n",
        "        Args:\n",
        "            received_signal (np.ndarray): Received noisy signal\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Decoded information bits\n",
        "        \"\"\"\n",
        "        # Initialize decoding variables\n",
        "        decoded_bits = np.zeros(self.block_length, dtype=int)\n",
        "\n",
        "        # Recursive decoding\n",
        "        for i in range(int(np.log2(self.block_length))):\n",
        "            step = 2 ** i\n",
        "            for j in range(0, self.block_length, 2 * step):\n",
        "                for k in range(j, j + step):\n",
        "                    # Bit decision based on likelihood\n",
        "                    llr = self._compute_log_likelihood_ratio(\n",
        "                        received_signal[k],\n",
        "                        received_signal[k + step],\n",
        "                        decoded_bits[k]\n",
        "                    )\n",
        "                    decoded_bits[k + step] = 1 if llr < 0 else 0\n",
        "\n",
        "        # Extract information bits\n",
        "        return decoded_bits[self.info_bit_positions]\n",
        "\n",
        "    def _successive_cancellation_list_decode(self, received_signal, list_size=8):\n",
        "        \"\"\"\n",
        "        Successive Cancellation List (SCL) Decoding\n",
        "\n",
        "        Args:\n",
        "            received_signal (np.ndarray): Received noisy signal\n",
        "            list_size (int): Number of candidate paths to explore\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Decoded information bits\n",
        "        \"\"\"\n",
        "        # Placeholder for more advanced SCL decoding\n",
        "        # This is a simplified implementation\n",
        "        return self._successive_cancellation_decode(received_signal)\n",
        "\n",
        "    def _compute_log_likelihood_ratio(self, left_signal, right_signal, previous_bit):\n",
        "        \"\"\"\n",
        "        Compute Log-Likelihood Ratio for bit decision\n",
        "\n",
        "        Args:\n",
        "            left_signal (float): Left branch signal\n",
        "            right_signal (float): Right branch signal\n",
        "            previous_bit (int): Previously decoded bit\n",
        "\n",
        "        Returns:\n",
        "            float: Log-Likelihood Ratio\n",
        "        \"\"\"\n",
        "        # Simplified LLR computation\n",
        "        return left_signal - right_signal + previous_bit\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N=128, K=64, crc_order=7):\n",
        "        \"\"\"\n",
        "        Polar Code Generator with CRC\n",
        "\n",
        "        Args:\n",
        "        - N: Block length\n",
        "        - K: Information bits\n",
        "        - crc_order: CRC polynomial order\n",
        "        \"\"\"\n",
        "        self.N = N  # Total block length\n",
        "        self.K = K  # Information bits\n",
        "        self.R = K / N  # Code rate\n",
        "        self.crc = CRC(order=crc_order)\n",
        "\n",
        "        # Channel polarization method\n",
        "        self.info_indices = self._get_info_bit_indices()\n",
        "\n",
        "    def _bhattacharyya_bound(self, W, n):\n",
        "        \"\"\"\n",
        "        Bhattacharyya parameter for channel polarization\n",
        "\n",
        "        Args:\n",
        "        - W: Initial channel parameter\n",
        "        - n: Recursion depth\n",
        "\n",
        "        Returns:\n",
        "        - Transformed channel parameter\n",
        "        \"\"\"\n",
        "        if n == 0:\n",
        "            return W\n",
        "\n",
        "        W_used = self._bhattacharyya_bound(W, n-1)\n",
        "        W_transform = 2 * W_used**2 - W_used**4\n",
        "        return W_transform\n",
        "\n",
        "    def _get_info_bit_indices(self):\n",
        "        \"\"\"\n",
        "        Determine information bit indices using channel polarization\n",
        "\n",
        "        Returns:\n",
        "        - Indices of information bits\n",
        "        \"\"\"\n",
        "        # Total number of bits including CRC\n",
        "        total_bits = self.K + self.crc.order\n",
        "\n",
        "        # Calculate channel capacities\n",
        "        channel_capacities = []\n",
        "        for i in range(self.N):\n",
        "            W = 0.5  # Initial channel parameter\n",
        "            capacity = self._bhattacharyya_bound(W, int(np.log2(self.N)))\n",
        "            channel_capacities.append(capacity)\n",
        "\n",
        "        # Select most reliable bit positions\n",
        "        sorted_indices = np.argsort(channel_capacities)\n",
        "        return sorted_indices[-total_bits:]\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Polar code encoding with CRC\n",
        "\n",
        "        Args:\n",
        "        - info_bits: Information bits to encode\n",
        "\n",
        "        Returns:\n",
        "        - Encoded codeword\n",
        "        \"\"\"\n",
        "        # Ensure correct input size\n",
        "        assert len(info_bits) == self.K, f\"Input must be {self.K} bits\"\n",
        "\n",
        "        # Generate CRC\n",
        "        crc_bits = self.crc.generate_crc(info_bits)\n",
        "\n",
        "        # Combine info and CRC bits\n",
        "        full_bits = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Initialize codeword\n",
        "        x = np.zeros(self.N, dtype=int)\n",
        "        x[self.info_indices] = full_bits\n",
        "\n",
        "        # Recursive encoding\n",
        "        n = int(np.log2(self.N))\n",
        "        for i in range(n):\n",
        "            for j in range(0, self.N, 2**(i+1)):\n",
        "                for k in range(2**i):\n",
        "                    u = x[j+k]\n",
        "                    v = x[j+k+2**i]\n",
        "                    x[j+k] = (u + v) % 2\n",
        "                    x[j+k+2**i] = v\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SCLDecoder:\n",
        "    def __init__(self, N, K, crc_order=7, list_sizes=[1, 4, 8]):\n",
        "        \"\"\"\n",
        "        Successive Cancellation List Decoder with CRC\n",
        "\n",
        "        Args:\n",
        "        - N: Block length\n",
        "        - K: Information bits\n",
        "        - crc_order: CRC polynomial order\n",
        "        - list_sizes: Different list sizes to explore\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc = CRC(order=crc_order)\n",
        "        self.list_sizes = list_sizes\n",
        "        self.crc_order = crc_order\n",
        "\n",
        "        # Add info_indices calculation\n",
        "        self.info_indices = self._get_info_bit_indices()\n",
        "\n",
        "    def _bhattacharyya_bound(self, W, n):\n",
        "        \"\"\"\n",
        "        Bhattacharyya parameter for channel polarization\n",
        "\n",
        "        Args:\n",
        "        - W: Initial channel parameter\n",
        "        - n: Recursion depth\n",
        "\n",
        "        Returns:\n",
        "        - Transformed channel parameter\n",
        "        \"\"\"\n",
        "        if n == 0:\n",
        "            return W\n",
        "\n",
        "        W_used = self._bhattacharyya_bound(W, n-1)\n",
        "        W_transform = 2 * W_used**2 - W_used**4\n",
        "        return W_transform\n",
        "\n",
        "    def _get_info_bit_indices(self):\n",
        "        \"\"\"\n",
        "        Determine information bit indices using channel polarization\n",
        "\n",
        "        Returns:\n",
        "        - Indices of information bits\n",
        "        \"\"\"\n",
        "        # Total number of bits including CRC\n",
        "        total_bits = self.K + self.crc_order\n",
        "\n",
        "        # Calculate channel capacities\n",
        "        channel_capacities = []\n",
        "        for i in range(self.N):\n",
        "            W = 0.5  # Initial channel parameter\n",
        "            capacity = self._bhattacharyya_bound(W, int(np.log2(self.N)))\n",
        "            channel_capacities.append(capacity)\n",
        "\n",
        "        # Select most reliable bit positions\n",
        "        sorted_indices = np.argsort(channel_capacities)\n",
        "        return sorted_indices[-total_bits:]\n",
        "\n",
        "    def decode(self, received_signal, list_size):\n",
        "        \"\"\"\n",
        "        SCL decoding algorithm with CRC verification\n",
        "\n",
        "        Args:\n",
        "        - received_signal: Channel received signal\n",
        "        - list_size: Decoding list size\n",
        "\n",
        "        Returns:\n",
        "        - Decoded information bits or None if CRC fails\n",
        "        \"\"\"\n",
        "        # Initialize paths\n",
        "        paths = [{'bits': np.zeros(self.N, dtype=int), 'metric': 0.0}]\n",
        "\n",
        "        for bit_position in range(self.N):\n",
        "            new_paths = []\n",
        "\n",
        "            for path in paths:\n",
        "                # Explore both 0 and 1 bit decisions\n",
        "                for bit_value in [0, 1]:\n",
        "                    new_path = path.copy()\n",
        "                    new_path['bits'][bit_position] = bit_value\n",
        "\n",
        "                    # Update path metric (simplified)\n",
        "                    path_metric = path['metric'] - np.abs(received_signal[bit_position] - (2*bit_value - 1))\n",
        "                    new_path['metric'] = path_metric\n",
        "\n",
        "                    new_paths.append(new_path)\n",
        "\n",
        "            # Sort and prune paths\n",
        "            new_paths.sort(key=lambda x: x['metric'])\n",
        "            paths = new_paths[:list_size]\n",
        "\n",
        "        # Select best path\n",
        "        best_path = paths[0]['bits']\n",
        "\n",
        "        # Extract info and CRC bits\n",
        "        decoded_info_bits = best_path[self.info_indices[:self.K]]\n",
        "        decoded_crc_bits = best_path[self.info_indices[self.K:]]\n",
        "\n",
        "        # Verify CRC\n",
        "        if self.crc.verify_crc(decoded_info_bits, decoded_crc_bits):\n",
        "            return decoded_info_bits\n",
        "        else:\n",
        "            return None  # CRC verification failed\n",
        "\n",
        "\n",
        "\n",
        "class ChannelSimulator:\n",
        "    @staticmethod\n",
        "    def awgn_channel(transmitted_signal, snr_db):\n",
        "        \"\"\"\n",
        "        Additive White Gaussian Noise (AWGN) Channel\n",
        "\n",
        "        Args:\n",
        "        - transmitted_signal: Transmitted signal\n",
        "        - snr_db: Signal-to-Noise Ratio in dB\n",
        "\n",
        "        Returns:\n",
        "        - Received signal\n",
        "        - Noise variance\n",
        "        \"\"\"\n",
        "        # Convert SNR from dB to linear scale\n",
        "        snr = 10 ** (snr_db / 10)\n",
        "\n",
        "        # Calculate noise variance\n",
        "        sigma = np.sqrt(1 / (2 * snr))\n",
        "\n",
        "        # Generate Gaussian noise\n",
        "        noise = np.random.normal(0, sigma, transmitted_signal.shape)\n",
        "\n",
        "        # Add noise to transmitted signal\n",
        "        received_signal = transmitted_signal + noise\n",
        "\n",
        "        return received_signal, sigma\n",
        "\n",
        "    @staticmethod\n",
        "    def rayleigh_fading_channel(transmitted_signal, snr_db):\n",
        "        \"\"\"\n",
        "        Rayleigh Fading Channel\n",
        "\n",
        "        Args:\n",
        "        - transmitted_signal: Transmitted signal\n",
        "        - snr_db: Signal-to-Noise Ratio in dB\n",
        "\n",
        "        Returns:\n",
        "        - Received signal\n",
        "        - Fading coefficient\n",
        "        \"\"\"\n",
        "        # Convert SNR from dB to linear scale\n",
        "        snr = 10 ** (snr_db / 10)\n",
        "\n",
        "        # Calculate noise variance\n",
        "        sigma = np.sqrt(1 / (2 * snr))\n",
        "\n",
        "        # Generate Rayleigh fading coefficient\n",
        "        fading_coeff = np.random.rayleigh(scale=1)\n",
        "\n",
        "        # Apply fading to transmitted signal\n",
        "        faded_signal = fading_coeff * transmitted_signal\n",
        "\n",
        "        # Generate Gaussian noise\n",
        "        noise = np.random.normal(0, sigma, faded_signal.shape)\n",
        "\n",
        "        # Received signal\n",
        "        received_signal = faded_signal + noise\n",
        "\n",
        "        return received_signal, fading_coeff\n",
        "\n",
        "class RNNPolarDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Simplified input handling\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            hidden_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=False  # Simplified\n",
        "        )\n",
        "\n",
        "        # Output layers\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size // 2, output_size)\n",
        "        )\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure 2D input\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Input projection\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        # Add sequence dimension\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Take last time step\n",
        "        out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Output layer\n",
        "        out = self.output_layer(out)\n",
        "\n",
        "        return self.sigmoid(out)\n",
        "# Decoding Utility Functions Section\n",
        "\n",
        "def traditional_decode(received_signal, block_length, info_bits):\n",
        "    \"\"\"Traditional hard-decision decoding\"\"\"\n",
        "    decoded_bits = np.zeros(info_bits, dtype=int)\n",
        "    for i in range(info_bits):\n",
        "        decoded_bits[i] = 1 if received_signal[i] > 0 else 0\n",
        "    return decoded_bits\n",
        "\n",
        "def ml_decode(received_signal, block_length, info_bits, model=None):\n",
        "    \"\"\"\n",
        "    Machine Learning-based decoding\n",
        "\n",
        "    Args:\n",
        "    - received_signal: Input signal\n",
        "    - block_length: Total block length\n",
        "    - info_bits: Number of information bits\n",
        "    - model: Trained neural network model\n",
        "\n",
        "    Returns:\n",
        "    - Decoded bits\n",
        "    \"\"\"\n",
        "    # Fallback to traditional decoding if no model\n",
        "    if model is None:\n",
        "        decoded_bits = np.zeros(info_bits, dtype=int)\n",
        "        for i in range(info_bits):\n",
        "            decoded_bits[i] = 1 if received_signal[i] > 0 else 0\n",
        "        return decoded_bits\n",
        "\n",
        "    try:\n",
        "        # Ensure model is in evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Prepare input tensor\n",
        "        if not isinstance(received_signal, torch.Tensor):\n",
        "            received_signal = torch.FloatTensor(received_signal).unsqueeze(0)\n",
        "\n",
        "        # ML decoding\n",
        "        with torch.no_grad():\n",
        "            decoded_prob = model(received_signal).squeeze().numpy()\n",
        "            decoded_bits = (decoded_prob > 0.5).astype(int)\n",
        "\n",
        "        return decoded_bits[:info_bits]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ML Decoding error: {e}\")\n",
        "        # Fallback to traditional decoding\n",
        "        decoded_bits = np.zeros(info_bits, dtype=int)\n",
        "        for i in range(info_bits):\n",
        "            decoded_bits[i] = 1 if received_signal[i] > 0 else 0\n",
        "        return decoded_bits\n",
        "\n",
        "# Then your generate_ber(), generate_waterfall_bler(), etc. functions follow\n",
        "\n",
        "class MLTrainer:\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, learning_rate=1e-3):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Simplified model initialization\n",
        "        self.model = RNNPolarDecoder(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            output_size=output_size\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Optimizer and loss\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def prepare_data(self, X, y, test_size=0.2):\n",
        "        # Convert to tensors\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        y_tensor = torch.FloatTensor(y)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_tensor, y_tensor, test_size=test_size, random_state=42\n",
        "        )\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=100):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_train_loss = 0\n",
        "\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Zero gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_train_loss += loss.item()\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    batch_x = batch_x.to(self.device)\n",
        "                    batch_y = batch_y.to(self.device)\n",
        "\n",
        "                    outputs = self.model(batch_x)\n",
        "                    val_loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                    epoch_val_loss += val_loss.item()\n",
        "\n",
        "            # Record losses\n",
        "            train_losses.append(epoch_train_loss / len(train_loader))\n",
        "            val_losses.append(epoch_val_loss / len(val_loader))\n",
        "\n",
        "            # Print progress\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, learning_rate=1e-3):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = RNNPolarDecoder(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            output_size=output_size\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "\n",
        "    def prepare_data(self, X, y, test_size=0.2):\n",
        "        # Convert to PyTorch tensors\n",
        "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
        "        y_tensor = torch.FloatTensor(y).to(self.device)\n",
        "\n",
        "        # Create dataset and dataloader\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        train_size = int((1 - test_size) * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            dataset, [train_size, val_size]\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=50):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_train_loss = 0\n",
        "\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Zero gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_train_loss += loss.item()\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    batch_x = batch_x.to(self.device)\n",
        "                    batch_y = batch_y.to(self.device)\n",
        "\n",
        "                    outputs = self.model(batch_x)\n",
        "                    loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                    epoch_val_loss += loss.item()\n",
        "\n",
        "            # Average losses\n",
        "            avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(avg_val_loss)\n",
        "\n",
        "            # Print progress\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "def plot_training_history(train_losses, val_losses):\n",
        "    \"\"\"\n",
        "    Plot training and validation losses\n",
        "\n",
        "    Args:\n",
        "    - train_losses: List of training losses\n",
        "    - val_losses: List of validation losses\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Training and Validation Loss Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Learning Rate Progression Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(trainer.scheduler.get_last_lr(), label='Learning Rate')\n",
        "    plt.title('Learning Rate Progression')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix\n",
        "\n",
        "    Args:\n",
        "    - y_true: True labels\n",
        "    - y_pred: Predicted labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix using seaborn\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(train_losses, val_losses):\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Training and Validation Loss Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Learning Rate Progression Plot\n",
        "   # plt.subplot(1, 2, 2)\n",
        "    #plt.plot(trainer.scheduler.get_last_lr(), label='Learning Rate')\n",
        "    #plt.title('Learning Rate Progression')\n",
        "    #plt.xlabel('Epochs')\n",
        "    #plt.ylabel('Learning Rate')\n",
        "    #plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix\n",
        "\n",
        "    Args:\n",
        "    - y_true: True labels\n",
        "    - y_pred: Predicted labels\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot confusion matrix using seaborn\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "class AdvancedSCLDecoder:\n",
        "    def __init__(self,\n",
        "                 block_length=None,\n",
        "                 info_length=None,\n",
        "                 N=None,  # Alternative for block_length\n",
        "                 K=None,  # Alternative for info_length\n",
        "                 crc_length=16,\n",
        "                 list_sizes=None):\n",
        "        \"\"\"\n",
        "        Flexible initialization for Advanced SCL Decoder\n",
        "\n",
        "        Args:\n",
        "            block_length (int): Total code block length\n",
        "            info_length (int): Number of information bits\n",
        "            N (int): Alternative name for block_length\n",
        "            K (int): Alternative name for info_length\n",
        "            crc_length (int): Length of CRC code\n",
        "            list_sizes (list): Decoding list sizes\n",
        "        \"\"\"\n",
        "        # Resolve block length\n",
        "        self.block_length = block_length or N\n",
        "        if self.block_length is None:\n",
        "            raise ValueError(\"Block length must be specified (block_length or N)\")\n",
        "\n",
        "        # Resolve info length\n",
        "        self.info_length = info_length or K\n",
        "        if self.info_length is None:\n",
        "            raise ValueError(\"Information length must be specified (info_length or K)\")\n",
        "\n",
        "        # Validate lengths\n",
        "        if not isinstance(self.block_length, int):\n",
        "            raise TypeError(f\"Block length must be an integer, got {type(self.block_length)}\")\n",
        "\n",
        "        if not isinstance(self.info_length, int):\n",
        "            raise TypeError(f\"Info length must be an integer, got {type(self.info_length)}\")\n",
        "\n",
        "        if self.block_length <= 0 or self.info_length <= 0:\n",
        "            raise ValueError(\"Block length and info length must be positive\")\n",
        "\n",
        "        if self.block_length < self.info_length:\n",
        "            raise ValueError(\"Block length must be greater than or equal to info length\")\n",
        "\n",
        "        # CRC length\n",
        "        self.crc_length = crc_length\n",
        "\n",
        "        # List sizes for decoding\n",
        "        self.list_sizes = list_sizes or [1, 2, 4, 8, 16]\n",
        "\n",
        "        # Logging initialization details\n",
        "        self._log_initialization()\n",
        "\n",
        "        # Initialize core components\n",
        "        self._initialize_components()\n",
        "\n",
        "    def _log_initialization(self):\n",
        "        \"\"\"Log initialization details\"\"\"\n",
        "        try:\n",
        "            import logging\n",
        "            logging.info(f\"Initializing Advanced SCL Decoder\")\n",
        "            logging.info(f\"Block Length: {self.block_length}\")\n",
        "            logging.info(f\"Info Length: {self.info_length}\")\n",
        "            logging.info(f\"CRC Length: {self.crc_length}\")\n",
        "            logging.info(f\"List Sizes: {self.list_sizes}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Logging error: {e}\")\n",
        "\n",
        "    def _initialize_components(self):\n",
        "        \"\"\"\n",
        "        Initialize decoder components with robust error handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Initialize Polar Code\n",
        "            self.polar_code = self._create_polar_code()\n",
        "\n",
        "            # Initialize CRC\n",
        "            self.crc = self._create_crc()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Component initialization failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _create_polar_code(self):\n",
        "        \"\"\"\n",
        "        Create Polar Code with flexible initialization\n",
        "\n",
        "        Returns:\n",
        "            PolarCode: Initialized Polar Code instance\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return PolarCode(\n",
        "                block_length=self.block_length,\n",
        "                info_length=self.info_length\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Polar Code creation error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _create_crc(self):\n",
        "        \"\"\"\n",
        "        Create CRC with flexible initialization\n",
        "\n",
        "        Returns:\n",
        "            CRC: Initialized CRC instance\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return CRC(self.crc_length)\n",
        "        except Exception as e:\n",
        "            print(f\"CRC creation error: {e}\")\n",
        "            raise\n",
        "\n",
        "    # Rest of the decoder implementation follows...\n",
        "\n",
        "\n",
        "\n",
        "  #  def __init__(self,\n",
        "   #              N=None,  # Block length\n",
        "    #             K=None,  # Information length\n",
        "     #            block_length=None,\n",
        "      #           info_length=None,\n",
        "       #          crc_length=16):\n",
        "        \"\"\"\n",
        "        Flexible constructor to handle different parameter naming\n",
        "\n",
        "        Args:\n",
        "            N/block_length (int): Total block length\n",
        "            K/info_length (int): Information bit length\n",
        "            crc_length (int): CRC length for error detection\n",
        "        \"\"\"\n",
        "        # Prioritize N and K if provided, otherwise use block_length and info_length\n",
        "        self.block_length = N or block_length\n",
        "        self.info_length = K or info_length\n",
        "\n",
        "        # Validate inputs\n",
        "        if self.block_length is None or self.info_length is None:\n",
        "            raise ValueError(\"Block length and information length must be specified\")\n",
        "\n",
        "        # Validate lengths\n",
        "        if self.block_length <= self.info_length:\n",
        "            raise ValueError(\"Block length must be greater than information length\")\n",
        "\n",
        "        self.crc_length = crc_length\n",
        "        self.list_sizes = [1, 2, 4, 8, 16]  # Multiple list sizes for decoding\n",
        "\n",
        "        # Initialize necessary components\n",
        "        self.polar_code = PolarCode(\n",
        "            block_length=self.block_length,\n",
        "            info_length=self.info_length\n",
        "        )\n",
        "        self.crc = CRC(self.crc_length)\n",
        "\n",
        "    # ... rest of the previous implementation remains the same\n",
        "\n",
        "\n",
        "    def decode(self, received_signal, list_size=16, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Advanced Successive Cancellation List (SCL) Decoder\n",
        "\n",
        "        Args:\n",
        "            received_signal (np.ndarray): Received noisy signal\n",
        "            list_size (int): Number of candidate paths to explore\n",
        "            channel_type (str): Type of communication channel\n",
        "\n",
        "        Returns:\n",
        "            decoded_bits (np.ndarray): Decoded information bits\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Validate input\n",
        "            if received_signal is None:\n",
        "                raise ValueError(\"Received signal cannot be None\")\n",
        "\n",
        "            # Preprocessing\n",
        "            preprocessed_signal = self._preprocess_signal(received_signal, channel_type)\n",
        "\n",
        "            # Path generation and decoding\n",
        "            decoded_paths = self._generate_decoding_paths(\n",
        "                preprocessed_signal,\n",
        "                list_size,\n",
        "                channel_type\n",
        "            )\n",
        "\n",
        "            # Path selection and CRC validation\n",
        "            best_path = self._select_best_path(decoded_paths)\n",
        "\n",
        "            return best_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Decoding error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _preprocess_signal(self, received_signal, channel_type):\n",
        "        \"\"\"\n",
        "        Signal preprocessing based on channel characteristics\n",
        "\n",
        "        Args:\n",
        "            received_signal (np.ndarray): Raw received signal\n",
        "            channel_type (str): Communication channel type\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Preprocessed signal\n",
        "        \"\"\"\n",
        "        if channel_type == 'AWGN':\n",
        "            # AWGN-specific preprocessing\n",
        "            return self._awgn_preprocessing(received_signal)\n",
        "        elif channel_type == 'Rayleigh':\n",
        "            # Rayleigh fading preprocessing\n",
        "            return self._rayleigh_preprocessing(received_signal)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {channel_type}\")\n",
        "\n",
        "    def _awgn_preprocessing(self, signal):\n",
        "        \"\"\"AWGN channel signal preprocessing\"\"\"\n",
        "        # Implement AWGN-specific signal normalization\n",
        "        normalized_signal = (signal - np.mean(signal)) / np.std(signal)\n",
        "        return normalized_signal\n",
        "\n",
        "    def _rayleigh_preprocessing(self, signal):\n",
        "        \"\"\"Rayleigh fading channel signal preprocessing\"\"\"\n",
        "        # Implement Rayleigh fading compensation\n",
        "        fading_compensation = np.abs(signal)\n",
        "        normalized_signal = fading_compensation / np.max(fading_compensation)\n",
        "        return normalized_signal\n",
        "\n",
        "    def _generate_decoding_paths(self, preprocessed_signal, list_size, channel_type):\n",
        "        \"\"\"\n",
        "        Generate multiple decoding candidate paths\n",
        "\n",
        "        Args:\n",
        "            preprocessed_signal (np.ndarray): Preprocessed received signal\n",
        "            list_size (int): Number of candidate paths\n",
        "            channel_type (str): Communication channel type\n",
        "\n",
        "        Returns:\n",
        "            list: Decoded candidate paths\n",
        "        \"\"\"\n",
        "        paths = []\n",
        "\n",
        "        # Implement SCL decoding logic\n",
        "        for _ in range(list_size):\n",
        "            # Generate path with some randomness\n",
        "            path = self._generate_single_path(preprocessed_signal, channel_type)\n",
        "            paths.append(path)\n",
        "\n",
        "        return paths\n",
        "\n",
        "    def _generate_single_path(self, signal, channel_type):\n",
        "        \"\"\"\n",
        "        Generate a single decoding path\n",
        "\n",
        "        Args:\n",
        "            signal (np.ndarray): Preprocessed signal\n",
        "            channel_type (str): Communication channel type\n",
        "\n",
        "        Returns:\n",
        "            dict: Decoded path information\n",
        "        \"\"\"\n",
        "        # Implement path generation logic\n",
        "        decoded_bits = self.polar_code.decode(signal)\n",
        "\n",
        "        # CRC validation\n",
        "        crc_valid = self.crc.validate(decoded_bits)\n",
        "\n",
        "        return {\n",
        "            'bits': decoded_bits,\n",
        "            'crc_valid': crc_valid,\n",
        "            'metric': np.random.random()  # Example path metric\n",
        "        }\n",
        "\n",
        "    def _select_best_path(self, paths):\n",
        "        \"\"\"\n",
        "        Select the best decoding path\n",
        "\n",
        "        Args:\n",
        "            paths (list): Candidate decoding paths\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Best decoded bits\n",
        "        \"\"\"\n",
        "        # Sort paths by CRC validity and path metric\n",
        "        valid_paths = [path for path in paths if path['crc_valid']]\n",
        "\n",
        "        if valid_paths:\n",
        "            best_path = max(valid_paths, key=lambda x: x['metric'])\n",
        "            return best_path['bits']\n",
        "\n",
        "        # Fallback to first path if no valid paths\n",
        "        return paths[0]['bits'] if paths else None\n",
        "\n",
        "\n",
        "    def _channel_polarization(self, W, n):\n",
        "        \"\"\"\n",
        "        Advanced channel polarization method\n",
        "\n",
        "        Args:\n",
        "        - W: Initial channel parameter\n",
        "        - n: Recursion depth\n",
        "\n",
        "        Returns:\n",
        "        - Transformed channel parameter with enhanced polarization\n",
        "        \"\"\"\n",
        "        if n == 0:\n",
        "            return W\n",
        "\n",
        "        W_used = self._channel_polarization(W, n-1)\n",
        "\n",
        "        # Enhanced polarization transformation\n",
        "        W_transform = 2 * W_used**2 - W_used**4\n",
        "\n",
        "        # Add non-linear perturbation\n",
        "        perturbation = np.random.normal(0, 0.1)\n",
        "        W_transform += perturbation * W_transform\n",
        "\n",
        "        return np.clip(W_transform, 0, 1)\n",
        "\n",
        "    def _get_info_bit_indices(self):\n",
        "        \"\"\"\n",
        "        Advanced information bit selection\n",
        "\n",
        "        Returns:\n",
        "        - Indices of most reliable bits\n",
        "        \"\"\"\n",
        "        # Total bits including CRC\n",
        "        total_bits = self.K + self.crc_order\n",
        "\n",
        "        # Calculate channel capacities with enhanced polarization\n",
        "        channel_capacities = []\n",
        "        for i in range(self.N):\n",
        "            W = 0.5  # Initial channel parameter\n",
        "            capacity = self._channel_polarization(W, int(np.log2(self.N)))\n",
        "            channel_capacities.append(capacity)\n",
        "\n",
        "        # Select most reliable bit positions with additional criteria\n",
        "        sorted_indices = np.argsort(channel_capacities)\n",
        "        return sorted_indices[-total_bits:]\n",
        "\n",
        "\n",
        "def decode(self, received_signal, list_size, channel_type):\n",
        "    # Add defensive programming checks\n",
        "    paths = self.generate_paths(received_signal, list_size, channel_type)\n",
        "\n",
        "    if not paths:\n",
        "        print(\"Warning: No valid paths generated during decoding\")\n",
        "        return None  # Or handle gracefully\n",
        "\n",
        "    best_paths = []\n",
        "    for ls in self.list_sizes:\n",
        "        if len(paths) >= ls:\n",
        "            best_path = paths[:ls][0]['bits']\n",
        "            best_paths.append(best_path)\n",
        "        else:\n",
        "            print(f\"Warning: Insufficient paths for list size {ls}\")\n",
        "            best_paths.append(None)\n",
        "\n",
        "    return best_paths\n",
        "\n",
        "\n",
        "def generate_paths(self, received_signal, list_size, channel_type):\n",
        "    paths = []\n",
        "    try:\n",
        "        # Your existing path generation logic\n",
        "        paths = self._generate_candidate_paths(received_signal, list_size, channel_type)\n",
        "\n",
        "        # Add logging\n",
        "        print(f\"Generated paths: {len(paths)}\")\n",
        "        print(f\"Path details: {paths[:2]}\")  # Print first two paths for inspection\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Path generation error: {e}\")\n",
        "\n",
        "    return paths\n",
        "\n",
        "def decode(self, received_signal, list_size, channel_type):\n",
        "    try:\n",
        "        paths = self.generate_paths(received_signal, list_size, channel_type)\n",
        "\n",
        "        if not paths:\n",
        "            logging.error(\"No paths generated during decoding\")\n",
        "            return None\n",
        "\n",
        "        best_paths = []\n",
        "        for ls in self.list_sizes:\n",
        "            try:\n",
        "                best_path = paths[:ls][0]['bits'] if paths else None\n",
        "                best_paths.append(best_path)\n",
        "            except IndexError:\n",
        "                logging.warning(f\"Insufficient paths for list size {ls}\")\n",
        "                best_paths.append(None)\n",
        "\n",
        "        return best_paths\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Decoding error: {e}\")\n",
        "        return None\n",
        "\n",
        "def diagnose_decoding_issue(self, received_signal, list_size, channel_type):\n",
        "    diagnostics = {\n",
        "        'signal_shape': received_signal.shape,\n",
        "        'list_size': list_size,\n",
        "        'channel_type': channel_type,\n",
        "        'signal_stats': {\n",
        "            'mean': np.mean(received_signal),\n",
        "            'std': np.std(received_signal),\n",
        "            'min': np.min(received_signal),\n",
        "            'max': np.max(received_signal)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"Decoding Diagnostics:\")\n",
        "    for key, value in diagnostics.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    return diagnostics\n",
        "\n",
        "    def decode(self, received_signal, list_size, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Advanced list decoding with multiple strategies\n",
        "\n",
        "        Args:\n",
        "        - received_signal: Channel received signal\n",
        "        - list_size: Decoding list size\n",
        "        - channel_type: Channel type for adaptive decoding\n",
        "\n",
        "        Returns:\n",
        "        - Decoded information bits\n",
        "        \"\"\"\n",
        "        # Initialize paths with adaptive starting metrics\n",
        "        paths = [{'bits': np.zeros(self.N, dtype=int), 'metric': 0.0}]\n",
        "\n",
        "        # Channel-specific noise modeling\n",
        "        if channel_type == 'AWGN':\n",
        "            noise_factor = 1.0\n",
        "        elif channel_type == 'Rayleigh':\n",
        "            noise_factor = np.random.rayleigh(scale=1.0)\n",
        "        else:\n",
        "            noise_factor = 1.0\n",
        "\n",
        "        # Advanced path exploration\n",
        "        for bit_position in range(self.N):\n",
        "            new_paths = []\n",
        "\n",
        "            for path in paths:\n",
        "                # Explore both 0 and 1 bit decisions with adaptive strategies\n",
        "                for bit_value in [0, 1]:\n",
        "                    new_path = path.copy()\n",
        "                    new_path['bits'][bit_position] = bit_value\n",
        "\n",
        "                    # Enhanced path metric calculation\n",
        "                    path_metric = path['metric'] - np.abs(\n",
        "                        received_signal[bit_position] - (2*bit_value - 1)\n",
        "                    ) * noise_factor\n",
        "\n",
        "                    # Add probabilistic path pruning\n",
        "                    if np.random.random() < 0.9:\n",
        "                        new_path['metric'] = path_metric\n",
        "                        new_paths.append(new_path)\n",
        "\n",
        "            # Adaptive path selection\n",
        "            new_paths.sort(key=lambda x: x['metric'])\n",
        "            paths = new_paths[:list_size]\n",
        "\n",
        "        # Select best path with multiple list sizes\n",
        "        best_paths = []\n",
        "        for ls in self.list_sizes:\n",
        "            best_path = paths[:ls][0]['bits']\n",
        "\n",
        "            # Extract info and CRC bits\n",
        "            decoded_info_bits = best_path[self.info_indices[:self.K]]\n",
        "            decoded_crc_bits = best_path[self.info_indices[self.K:]]\n",
        "\n",
        "            # CRC verification\n",
        "            if self.crc.verify_crc(decoded_info_bits, decoded_crc_bits):\n",
        "                best_paths.append(decoded_info_bits)\n",
        "\n",
        "        # Return the most reliable decoded bits\n",
        "        return best_paths[0] if best_paths else None\n",
        "\n",
        "\n",
        "def generate_advanced_ber(block_length, info_bits, snr_range, model, num_trials):\n",
        "    \"\"\"\n",
        "    Advanced Bit Error Rate (BER) Generation with Robust Error Handling\n",
        "\n",
        "    Args:\n",
        "        block_length (int): Polar code block length\n",
        "        info_bits (int): Number of information bits\n",
        "        snr_range (np.ndarray): Signal-to-Noise Ratio range\n",
        "        model (str): Decoding model type\n",
        "        num_trials (int): Number of simulation trials\n",
        "\n",
        "    Returns:\n",
        "        tuple: SNR range and corresponding BER values\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize components\n",
        "        awgn_channel = ChannelSimulator()\n",
        "        rayleigh_channel = ChannelSimulator()\n",
        "        advanced_scl_decoder = AdvancedSCLDecoder(\n",
        "            block_length=block_length,\n",
        "            info_length=info_bits\n",
        "        )\n",
        "\n",
        "        # Initialize result storage\n",
        "        ber_values = np.zeros_like(snr_range, dtype=float)\n",
        "        bler_values = np.zeros_like(snr_range, dtype=float)\n",
        "\n",
        "        # Simulation progress tracking\n",
        "        print(f\"Starting BER Simulation: Block Length={block_length}, Info Bits={info_bits}\")\n",
        "\n",
        "        # Iterate through SNR points\n",
        "        for snr_idx, snr in enumerate(snr_range):\n",
        "            # Trial-level error tracking\n",
        "            trial_errors = 0\n",
        "            trial_block_errors = 0\n",
        "\n",
        "            # Multiple trials for statistical significance\n",
        "            for _ in range(num_trials):\n",
        "                # Generate random information bits\n",
        "                info_bits_array = np.random.randint(2, size=info_bits)\n",
        "\n",
        "                try:\n",
        "                    # Polar Code Encoding\n",
        "                    encoded_signal = advanced_scl_decoder.polar_code.encode(info_bits_array)\n",
        "\n",
        "                    # AWGN Channel Simulation\n",
        "                    awgn_received = awgn_channel.simulate(\n",
        "                        encoded_signal,\n",
        "                        snr=snr,\n",
        "                        channel_type='AWGN'\n",
        "                    )\n",
        "\n",
        "                    # Decoding\n",
        "                    decoded_bits = advanced_scl_decoder.decode(\n",
        "                        awgn_received,\n",
        "                        list_size=16,\n",
        "                        channel_type='AWGN'\n",
        "                    )\n",
        "\n",
        "                    # Error Detection\n",
        "                    if decoded_bits is not None:\n",
        "                        bit_errors = np.sum(decoded_bits != info_bits_array)\n",
        "                        block_error = bit_errors > 0\n",
        "\n",
        "                        trial_errors += bit_errors\n",
        "                        trial_block_errors += int(block_error)\n",
        "                    else:\n",
        "                        # Treat None as a block error\n",
        "                        trial_block_errors += 1\n",
        "\n",
        "                except Exception as trial_error:\n",
        "                    print(f\"Trial simulation error at SNR {snr}: {trial_error}\")\n",
        "                    trial_block_errors += 1\n",
        "\n",
        "            # Compute average error rates\n",
        "            ber_values[snr_idx] = trial_errors / (num_trials * info_bits)\n",
        "            bler_values[snr_idx] = trial_block_errors / num_trials\n",
        "\n",
        "            # Progress logging\n",
        "            print(f\"SNR {snr} dB: BER = {ber_values[snr_idx]:.2e}, BLER = {bler_values[snr_idx]:.2e}\")\n",
        "\n",
        "        return snr_range, ber_values, bler_values\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"BER Generation Error: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def plot_performance_metrics(snr_range, ber_values, bler_values, title=\"Polar Code Performance\"):\n",
        "    \"\"\"\n",
        "    Robust Performance Metric Plotting\n",
        "\n",
        "    Args:\n",
        "        snr_range (np.ndarray): Signal-to-Noise Ratio range\n",
        "        ber_values (np.ndarray): Bit Error Rate values\n",
        "        bler_values (np.ndarray): Block Error Rate values\n",
        "        title (str): Plot title\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate input data\n",
        "        if (snr_range is None or ber_values is None or\n",
        "            len(snr_range) == 0 or len(ber_values) == 0):\n",
        "            print(\"Error: Insufficient data for plotting\")\n",
        "            return\n",
        "\n",
        "        # Remove zero or negative values for log scale\n",
        "        valid_indices = (ber_values > 0) & (bler_values > 0)\n",
        "\n",
        "        if not np.any(valid_indices):\n",
        "            print(\"No valid data points for plotting\")\n",
        "            return\n",
        "\n",
        "        # Filter valid data\n",
        "        plot_snr = snr_range[valid_indices]\n",
        "        plot_ber = ber_values[valid_indices]\n",
        "        plot_bler = bler_values[valid_indices]\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.semilogy(plot_snr, plot_ber, 'bo-', label='Bit Error Rate (BER)')\n",
        "        plt.semilogy(plot_snr, plot_bler, 'ro-', label='Block Error Rate (BLER)')\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Signal-to-Noise Ratio (dB)')\n",
        "        plt.ylabel('Error Rate')\n",
        "        plt.grid(True, which='both', ls='-', alpha=0.5)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as plot_error:\n",
        "        print(f\"Plotting Error: {plot_error}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_advanced_waterfall_bler(block_length, info_bits, snr_range, model=None, num_trials=100):\n",
        "    \"\"\"\n",
        "    Advanced Block Error Rate generation with sophisticated modeling\n",
        "    \"\"\"\n",
        "    # Initialize results dictionary\n",
        "    bler_results = {\n",
        "        'AWGN_Traditional': [],\n",
        "        'AWGN_ML': [],\n",
        "        'Rayleigh_Traditional': [],\n",
        "        'Rayleigh_ML': []\n",
        "    }\n",
        "\n",
        "    # Initialize advanced components\n",
        "    polar_code_gen = PolarCodeGenerator(N=block_length, K=info_bits)\n",
        "    awgn_channel = ChannelSimulator()\n",
        "    rayleigh_channel = ChannelSimulator()\n",
        "    advanced_scl_decoder = AdvancedSCLDecoder(N=block_length, K=info_bits)\n",
        "\n",
        "    # Simulation for each SNR point with enhanced modeling\n",
        "    for snr in snr_range:\n",
        "        # Initialize block error counters with adaptive tracking\n",
        "        awgn_traditional_block_errors = 0\n",
        "        awgn_ml_block_errors = 0\n",
        "        rayleigh_traditional_block_errors = 0\n",
        "        rayleigh_ml_block_errors = 0\n",
        "\n",
        "        for _ in range(num_trials):\n",
        "            # Generate random information bits\n",
        "            info_bits_array = np.random.randint(2, size=info_bits)\n",
        "\n",
        "            # Encode with advanced polarization\n",
        "            encoded_bits = polar_code_gen.encode(info_bits_array)\n",
        "\n",
        "            # Channel-specific advanced simulation\n",
        "            awgn_received_traditional, _ = awgn_channel.awgn_channel(\n",
        "                2 * encoded_bits - 1, snr\n",
        "            )\n",
        "            rayleigh_received_traditional, _ = rayleigh_channel.rayleigh_fading_channel(\n",
        "                2 * encoded_bits - 1, snr\n",
        "            )\n",
        "\n",
        "            # Advanced decoding with multiple list sizes\n",
        "            awgn_decoded_traditional = advanced_scl_decoder.decode(\n",
        "                awgn_received_traditional, list_size=16, channel_type='AWGN'\n",
        "            )\n",
        "            rayleigh_decoded_traditional = advanced_scl_decoder.decode(\n",
        "                rayleigh_received_traditional, list_size=16, channel_type='Rayleigh'\n",
        "            )\n",
        "\n",
        "            # Block error calculation with adaptive modeling\n",
        "            if awgn_decoded_traditional is not None:\n",
        "                awgn_traditional_block_errors += int(np.any(info_bits_array != awgn_decoded_traditional))\n",
        "\n",
        "            if rayleigh_decoded_traditional is not None:\n",
        "                rayleigh_traditional_block_errors += int(np.any(info_bits_array != rayleigh_decoded_traditional))\n",
        "\n",
        "        # Calculate BLER with enhanced error estimation\n",
        "        bler_results['AWGN_Traditional'].append(awgn_traditional_block_errors / num_trials)\n",
        "        bler_results['Rayleigh_Traditional'].append(rayleigh_traditional_block_errors / num_trials)\n",
        "\n",
        "    return bler_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_ber_bler_comparison(snr_range, ber_results, bler_results):\n",
        "    \"\"\"\n",
        "    Plot BER and BLER for different channels and decoding methods\n",
        "\n",
        "    Args:\n",
        "    - snr_range: SNR values\n",
        "    - ber_results: Bit Error Rate results\n",
        "    - bler_results: Block Error Rate results\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # BER Subplot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for key, values in ber_results.items():\n",
        "        plt.semilogy(snr_range, values, label=key, marker='o')\n",
        "\n",
        "    plt.title('Bit Error Rate')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER (log scale)')\n",
        "    plt.ylim(1e-5, 1e0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which='both', ls='-', alpha=0.5)\n",
        "\n",
        "    # BLER Subplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for key, values in bler_results.items():\n",
        "        plt.semilogy(snr_range, values, label=key, marker='o')\n",
        "\n",
        "    plt.title('Block Error Rate')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER (log scale)')\n",
        "    plt.ylim(1e-5, 1e0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which='both', ls='-', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics\n",
        "\n",
        "    Args:\n",
        "        history: Keras training history\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Plot training & validation accuracy values\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        # Accuracy subplot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "        # Loss subplot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training History Plot Error: {e}\")\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, X_train, y_train):\n",
        "    try:\n",
        "        # Predictions\n",
        "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "        # Classification Report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        # Plot Confusion Matrix\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            cbar=False\n",
        "        )\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return model.history\n",
        "\n",
        "def train_deep_learning_model(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Train Deep Learning Model for Polar Code Decoding\n",
        "    \"\"\"\n",
        "    # Clear any existing TensorFlow sessions\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Define model architecture\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Early stopping callback\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"Model Training Completed\")\n",
        "    return model, history\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Global Constants\n",
        "BLOCK_LENGTH = 128  # N\n",
        "info_bits = 64      # K (using lowercase as per previous convention)\n",
        "SNR_RANGE = np.linspace(-2, 6, 31)\n",
        "NUM_TRIALS = 50\n",
        "NUM_SAMPLES = 500\n",
        "EPOCHS = 50  # Number of training epochs\n",
        "BATCH_SIZE = 64  # Batch size for training\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Comprehensive Simulation with Performance Analysis and Visualization\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Generate Performance Metrics for Polar Code\n",
        "        snr, ber, bler = generate_advanced_ber(\n",
        "            N=BLOCK_LENGTH,\n",
        "            K=info_bits,  # Changed from INFO_LENGTH\n",
        "            snr_range=SNR_RANGE,\n",
        "            model='SCL',\n",
        "            num_trials=NUM_TRIALS\n",
        "        )\n",
        "\n",
        "        # 2. Prepare Machine Learning Dataset\n",
        "        X, y = prepare_ml_dataset()\n",
        "\n",
        "        # 3. Split Dataset\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # 4. Train Deep Learning Model\n",
        "        model, history = train_deep_learning_model(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # 5. Model Evaluation\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "        # 6. Visualization Plots\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 6.1 Training History Plot\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        # 6.2 Loss History Plot\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # 6.3 Confusion Matrix\n",
        "        plt.subplot(2, 2, 3)\n",
        "        cm = confusion_matrix(y_test, y_pred_classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        # 6.4 BER/BLER Performance\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.semilogy(snr, ber, 'bo-', label='Bit Error Rate (BER)')\n",
        "        plt.semilogy(snr, bler, 'ro-', label='Block Error Rate (BLER)')\n",
        "        plt.title('Polar Code Performance')\n",
        "        plt.xlabel('Signal-to-Noise Ratio (dB)')\n",
        "        plt.ylabel('Error Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 7. Print Detailed Classification Report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Simulation Error: {e}\")\n",
        "        import traceback\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "12HB2FzyoHW8",
        "outputId": "224fb860-a9ae-4428-c586-74013150676d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected 'except' or 'finally' block (<ipython-input-60-077b7e8a1368>, line 1709)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-077b7e8a1368>\"\u001b[0;36m, line \u001b[0;32m1709\u001b[0m\n\u001b[0;31m    def train_deep_learning_model(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE):\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import sys\n",
        "# Increase recursion limit\n",
        "sys.setrecursionlimit(3000)  # Adjust as needed\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt  # Add this import\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Configure logging at the start of your script\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s: %(message)s'\n",
        ")\n",
        "\n",
        "class CRC:\n",
        "    def __init__(self, polynomial=0b10011011, order=7):\n",
        "        self.polynomial = polynomial\n",
        "        self.order = order\n",
        "\n",
        "    def generate_crc(self, data):\n",
        "        \"\"\"Generate CRC checksum.\"\"\"\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.order, dtype=int)])\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    data_with_zeros[i + j] ^= ((self.polynomial >> j) & 1)\n",
        "        return data_with_zeros[-self.order:]\n",
        "\n",
        "    def verify_crc(self, data, received_crc):\n",
        "        \"\"\"Verify CRC checksum.\"\"\"\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    full_data[i + j] ^= ((self.polynomial >> j) & 1)\n",
        "        return np.all(full_data[-self.order:] == 0)\n",
        "\n",
        "\n",
        "class PolarCode:\n",
        "    def __init__(self, block_length, info_length, design_snr=0):\n",
        "        \"\"\"Polar Code Constructor.\"\"\"\n",
        "        if block_length <= 0 or info_length <= 0:\n",
        "            raise ValueError(\"Block length and info length must be positive\")\n",
        "        if block_length < info_length:\n",
        "            raise ValueError(\"Block length must be greater than or equal to info length\")\n",
        "\n",
        "        self.block_length = block_length\n",
        "        self.info_length = info_length\n",
        "        self.design_snr = design_snr\n",
        "        self.channel_capacities = self._compute_channel_capacities()\n",
        "        self.info_bit_positions = self._select_info_bit_positions()\n",
        "\n",
        "    def _compute_channel_capacities(self):\n",
        "        \"\"\"Compute channel capacities using Bhattacharyya parameter approximation.\"\"\"\n",
        "        def bhattacharyya_parameter(design_snr):\n",
        "            return np.exp(-((10 ** (design_snr / 10)) / 2))\n",
        "\n",
        "        capacities = np.zeros(self.block_length)\n",
        "        z = bhattacharyya_parameter(self.design_snr)\n",
        "        capacities[0] = z\n",
        "\n",
        "        for i in range(1, self.block_length):\n",
        "            j = i // 2\n",
        "            capacities[i] = capacities[j] ** 2 if i % 2 == 0 else 1 - (1 - capacities[j] ** 2) ** 2\n",
        "\n",
        "        return capacities\n",
        "\n",
        "    def _select_info_bit_positions(self):\n",
        "        \"\"\"Select best bit positions for information transmission.\"\"\"\n",
        "        sorted_indices = np.argsort(self.channel_capacities)\n",
        "        info_positions = sorted_indices[-self.info_length:]\n",
        "        return np.sort(info_positions)\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        \"\"\"Polar Code Encoder.\"\"\"\n",
        "        if len(info_bits) != self.info_length:\n",
        "            raise ValueError(f\"Expected {self.info_length} info bits, got {len(info_bits)}\")\n",
        "\n",
        "        codeword = np.zeros(self.block_length, dtype=int)\n",
        "        codeword[self.info_bit_positions] = info_bits\n",
        "\n",
        "        for i in range(int(np.log2(self.block_length))):\n",
        "            step = 2 ** i\n",
        "            for j in range(0, self.block_length, 2 * step):\n",
        "                for k in range(j, j + step):\n",
        "                    codeword[k + step] ^= codeword[k]\n",
        "\n",
        "        return codeword\n",
        "\n",
        "\n",
        "\n",
        "class PolarCode(PolarCode): # Continuing from Part 1\n",
        "    # ... (encode method from Part 1) ...\n",
        "\n",
        "    def decode(self, received_signal, decoding_method='SC'):\n",
        "        \"\"\"Polar Code Decoder.\"\"\"\n",
        "        if decoding_method == 'SC':\n",
        "            return self._successive_cancellation_decode(received_signal)\n",
        "        # Add other decoding methods if needed (e.g., SCL)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported decoding method: {decoding_method}\")\n",
        "\n",
        "    def _successive_cancellation_decode(self, received_signal):\n",
        "        \"\"\"Successive Cancellation (SC) Decoding.\"\"\"\n",
        "        decoded_bits = np.zeros(self.block_length, dtype=int)\n",
        "\n",
        "        for i in range(int(np.log2(self.block_length))):\n",
        "            step = 2 ** i\n",
        "            for j in range(0, self.block_length, 2 * step):\n",
        "                for k in range(j, j + step):\n",
        "                    llr = self._compute_log_likelihood_ratio(received_signal[k], received_signal[k + step], decoded_bits[k])\n",
        "                    decoded_bits[k + step] = 1 if llr < 0 else 0\n",
        "\n",
        "        return decoded_bits[self.info_bit_positions]\n",
        "\n",
        "    def _compute_log_likelihood_ratio(self, left_signal, right_signal, previous_bit):\n",
        "        \"\"\"Compute Log-Likelihood Ratio for bit decision.\"\"\"\n",
        "        return left_signal - right_signal + previous_bit\n",
        "\n",
        "\n",
        "class SCLDecoder:\n",
        "    def __init__(self, N, K, crc_order=7, list_sizes=[1, 4, 8]):\n",
        "        \"\"\"Successive Cancellation List Decoder with CRC.\"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc = CRC(order=crc_order)\n",
        "        self.list_sizes = list_sizes\n",
        "        self.crc_order = crc_order\n",
        "        self.info_indices = self._get_info_bit_indices()\n",
        "\n",
        "    # ... (Methods _bhattacharyya_bound and _get_info_bit_indices are the same as before) ...\n",
        "\n",
        "    def decode(self, received_signal, list_size):\n",
        "        \"\"\"SCL decoding algorithm with CRC verification and early stopping.\"\"\"\n",
        "        paths = [{'bits': np.zeros(self.N, dtype=int), 'metric': 0.0}]\n",
        "\n",
        "        for bit_position in range(self.N):\n",
        "            new_paths = []\n",
        "            for path in paths:\n",
        "                for bit_value in [0, 1]:\n",
        "                    new_path = path.copy()\n",
        "                    new_path['bits'][bit_position] = bit_value\n",
        "                    path_metric = path['metric'] - np.abs(received_signal[bit_position] - (2 * bit_value - 1))\n",
        "                    new_path['metric'] = path_metric\n",
        "                    new_paths.append(new_path)\n",
        "\n",
        "            # Early Stopping Check\n",
        "            for path in new_paths:  # Check in new_paths before sorting\n",
        "                decoded_info_bits = path['bits'][self.info_indices[:self.K]]\n",
        "                decoded_crc_bits = path['bits'][self.info_indices[self.K:]]\n",
        "                if self.crc.verify_crc(decoded_info_bits, decoded_crc_bits):\n",
        "                    return decoded_info_bits  # Return if CRC is valid\n",
        "\n",
        "            # Sort and prune paths (only if early stopping didn't occur)\n",
        "            new_paths.sort(key=lambda x: x['metric'], reverse=True)  # Sort by metric in descending order\n",
        "            paths = new_paths[:list_size]\n",
        "\n",
        "        # Select best path if early stopping didn't occur\n",
        "        if paths:  # Check if paths is not empty\n",
        "            best_path = paths[0]['bits']\n",
        "            decoded_info_bits = best_path[self.info_indices[:self.K]]\n",
        "            return decoded_info_bits\n",
        "        else:\n",
        "            return None  # Or handle empty paths as needed\n",
        "\n",
        "\n",
        "class ChannelSimulator:\n",
        "    @staticmethod\n",
        "    def awgn_channel(transmitted_signal, snr_db):\n",
        "        \"\"\"Additive White Gaussian Noise (AWGN) Channel.\"\"\"\n",
        "        snr = 10 ** (snr_db / 10)\n",
        "        sigma = np.sqrt(1 / (2 * snr))\n",
        "        noise = np.random.normal(0, sigma, transmitted_signal.shape)\n",
        "        received_signal = transmitted_signal + noise\n",
        "        return received_signal, sigma\n",
        "\n",
        "    @staticmethod\n",
        "    def rayleigh_fading_channel(transmitted_signal, snr_db):\n",
        "        \"\"\"Rayleigh Fading Channel.\"\"\"\n",
        "        snr = 10 ** (snr_db / 10)\n",
        "        sigma = np.sqrt(1 / (2 * snr))\n",
        "        fading_coeff = np.random.rayleigh(scale=1)\n",
        "        faded_signal = fading_coeff * transmitted_signal\n",
        "        noise = np.random.normal(0, sigma, faded_signal.shape)\n",
        "        received_signal = faded_signal + noise\n",
        "        return received_signal, fading_coeff\n",
        "\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Decode hidden state of last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        # Apply sigmoid activation to get probabilities\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MLTrainer:\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, learning_rate=1e-3):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = RNNDecoder(input_size, hidden_size, num_layers, output_size).to(self.device)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    def prepare_data(self, X, y, test_size=0.2):\n",
        "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
        "        y_tensor = torch.FloatTensor(y).to(self.device)\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        train_size = int((1 - test_size) * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "        return train_loader, val_loader\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=50):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_train_loss = 0\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_train_loss += loss.item()\n",
        "\n",
        "            self.model.eval()\n",
        "            epoch_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    outputs = self.model(batch_x)\n",
        "                    loss = self.criterion(outputs, batch_y)\n",
        "                    epoch_val_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "            avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            self.scheduler.step(avg_val_loss)\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
        "        return train_losses, val_losses\n",
        "\n",
        "def traditional_decode(received_signal, block_length, info_bits):\n",
        "    \"\"\"Traditional hard-decision decoding.\"\"\"\n",
        "    decoded_bits = np.zeros(info_bits, dtype=int)\n",
        "    for i in range(info_bits):\n",
        "        decoded_bits[i] = 1 if received_signal[i] > 0 else 0\n",
        "    return decoded_bits\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main simulation function.\"\"\"\n",
        "\n",
        "    # Simulation parameters\n",
        "    block_length = 128\n",
        "    info_bits = 64\n",
        "    epochs=10\n",
        "\n",
        "    code_rate = info_bits / block_length  # Calculate code rate\n",
        "\n",
        "    print(f\"Code Rate (R): {code_rate}\")  # Display code rate\n",
        "\n",
        "    snr_range_db = np.arange(-5, 10, 1)  # Adjust SNR range as needed\n",
        "    num_trials = 50  # Number of trials for each SNR point\n",
        "\n",
        "    # Create instances of components\n",
        "    polar_code = PolarCode(block_length, info_bits)\n",
        "    scl_decoder = SCLDecoder(block_length, info_bits, list_sizes=[1, 8, 16])\n",
        "    # ... (Create or load ML-based decoder, if using) ...\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    ber_results = []\n",
        "    bler_results = []\n",
        "\n",
        "    # ... (Simulation parameters, component creation - same as before) ...\n",
        "    # Simulation loop\n",
        "    for channel_type in ['AWGN', 'Rayleigh']:\n",
        "        for snr_db in snr_range_db:\n",
        "            # ... (Data generation, encoding) ...\n",
        "\n",
        "            # Channel transmission\n",
        "            if channel_type == 'AWGN':\n",
        "                received_signal = ChannelSimulator.awgn_channel(encoded_bits, snr_range_db)\n",
        "            else:  # Rayleigh\n",
        "                received_signal = ChannelSimulator.rayleigh_fading_channel(encoded_bits, snr_range_db)\n",
        "    # --- Training and Validation ---\n",
        "    # 1. Generate training data (you might need to adapt this part)\n",
        "    num_training_samples = 500\n",
        "    X_train = np.random.randint(0, 2, size=(num_training_samples, block_length))\n",
        "    y_train = np.random.randint(0, 2, size=(num_training_samples, info_bits))  # Adjust if needed\n",
        "\n",
        "    # 2. Create and train the ML model (if using)\n",
        "    ml_trainer = MLTrainer(input_size=block_length, hidden_size=256, num_layers=2, output_size=info_bits)\n",
        "    train_loader, val_loader = ml_trainer.prepare_data(X_train, y_train)\n",
        "    train_losses, val_losses = ml_trainer.train(train_loader, val_loader, epochs=10)\n",
        "\n",
        "    # 3. Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('ML Model Training')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Simulation Loop (for both AWGN and Rayleigh) ---\n",
        "    channel_types = ['AWGN', 'Rayleigh']\n",
        "    ber_results = {channel_type: [] for channel_type in channel_types}\n",
        "    bler_results = {channel_type: [] for channel_type in channel_types}\n",
        "\n",
        "    for channel_type in channel_types:\n",
        "        for snr_db in snr_range_db:\n",
        "            # ... (Data generation, encoding - same as before) ...\n",
        "\n",
        "            # Channel Transmission (based on channel_type)\n",
        "            if channel_type == 'AWGN':\n",
        "                received_signal, _ = ChannelSimulator.awgn_channel(encoded_bits, snr_db)\n",
        "            else:  # Rayleigh\n",
        "                received_signal, _ = ChannelSimulator.rayleigh_fading_channel(encoded_bits, snr_db)\n",
        "\n",
        "            # ... (Decoding with SCL, ML, traditional - same as before) ...\n",
        "\n",
        "            # ... (Calculate and store BER/BLER for each channel type) ...\n",
        "\n",
        "        # --- Confusion Matrix (for ML decoder - adapt if needed) ---\n",
        "        if ml_trainer.model is not None:  # If you're using an ML decoder\n",
        "            y_pred = ml_trainer.model(torch.tensor(received_signal).float()).detach().numpy()\n",
        "            y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to 0/1\n",
        "            cm = confusion_matrix(data_bits, y_pred)\n",
        "            # ... (Plot or print the confusion matrix) ...\n",
        "           # Plotting the confusion matrix\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "            plt.xlabel(\"Predicted\")\n",
        "            plt.ylabel(\"Actual\")\n",
        "            plt.title(f\"Confusion Matrix ({channel_type}, SNR: {snr_db} dB)\")\n",
        "            plt.show()\n",
        "\n",
        "    # --- Plotting BER/BLER for both channels ---\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for channel_type in channel_types:\n",
        "        plt.semilogy(snr_range_db, ber_results[channel_type], marker='o', label=f'BER ({channel_type})')\n",
        "        plt.semilogy(snr_range_db, bler_results[channel_type], marker='x', label=f'BLER ({channel_type})')\n",
        "    # ... (Rest of the plotting code - same as before) ...\n",
        "    # Simulation loop\n",
        "    for snr_db in snr_range_db:\n",
        "        total_bit_errors = 0\n",
        "        total_block_errors = 0\n",
        "\n",
        "        for _ in range(num_trials):\n",
        "            # Generate random data\n",
        "            data_bits = np.random.randint(0, 2, info_bits)\n",
        "\n",
        "            # Encode the data\n",
        "            encoded_bits = polar_code.encode(data_bits)\n",
        "\n",
        "            # Transmit through the channel (choose AWGN or Rayleigh)\n",
        "            received_signal, _ = ChannelSimulator.awgn_channel(encoded_bits, snr_range_db)\n",
        "            # received_signal, _ = ChannelSimulator.rayleigh_fading_channel(encoded_bits, snr_db)\n",
        "\n",
        "            # Decode the received signal (choose decoding method)\n",
        "            decoded_bits_scl = scl_decoder.decode(received_signal, list_size=16)\n",
        "            # decoded_bits_ml = ... (Decoding with ML model) ...\n",
        "            # decoded_bits_traditional = traditional_decode(received_signal, block_length, info_bits)\n",
        "\n",
        "            # Calculate bit errors and block errors (for SCL decoder)\n",
        "            bit_errors = np.sum(data_bits != decoded_bits_scl)\n",
        "            total_bit_errors += bit_errors\n",
        "            if bit_errors > 0:\n",
        "                total_block_errors += 1\n",
        "\n",
        "            # ... (Calculate errors for other decoding methods, if needed) ...\n",
        "\n",
        "        # Calculate BER and BLER for the current SNR\n",
        "        ber = total_bit_errors / (num_trials * info_bits)\n",
        "        bler = total_block_errors / num_trials\n",
        "\n",
        "        # Store the results\n",
        "        ber_results.append(ber)\n",
        "        bler_results.append(bler)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"SNR: {snr_db} dB, BER: {ber:.4f}, BLER: {bler:.4f}\")\n",
        "\n",
        "        # --- Plotting BER/BLER ---\n",
        "\n",
        "# Plot BER for AWGN\n",
        "# --- Plotting BER ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(snr_range_db, ber_results['AWGN']['Traditional'], marker='o', label='Traditional (AWGN)')\n",
        "plt.semilogy(snr_range_db, ber_results['AWGN']['SCL_L1'], marker='x', label='SCL (L=1, AWGN)')\n",
        "plt.semilogy(snr_range_db, ber_results['AWGN']['SCL_L8'], marker='s', label='SCL (L=8, AWGN)')\n",
        "plt.semilogy(snr_range_db, ber_results['AWGN']['SCL_L16'], marker='^', label='SCL (L=16, AWGN)')\n",
        "plt.semilogy(snr_range_db, ber_results['AWGN']['ML'], marker='v', label='ML (AWGN)')\n",
        "\n",
        "plt.semilogy(snr_range_db, ber_results['Rayleigh']['Traditional'], marker='o', linestyle='--', label='Traditional (Rayleigh)')\n",
        "plt.semilogy(snr_range_db, ber_results['Rayleigh']['SCL_L1'], marker='x', linestyle='--', label='SCL (L=1, Rayleigh)')\n",
        "plt.semilogy(snr_range_db, ber_results['Rayleigh']['SCL_L8'], marker='s', linestyle='--', label='SCL (L=8, Rayleigh)')\n",
        "plt.semilogy(snr_range_db, ber_results['Rayleigh']['SCL_L16'], marker='^', linestyle='--', label='SCL (L=16, Rayleigh)')\n",
        "plt.semilogy(snr_range_db, ber_results['Rayleigh']['ML'], marker='v', linestyle='--', label='ML (Rayleigh)')\n",
        "\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"BER\")\n",
        "plt.title(\"BER Performance\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Plotting BLER ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogy(snr_range_db, bler_results['AWGN']['Traditional'], marker='o', label='Traditional (AWGN)')\n",
        "plt.semilogy(snr_range_db, bler_results['AWGN']['SCL_L1'], marker='x', label='SCL (L=1, AWGN)')\n",
        "plt.semilogy(snr_range_db, bler_results['AWGN']['SCL_L8'], marker='s', label='SCL (L=8, AWGN)')\n",
        "plt.semilogy(snr_range_db, bler_results['AWGN']['SCL_L16'], marker='^', label='SCL (L=16, AWGN)')\n",
        "plt.semilogy(snr_range_db, bler_results['AWGN']['ML'], marker='v', label='ML (AWGN)')\n",
        "\n",
        "plt.semilogy(snr_range_db, bler_results['Rayleigh']['Traditional'], marker='o', linestyle='--', label='Traditional (Rayleigh)')\n",
        "plt.semilogy(snr_range_db, bler_results['Rayleigh']['SCL_L1'], marker='x', linestyle='--', label='SCL (L=1, Rayleigh)')\n",
        "plt.semilogy(snr_range_db, bler_results['Rayleigh']['SCL_L8'], marker='s', linestyle='--', label='SCL (L=8, Rayleigh)')\n",
        "plt.semilogy(snr_range_db, bler_results['Rayleigh']['SCL_L16'], marker='^', linestyle='--', label='SCL (L=16, Rayleigh)')\n",
        "plt.semilogy(snr_range_db, bler_results['Rayleigh']['ML'], marker='v', linestyle='--', label='ML (Rayleigh)')\n",
        "\n",
        "plt.xlabel(\"SNR (dB)\")\n",
        "plt.ylabel(\"BLER\")  # Changed to BLER\n",
        "plt.title(\"BLER Performance\")  # Changed to BLER\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "F_V-Xysf7Qxw",
        "outputId": "3f4473a2-edc1-45d8-e8f0-fc9ebc25ef32"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'snr_range_db' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-9e6c20a6a962>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;31m# --- Plotting BER ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnr_range_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mber_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWGN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Traditional'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Traditional (AWGN)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnr_range_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mber_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWGN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SCL_L1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SCL (L=1, AWGN)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnr_range_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mber_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWGN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SCL_L8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SCL (L=8, AWGN)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'snr_range_db' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
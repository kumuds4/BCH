{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# polar_code_generator.py\n",
        "!pip install ipympl\n",
        "!pip install scikit-learn\n",
        "!pip install numpy torch matplotlib scikit-learn\n",
        "!pip install -U matplotlib\n",
        "import numpy as np\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib\n",
        "#import ipympl\n",
        "matplotlib.use('Agg')  # Or try 'TkAgg', 'Qt5Agg'\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Add this line\n",
        "import logging\n",
        "import traceback\n",
        "# Import custom modules\n",
        "#from polar_code_generator import PolarCodeGenerator\n",
        "#from channel_simulator import ChannelSimulator\n",
        "#from neural_decoder import NeuralDecoder\n",
        "#from rnn_trainer import RNNTrainer\n",
        "#from ml_trainer import MLTrainer\n",
        "#from dataset_preparation import (\n",
        " #   prepare_polar_dataset,\n",
        "  #  prepare_dataset_for_training,\n",
        " #   normalize_features\n",
        "#)\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N=128, K=64):\n",
        "        \"\"\"\n",
        "        Initialize Polar Code Generator with CRC-7 Polynomial\n",
        "\n",
        "        Args:\n",
        "            N (int): Total block length\n",
        "            K (int): Information bit length\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.R = K / N  # Code rate\n",
        "\n",
        "        # CRC-7 Polynomial (Standard polynomial for communication)\n",
        "        # x^7 + x^6 + x^5 + x^2 + x^0\n",
        "        self.crc_polynomial = 0b10100011  # CRC-7 polynomial\n",
        "        self.crc_order = 7  # 7-bit CRC\n",
        "\n",
        "    def crc_generate(self, data):\n",
        "        \"\"\"\n",
        "        Generate CRC-7 checksum\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Input data bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: CRC checksum bits\n",
        "        \"\"\"\n",
        "        # Convert input to numpy array\n",
        "        data = np.asarray(data)\n",
        "\n",
        "        # Create data with zero padding for CRC\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.crc_order, dtype=int)])\n",
        "\n",
        "        # CRC calculation\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.crc_order + 1):\n",
        "                    data_with_zeros[i+j] ^= ((self.crc_polynomial >> j) & 1)\n",
        "\n",
        "        # Return the last 'crc_order' bits as CRC\n",
        "        return data_with_zeros[-self.crc_order:]\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        \"\"\"\n",
        "        Generate random information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Random information bits\n",
        "        \"\"\"\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Systematic Polar Encoding with CRC\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits to encode\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Generate CRC\n",
        "        crc_bits = self.crc_generate(info_bits)\n",
        "\n",
        "        # Combine info bits and CRC\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Initialize codeword\n",
        "        encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Place information bits\n",
        "        encoded_bits[:len(full_info)] = full_info\n",
        "\n",
        "        return encoded_bits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def crc_verify(self, data, received_crc):\n",
        "        \"\"\"\n",
        "        Verify CRC-7 checksum\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Original data bits\n",
        "            received_crc (np.ndarray): Received CRC checksum\n",
        "\n",
        "        Returns:\n",
        "            bool: True if CRC is valid, False otherwise\n",
        "        \"\"\"\n",
        "        # Combine data and received CRC\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "\n",
        "        # CRC verification\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.crc_order + 1):\n",
        "                    full_data[i+j] ^= ((self.crc_polynomial >> j) & 1)\n",
        "\n",
        "        # Check if the last 'crc_order' bits are zero\n",
        "        return np.all(full_data[-self.crc_order:] == 0)\n",
        "\n",
        "    def bhattacharyya_parameter(self, W, n):\n",
        "        \"\"\"\n",
        "        Compute Bhattacharyya parameter for channel polarization\n",
        "\n",
        "        Args:\n",
        "            W (float): Initial channel crossover probability\n",
        "            n (int): Recursion depth\n",
        "\n",
        "        Returns:\n",
        "            float: Bhattacharyya parameter\n",
        "        \"\"\"\n",
        "        if n == 0:\n",
        "            return W\n",
        "\n",
        "        # Recursive Bhattacharyya parameter computation\n",
        "        W_used = self.bhattacharyya_parameter(W, n-1)\n",
        "        W_transform = 2 * (W_used ** 2) - (W_used ** 4)\n",
        "\n",
        "        return W_transform\n",
        "\n",
        "def generate_polar_code_matrix(self):\n",
        "    \"\"\"\n",
        "    Generate polar code matrix using Bhattacharyya parameter\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Indices of information bit positions\n",
        "    \"\"\"\n",
        "    # Initial channel crossover probability (Binary Symmetric Channel)\n",
        "    W = 0.5\n",
        "\n",
        "    # Compute channel capacities\n",
        "    channel_capacities = []\n",
        "    for _ in range(self.N):\n",
        "        # Compute Bhattacharyya parameter\n",
        "        capacity = self.bhattacharyya_parameter(W, int(math.log2(self.N)))\n",
        "        channel_capacities.append(capacity)\n",
        "\n",
        "    # Sort channel capacities\n",
        "    sorted_indices = np.argsort(channel_capacities)\n",
        "\n",
        "    # Select best channels for information bits\n",
        "    info_indices = sorted_indices[self.N - self.K:]\n",
        "\n",
        "    return info_indices\n",
        "\n",
        "def polar_encode(self, info_bits):\n",
        "    \"\"\"\n",
        "    Systematic Polar Encoding with CRC\n",
        "\n",
        "    Args:\n",
        "        info_bits (np.ndarray): Information bits to encode\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Encoded codeword\n",
        "    \"\"\"\n",
        "    # Generate CRC\n",
        "    crc_bits = self.crc_generate(info_bits)\n",
        "\n",
        "    # Combine info bits and CRC\n",
        "    full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "    # Initialize codeword\n",
        "    encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "    # Get indices for information bits\n",
        "    info_indices = self.generate_polar_code_matrix()\n",
        "\n",
        "    # Ensure we don't exceed available indices\n",
        "    max_info_length = min(len(full_info), len(info_indices))\n",
        "\n",
        "    # Place information bits at selected indices\n",
        "    encoded_bits[info_indices[:max_info_length]] = full_info[:max_info_length]\n",
        "\n",
        "    return encoded_bits\n",
        "\n",
        "def bhattacharyya_parameter(self, W, n):\n",
        "    \"\"\"\n",
        "    Compute Bhattacharyya parameter for channel polarization\n",
        "\n",
        "    Args:\n",
        "        W (float): Initial channel crossover probability\n",
        "        n (int): Recursion depth\n",
        "\n",
        "    Returns:\n",
        "        float: Bhattacharyya parameter\n",
        "    \"\"\"\n",
        "    if n == 0:\n",
        "        return W\n",
        "\n",
        "    # Recursive Bhattacharyya parameter computation\n",
        "    W_used = self.bhattacharyya_parameter(W, n-1)\n",
        "    W_transform = 2 * (W_used ** 2) - (W_used ** 4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        \"\"\"\n",
        "        Generate random information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Random information bits\n",
        "        \"\"\"\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Systematic Polar Encoding with CRC\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits to encode\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Generate CRC\n",
        "        crc_bits = self.crc_generate(info_bits)\n",
        "\n",
        "        # Combine info bits and CRC\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Initialize codeword\n",
        "        encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Get indices for information bits\n",
        "        info_indices = self.generate_polar_code_matrix()\n",
        "        info_indices = info_indices[:len(full_info)]\n",
        "\n",
        "        # Place information bits at selected indices\n",
        "        encoded_bits[info_indices] = full_info\n",
        "\n",
        "        return encoded_bits\n",
        "\n",
        "# Part 1: Channel Simulator\n",
        "class ChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Initialize Channel Simulator\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Type of channel (AWGN or Rayleigh)\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def transmit(self, signal, snr):\n",
        "        \"\"\"\n",
        "        Transmit signal through channel\n",
        "\n",
        "        Args:\n",
        "            signal (np.ndarray): Input signal\n",
        "            snr (float): Signal-to-Noise Ratio in dB\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received noisy signal\n",
        "        \"\"\"\n",
        "        # Convert SNR to linear scale\n",
        "        snr_linear = 10 ** (snr / 10)\n",
        "\n",
        "        # Noise standard deviation\n",
        "        noise_std = np.sqrt(1 / (2 * snr_linear))\n",
        "\n",
        "        # Generate noise\n",
        "        if self.channel_type == 'AWGN':\n",
        "            # Additive White Gaussian Noise\n",
        "            noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        elif self.channel_type == 'Rayleigh':\n",
        "            # Rayleigh Fading Channel\n",
        "            fading = np.random.rayleigh(scale=1, size=signal.shape)\n",
        "            noise = fading * np.random.normal(0, noise_std, signal.shape)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "        return signal + noise\n",
        "\n",
        "\n",
        "# channel_simulator.py\n",
        "\n",
        "\n",
        "class ChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Initialize Channel Simulator\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Type of channel (AWGN or Rayleigh)\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def transmit(self, signal, snr):\n",
        "        \"\"\"\n",
        "        Transmit signal through channel\n",
        "\n",
        "        Args:\n",
        "            signal (np.ndarray): Input signal\n",
        "            snr (float): Signal-to-Noise Ratio in dB\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received noisy signal\n",
        "        \"\"\"\n",
        "        # Convert SNR to linear scale\n",
        "        snr_linear = 10 ** (snr / 10)\n",
        "\n",
        "        # Noise standard deviation\n",
        "        noise_std = np.sqrt(1 / (2 * snr_linear))\n",
        "\n",
        "        # Generate noise\n",
        "        if self.channel_type == 'AWGN':\n",
        "            # Additive White Gaussian Noise\n",
        "            noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        elif self.channel_type == 'Rayleigh':\n",
        "            # Rayleigh Fading Channel\n",
        "            fading = np.random.rayleigh(scale=1, size=signal.shape)\n",
        "            noise = fading * np.random.normal(0, noise_std, signal.shape)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "        return signal + noise\n",
        "\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize Trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): Neural network model\n",
        "            learning_rate (float): Optimization learning rate\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20, batch_size=64):\n",
        "        \"\"\"\n",
        "        Train the model\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features\n",
        "            y_train (np.ndarray): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Training batch size\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Convert to torch tensors\n",
        "        X_train = torch.FloatTensor(X_train)\n",
        "        y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            outputs = self.model(X_train)\n",
        "            loss = self.criterion(outputs, y_train)\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        return train_losses\n",
        "\n",
        "    def plot_training_performance(self, train_losses, title='Training Performance'):\n",
        "        \"\"\"\n",
        "        Plot training performance\n",
        "\n",
        "        Args:\n",
        "            train_losses (list): Training losses\n",
        "            title (str): Plot title\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(train_losses)\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "class RNNTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize RNN Trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): RNN neural network model\n",
        "            learning_rate (float): Optimization learning rate\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20, batch_size=64, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the RNN model\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features\n",
        "            y_train (np.ndarray): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Training batch size\n",
        "            validation_split (float): Proportion of data for validation\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Convert to torch tensors and move to device\n",
        "        X_train = torch.FloatTensor(X_train).to(self.device)\n",
        "        y_train = torch.FloatTensor(y_train).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # Reshape input for RNN if needed\n",
        "        if X_train.dim() == 2:\n",
        "            X_train = X_train.unsqueeze(2)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X_train))\n",
        "        X_val, y_val = X_train[train_size:], y_train[train_size:]\n",
        "        X_train, y_train = X_train[:train_size], y_train[:train_size]\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_train_losses = []\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[i:i+batch_size]\n",
        "                batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_train_losses.append(loss.item())\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val)\n",
        "                val_loss = self.criterion(val_outputs, y_val)\n",
        "\n",
        "            # Record losses\n",
        "            avg_train_loss = np.mean(epoch_train_losses)\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def plot_training_performance(self, train_losses, val_losses=None, title='RNN Training Performance'):\n",
        "        \"\"\"\n",
        "        Plot RNN training performance\n",
        "\n",
        "        Args:\n",
        "            train_losses (list): Training losses\n",
        "            val_losses (list, optional): Validation losses\n",
        "            title (str): Plot title\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Training Loss Plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.title(f'{title} - Training Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Validation Loss Plot\n",
        "        if val_losses:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "            plt.title(f'{title} - Validation Loss')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features for prediction\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities\n",
        "        \"\"\"\n",
        "        # Ensure input is a torch tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X).to(self.device)\n",
        "\n",
        "        # Reshape input for RNN if needed\n",
        "        if X.dim() == 2:\n",
        "            X = X.unsqueeze(2)\n",
        "\n",
        "        # Prediction\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X)\n",
        "\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "class MLTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize ML (MLP) Trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): Multi-Layer Perceptron model\n",
        "            learning_rate (float): Optimization learning rate\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20, batch_size=64, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the ML model\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features\n",
        "            y_train (np.ndarray): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Training batch size\n",
        "            validation_split (float): Proportion of data for validation\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Convert to torch tensors and move to device\n",
        "        X_train = torch.FloatTensor(X_train).to(self.device)\n",
        "        y_train = torch.FloatTensor(y_train).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X_train))\n",
        "        X_val, y_val = X_train[train_size:], y_train[train_size:]\n",
        "        X_train, y_train = X_train[:train_size], y_train[:train_size]\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_train_losses = []\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[i:i+batch_size]\n",
        "                batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_train_losses.append(loss.item())\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val)\n",
        "                val_loss = self.criterion(val_outputs, y_val)\n",
        "\n",
        "            # Record losses\n",
        "            avg_train_loss = np.mean(epoch_train_losses)\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def plot_training_performance(self, train_losses, val_losses=None, title='ML Training Performance'):\n",
        "        \"\"\"\n",
        "        Plot ML training performance\n",
        "\n",
        "        Args:\n",
        "            train_losses (list): Training losses\n",
        "            val_losses (list, optional): Validation losses\n",
        "            title (str): Plot title\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Training Loss Plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.title(f'{title} - Training Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Validation Loss Plot\n",
        "        if val_losses:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "            plt.title(f'{title} - Validation Loss')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features for prediction\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities\n",
        "        \"\"\"\n",
        "        # Ensure input is a torch tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X).to(self.device)\n",
        "\n",
        "        # Prediction\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X)\n",
        "\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, feature_type='codeword'):\n",
        "    \"\"\"\n",
        "    Advanced dataset preparation for Polar Codes\n",
        "\n",
        "    Args:\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "        num_samples (int): Number of samples to generate\n",
        "        feature_type (str): Type of feature extraction\n",
        "\n",
        "    Returns:\n",
        "        tuple: Features and labels\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate info bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Encode\n",
        "        codeword = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # Feature extraction\n",
        "        if feature_type == 'codeword':\n",
        "            # Use full codeword as features\n",
        "            features = codeword\n",
        "        elif feature_type == 'statistical':\n",
        "            # Statistical features\n",
        "            features = [\n",
        "                np.mean(codeword),\n",
        "                np.std(codeword),\n",
        "                np.sum(codeword),\n",
        "                np.count_nonzero(codeword)\n",
        "            ]\n",
        "        elif feature_type == 'frequency':\n",
        "            # Frequency-based features\n",
        "            unique, counts = np.unique(codeword, return_counts=True)\n",
        "            features = dict(zip(unique, counts))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported feature type: {feature_type}\")\n",
        "\n",
        "        X.append(features)\n",
        "\n",
        "        # Binary classification label (e.g., based on mean)\n",
        "        y.append(1 if np.mean(codeword) > 0.5 else 0)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_dataset_for_training(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Prepare dataset for neural network training\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Features\n",
        "        y (np.ndarray): Labels\n",
        "        test_size (float): Proportion of test data\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        tuple: Train and test splits\n",
        "    \"\"\"\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def normalize_features(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Normalize features using min-max scaling\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Training features\n",
        "        X_test (np.ndarray): Test features\n",
        "\n",
        "    Returns:\n",
        "        tuple: Normalized training and test features\n",
        "    \"\"\"\n",
        "    # Compute min and max for each feature\n",
        "    min_vals = np.min(X_train, axis=0)\n",
        "    max_vals = np.max(X_train, axis=0)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    max_vals[max_vals == min_vals] = 1\n",
        "\n",
        "    # Normalize\n",
        "    X_train_normalized = (X_train - min_vals) / (max_vals - min_vals)\n",
        "    X_test_normalized = (X_test - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "    return X_train_normalized, X_test_normalized\n",
        "\n",
        "def create_rnn_model(input_size):\n",
        "    \"\"\"\n",
        "    Create RNN Model\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Input feature dimension\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: RNN model\n",
        "    \"\"\"\n",
        "    class RNNDecoder(nn.Module):\n",
        "        def __init__(self, input_size):\n",
        "            super(RNNDecoder, self).__init__()\n",
        "\n",
        "            self.rnn = nn.Sequential(\n",
        "                nn.LSTM(\n",
        "                    input_size=input_size,\n",
        "                    hidden_size=64,\n",
        "                    num_layers=2,\n",
        "                    batch_first=True\n",
        "                ),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3)\n",
        "            )\n",
        "\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Ensure input is 3D for RNN\n",
        "            if x.dim() == 2:\n",
        "                x = x.unsqueeze(2)\n",
        "\n",
        "            # RNN processing\n",
        "            rnn_out, _ = self.rnn(x)\n",
        "\n",
        "            # Take the last time step\n",
        "            out = rnn_out[:, -1, :]\n",
        "\n",
        "            # Final classification\n",
        "            return self.fc(out)\n",
        "\n",
        "    return RNNDecoder(input_size)\n",
        "def compute_channel_performance(model, channel, snr_range, polar_code_gen):\n",
        "    \"\"\"\n",
        "    Compute Bit Error Rate (BER) and Block Error Rate (BLER)\n",
        "\n",
        "    Args:\n",
        "        model: Trained decoder model\n",
        "        channel: Channel simulator\n",
        "        snr_range (np.ndarray): SNR range\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: BER and BLER arrays\n",
        "    \"\"\"\n",
        "    ber_values = []\n",
        "    bler_values = []\n",
        "\n",
        "    for snr in snr_range:\n",
        "        block_errors = 0\n",
        "        bit_errors = 0\n",
        "        total_blocks = 100\n",
        "\n",
        "        for _ in range(total_blocks):\n",
        "            # Generate info bits\n",
        "            info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "            # Encode\n",
        "            encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "            # Transmit through channel\n",
        "            received_signal = channel.transmit(encoded_signal, snr)\n",
        "\n",
        "            # Decode\n",
        "            decoded_bits = model.predict(received_signal.reshape(1, -1))\n",
        "            decoded_bits = (decoded_bits > 0.5).astype(int).flatten()\n",
        "\n",
        "            # Compute errors\n",
        "            block_error = not np.array_equal(info_bits, decoded_bits)\n",
        "            bit_error = np.sum(info_bits != decoded_bits)\n",
        "\n",
        "            block_errors += block_error\n",
        "            bit_errors += bit_error\n",
        "\n",
        "        # Compute BER and BLER\n",
        "        ber = bit_errors / (total_blocks * len(info_bits))\n",
        "        bler = block_errors / total_blocks\n",
        "\n",
        "        ber_values.append(ber)\n",
        "        bler_values.append(bler)\n",
        "\n",
        "    return np.array(ber_values), np.array(bler_values)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True labels\n",
        "        y_pred (array-like): Predicted labels\n",
        "        title (str): Plot title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_error_performance(snr_range, ber_data, bler_data):\n",
        "    \"\"\"\n",
        "    Plot BER and BLER performance\n",
        "\n",
        "    Args:\n",
        "        snr_range (np.ndarray): SNR range\n",
        "        ber_data (dict): Bit Error Rate data\n",
        "        bler_data (dict): Block Error Rate data\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for model, ber in ber_data.items():\n",
        "        plt.semilogy(snr_range, ber, label=f'{model} BER')\n",
        "    plt.title('Bit Error Rate Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for model, bler in bler_data.items():\n",
        "        plt.semilogy(snr_range, bler, label=f'{model} BLER')\n",
        "    plt.title('Block Error Rate Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('error_performance.png')\n",
        "    plt.close()\n",
        "\n",
        "# The main function\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Comprehensive Polar Code Simulation and Machine Learning Decoder Evaluation\n",
        "    \"\"\"\n",
        "    # Simple logging function\n",
        "    def log(message, level='INFO'):\n",
        "        \"\"\"\n",
        "        Simple logging function\n",
        "\n",
        "        Args:\n",
        "            message (str): Log message\n",
        "            level (str): Log level (INFO, ERROR, WARNING)\n",
        "        \"\"\"\n",
        "        print(f\"{level}: {message}\")\n",
        "\n",
        "    # RNN Decoder Definition\n",
        "    class RNNDecoder(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size=64, num_layers=2):\n",
        "            \"\"\"\n",
        "            RNN Decoder for Polar Codes\n",
        "\n",
        "            Args:\n",
        "                input_size (int): Input feature dimension\n",
        "                hidden_size (int): RNN hidden layer size\n",
        "                num_layers (int): Number of RNN layers\n",
        "            \"\"\"\n",
        "            super(RNNDecoder, self).__init__()\n",
        "\n",
        "            # LSTM Layer\n",
        "            self.lstm = nn.LSTM(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=num_layers,\n",
        "                batch_first=True\n",
        "            )\n",
        "\n",
        "            # Fully Connected Layers\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(hidden_size, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3),\n",
        "                nn.Linear(32, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            \"\"\"\n",
        "            Forward pass\n",
        "\n",
        "            Args:\n",
        "                x (torch.Tensor): Input tensor\n",
        "\n",
        "            Returns:\n",
        "                torch.Tensor: Output predictions\n",
        "            \"\"\"\n",
        "            # Ensure input is 3D (batch_size, sequence_length, input_size)\n",
        "            if x.dim() == 2:\n",
        "                x = x.unsqueeze(2)\n",
        "\n",
        "            # Ensure input has correct number of features\n",
        "            if x.size(-1) != 4:\n",
        "                # Pad or repeat features to match expected input size\n",
        "                x = x.repeat(1, 1, 4)[:, :, :4]\n",
        "\n",
        "            # LSTM processing\n",
        "            lstm_out, _ = self.lstm(x)\n",
        "\n",
        "            # Take the last time step\n",
        "            out = lstm_out[:, -1, :]\n",
        "\n",
        "            # Final classification\n",
        "            return self.fc(out)\n",
        "\n",
        "    # ML Decoder Definition\n",
        "    class MLDecoder(nn.Module):\n",
        "        def __init__(self, input_size, hidden_layers=[128, 64], output_size=1):\n",
        "            \"\"\"\n",
        "            Multi-Layer Perceptron Decoder\n",
        "\n",
        "            Args:\n",
        "                input_size (int): Input feature dimension\n",
        "                hidden_layers (list): Hidden layer sizes\n",
        "                output_size (int): Output dimension\n",
        "            \"\"\"\n",
        "            super(MLDecoder, self).__init__()\n",
        "\n",
        "            layers = []\n",
        "            prev_size = input_size\n",
        "\n",
        "            for hidden_size in hidden_layers:\n",
        "                layers.extend([\n",
        "                    nn.Linear(prev_size, hidden_size),\n",
        "                    nn.BatchNorm1d(hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3)\n",
        "                ])\n",
        "                prev_size = hidden_size\n",
        "\n",
        "            layers.append(nn.Linear(prev_size, output_size))\n",
        "            layers.append(nn.Sigmoid())\n",
        "\n",
        "            self.model = nn.Sequential(*layers)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.model(x)\n",
        "\n",
        "    try:\n",
        "        # Global Simulation Parameters\n",
        "        BLOCK_LENGTH = 128\n",
        "        INFO_BITS = 64\n",
        "        LEARNING_RATE = 1e-3\n",
        "        EPOCHS = 100\n",
        "        BATCH_SIZE = 64\n",
        "        NUM_SAMPLES = 40000\n",
        "        SNR_RANGE = np.linspace(0, 10, 10)\n",
        "        LIST_SIZES = [1, 8, 16]\n",
        "        NUM_TRIALS = 3000\n",
        "\n",
        "        # Device Configuration\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        log(f\" Using Device: {device}\")\n",
        "\n",
        "        # Diagnostic Print\n",
        "        log(\" Polar Code Simulation Initialization\")\n",
        "        log(f\"Block Length: {BLOCK_LENGTH}\")\n",
        "        log(f\"Information Bits: {INFO_BITS}\")\n",
        "        log(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "        log(f\"Epochs: {EPOCHS}\")\n",
        "        log(f\"Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "        # 1. Polar Code Generator\n",
        "        polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "        log(\" Polar Code Generator Initialized\")\n",
        "\n",
        "        # 2. Prepare Dataset\n",
        "        def prepare_dataset(polar_code_gen, num_samples):\n",
        "            \"\"\"\n",
        "            Prepare dataset for training\n",
        "            \"\"\"\n",
        "            X = []\n",
        "            y = []\n",
        "\n",
        "            for _ in range(num_samples):\n",
        "                # Generate info bits\n",
        "                info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "                # Encode\n",
        "                codeword = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "                # Extract features\n",
        "                X.append(codeword)\n",
        "                y.append(1 if np.mean(codeword) > 0.5 else 0)\n",
        "\n",
        "            return np.array(X), np.array(y)\n",
        "\n",
        "        # Generate Dataset\n",
        "        X, y = prepare_dataset(polar_code_gen, num_samples=NUM_SAMPLES)\n",
        "\n",
        "        # Validate dataset\n",
        "        if X is None or y is None:\n",
        "            raise ValueError(\"Dataset preparation failed\")\n",
        "\n",
        "        log(f\"Dataset Prepared: X shape {X.shape}, y shape {y.shape}\")\n",
        "\n",
        "        # 3. Split Dataset\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        X_train = torch.FloatTensor(X_train).to(device)\n",
        "        X_test = torch.FloatTensor(X_test).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device).unsqueeze(1)\n",
        "        y_test = torch.FloatTensor(y_test).to(device).unsqueeze(1)\n",
        "\n",
        "        log(\" Dataset Split Completed\")\n",
        "\n",
        "        # 4. RNN Decoder Training\n",
        "        rnn_model = RNNDecoder(\n",
        "            input_size=4,  # Adjusted input size\n",
        "            hidden_size=64,\n",
        "            num_layers=2\n",
        "        ).to(device)\n",
        "\n",
        "        # Create RNN Trainer (you'll need to define this class)\n",
        "        class RNNTrainer:\n",
        "            def __init__(self, model, learning_rate=1e-3):\n",
        "                self.model = model\n",
        "                self.criterion = nn.BCELoss()\n",
        "                self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            def train(self, X_train, y_train, epochs=20, batch_size=64):\n",
        "                train_losses = []\n",
        "                for epoch in range(epochs):\n",
        "                    self.model.train()\n",
        "                    total_loss = 0\n",
        "\n",
        "                    for i in range(0, len(X_train), batch_size):\n",
        "                        batch_X = X_train[i:i+batch_size]\n",
        "                        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "                        self.optimizer.zero_grad()\n",
        "                        outputs = self.model(batch_X)\n",
        "                        loss = self.criterion(outputs, batch_y)\n",
        "                        loss.backward()\n",
        "                        self.optimizer.step()\n",
        "\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    avg_loss = total_loss / (len(X_train) // batch_size)\n",
        "                    train_losses.append(avg_loss)\n",
        "                    log(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                return train_losses\n",
        "\n",
        "            def predict(self, X):\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    X_tensor = torch.FloatTensor(X).to(device)\n",
        "                    predictions = self.model(X_tensor)\n",
        "                return predictions.cpu().numpy()\n",
        "\n",
        "        rnn_trainer = RNNTrainer(rnn_model, learning_rate=LEARNING_RATE)\n",
        "        rnn_train_losses = rnn_trainer.train(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "        log(\" RNN Decoder Training Completed\")\n",
        "\n",
        "        # 5. ML Decoder Training\n",
        "        ml_model = MLDecoder(\n",
        "            input_size=X_train.shape[1],\n",
        "            hidden_layers=[128, 256, 128],\n",
        "            output_size=1\n",
        "        ).to(device)\n",
        "\n",
        "        # Create ML Trainer (similar to RNN Trainer)\n",
        "        class MLTrainer:\n",
        "            def __init__(self, model, learning_rate=1e-3):\n",
        "                self.model = model\n",
        "                self.criterion = nn.BCELoss()\n",
        "                self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            def train(self, X_train, y_train, epochs=60, batch_size=64):\n",
        "                train_losses = []\n",
        "                for epoch in range(epochs):\n",
        "                    self.model.train()\n",
        "                    total_loss = 0\n",
        "\n",
        "                    for i in range(0, len(X_train), batch_size):\n",
        "                        batch_X = X_train[i:i+batch_size]\n",
        "                        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "                        self.optimizer.zero_grad()\n",
        "                        outputs = self.model(batch_X)\n",
        "                        loss = self.criterion(outputs, batch_y)\n",
        "                        loss.backward()\n",
        "                        self.optimizer.step()\n",
        "\n",
        "                        total_loss += loss.item()\n",
        "\n",
        "                    avg_loss = total_loss / (len(X_train) // batch_size)\n",
        "                    train_losses.append(avg_loss)\n",
        "                    log(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "                return train_losses\n",
        "\n",
        "            def predict(self, X):\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    X_tensor = torch.FloatTensor(X).to(device)\n",
        "                    predictions = self.model(X_tensor)\n",
        "                return predictions.cpu().numpy()\n",
        "\n",
        "        ml_trainer = MLTrainer(ml_model, learning_rate=LEARNING_RATE)\n",
        "        ml_train_losses = ml_trainer.train(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "        log(\" ML Decoder Training Completed\")\n",
        "\n",
        "        # 6. Visualization and Performance Analysis\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(rnn_train_losses, label='RNN Training Loss')\n",
        "        plt.title('RNN Decoder Training Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(ml_train_losses, label='ML Training Loss')\n",
        "        plt.title('ML Decoder Training Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_performance.png')\n",
        "        plt.close()\n",
        "\n",
        "        log(\"Simulation Complete!\")\n",
        "         # Channel Performance Computation Function\n",
        "        def compute_channel_performance(trainer, channel, snr_range, polar_code_gen):\n",
        "            \"\"\"\n",
        "            Compute Bit Error Rate (BER) and Block Error Rate (BLER)\n",
        "\n",
        "            Args:\n",
        "                trainer: Trained model trainer\n",
        "                channel: Channel simulator\n",
        "                snr_range (np.ndarray): SNR range\n",
        "                polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "\n",
        "            Returns:\n",
        "                tuple: BER and BLER arrays\n",
        "            \"\"\"\n",
        "            ber_values = []\n",
        "            bler_values = []\n",
        "\n",
        "            for snr in snr_range:\n",
        "                block_errors = 0\n",
        "                bit_errors = 0\n",
        "                total_blocks = 100\n",
        "\n",
        "                for _ in range(total_blocks):\n",
        "                    # Generate info bits\n",
        "                    info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "                    # Encode\n",
        "                    encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "                    # Transmit through channel\n",
        "                    received_signal = channel.transmit(encoded_signal, snr)\n",
        "\n",
        "                    # Decode\n",
        "                    decoded_bits = trainer.predict(received_signal.reshape(1, -1))\n",
        "                    decoded_bits = (decoded_bits > 0.5).astype(int).flatten()\n",
        "\n",
        "                    # Compute errors\n",
        "                    block_error = not np.array_equal(info_bits, decoded_bits)\n",
        "                    bit_error = np.sum(info_bits != decoded_bits)\n",
        "\n",
        "                    block_errors += block_error\n",
        "                    bit_errors += bit_error\n",
        "\n",
        "                # Compute BER and BLER\n",
        "                ber = bit_errors / (total_blocks * len(info_bits))\n",
        "                bler = block_errors / total_blocks\n",
        "\n",
        "                ber_values.append(ber)\n",
        "                bler_values.append(bler)\n",
        "\n",
        "            return np.array(ber_values), np.array(bler_values)\n",
        "\n",
        "        # Channel Simulators\n",
        "        awgn_channel = ChannelSimulator(channel_type='AWGN')\n",
        "        rayleigh_channel = ChannelSimulator(channel_type='Rayleigh')\n",
        "\n",
        "        # Compute Channel Performance\n",
        "        rnn_ber_awgn, rnn_bler_awgn = compute_channel_performance(\n",
        "            rnn_trainer,\n",
        "            awgn_channel,\n",
        "            SNR_RANGE,\n",
        "            polar_code_gen\n",
        "        )\n",
        "\n",
        "        ml_ber_awgn, ml_bler_awgn = compute_channel_performance(\n",
        "            ml_trainer,\n",
        "            awgn_channel,\n",
        "            SNR_RANGE,\n",
        "            polar_code_gen\n",
        "        )\n",
        "\n",
        "        rnn_ber_rayleigh, rnn_bler_rayleigh = compute_channel_performance(\n",
        "            rnn_trainer,\n",
        "            rayleigh_channel,\n",
        "            SNR_RANGE,\n",
        "            polar_code_gen\n",
        "        )\n",
        "\n",
        "        ml_ber_rayleigh, ml_bler_rayleigh = compute_channel_performance(\n",
        "            ml_trainer,\n",
        "            rayleigh_channel,\n",
        "            SNR_RANGE,\n",
        "            polar_code_gen\n",
        "        )\n",
        "\n",
        "        # BER and BLER Performance Visualization\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # AWGN Channel BER\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.semilogy(SNR_RANGE, rnn_ber_awgn, label='RNN BER', marker='o')\n",
        "        plt.semilogy(SNR_RANGE, ml_ber_awgn, label='ML BER', marker='s')\n",
        "        plt.title('BER - AWGN Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Bit Error Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # AWGN Channel BLER\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.semilogy(SNR_RANGE, rnn_bler_awgn, label='RNN BLER', marker='o')\n",
        "        plt.semilogy(SNR_RANGE, ml_bler_awgn, label='ML BLER', marker='s')\n",
        "        plt.title('BLER - AWGN Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Block Error Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Rayleigh Channel BER\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.semilogy(SNR_RANGE, rnn_ber_rayleigh, label='RNN BER', marker='^')\n",
        "        plt.semilogy(SNR_RANGE, ml_ber_rayleigh, label='ML BER', marker='v')\n",
        "        plt.title('BER - Rayleigh Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Bit Error Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Rayleigh Channel BLER\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.semilogy(SNR_RANGE, rnn_bler_rayleigh, label='RNN BLER', marker='^')\n",
        "        plt.semilogy(SNR_RANGE, ml_bler_rayleigh, label='ML BLER', marker='v')\n",
        "        plt.title('BLER - Rayleigh Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Block Error Rate')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('ber_bler_performance.png')\n",
        "        plt.close()\n",
        "\n",
        "        log(\" BER/BLER Performance Plots Saved\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\" Comprehensive Simulation Error: {e}\", level='ERROR')\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "       # Add this section after the BER/BLER performance visualization\n",
        "\n",
        "       # plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Ensure predictions and labels are properly converted\n",
        "# Convert y_test to numpy first\n",
        "        y_test_numpy = y_test.cpu().numpy().flatten()\n",
        "\n",
        "# RNN Predictions\n",
        "        rnn_predictions = rnn_trainer.predict(X_test.cpu().numpy())\n",
        "        rnn_pred_classes = (rnn_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "        # Pot confusion matrix\n",
        "        plt.figure(figsize=(15, 6))\n",
        "# RNN Decoder Confusion Matrix\n",
        "        plt.subplot(1, 2, 1)\n",
        "        rnn_cm = confusion_matrix(y_test_numpy, rnn_pred_classes)\n",
        "        sns.heatmap(rnn_cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('RNN Decoder Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "# ML Decoder Predictions\n",
        "        ml_predictions = ml_trainer.predict(X_test.cpu().numpy())\n",
        "        ml_pred_classes = (ml_predictions > 0.5).astype(int).flatten()\n",
        "\n",
        "# ML Decoder Confusion Matrix\n",
        "        plt.subplot(1, 2, 2)\n",
        "        ml_cm = confusion_matrix(y_test_numpy, ml_pred_classes)\n",
        "        sns.heatmap(ml_cm, annot=True, fmt='d', cmap='Greens')\n",
        "        plt.title('ML Decoder Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrices.png')\n",
        "        plt.close()\n",
        "\n",
        "# Diagnostic prints\n",
        "        log(\"Diagnostic Information:\")\n",
        "        log(f\"Y Test Shape (Numpy): {y_test_numpy.shape}\")\n",
        "        log(f\"RNN Predictions Shape: {rnn_predictions.shape}\")\n",
        "        log(f\"RNN Pred Classes Shape: {rnn_pred_classes.shape}\")\n",
        "        log(f\"ML Predictions Shape: {ml_predictions.shape}\")\n",
        "        log(f\"ML Pred Classes Shape: {ml_pred_classes.shape}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "        log(\"Classification Reports:\")\n",
        "\n",
        "        log(\"RNN Decoder Classification Report:\")\n",
        "        print(classification_report(\n",
        "           y_test_numpy,\n",
        "           rnn_pred_classes\n",
        "))\n",
        "\n",
        "        log(\"ML Decoder Classification Report:\")\n",
        "        print(classification_report(\n",
        "           y_test_numpy,\n",
        "           ml_pred_classes\n",
        "))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ensure predictions are converted to numpy and classes are determined\n",
        "        rnn_predictions = rnn_trainer.predict(X_test.cpu().numpy())\n",
        "        rnn_pred_classes = (rnn_predictions > 0.5).astype(int).flatten()\n",
        "        y_test_numpy = y_test.cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Performance Metrics Calculation\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate detailed performance metrics\n",
        "\n",
        "    Args:\n",
        "        y_true (np.ndarray): True labels\n",
        "        y_pred (np.ndarray): Predicted labels\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics\n",
        "    \"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "    metrics = {\n",
        "        'True Negatives': tn,\n",
        "        'False Positives': fp,\n",
        "        'False Negatives': fn,\n",
        "        'True Positives': tp,\n",
        "        'Accuracy': (tp + tn) / (tp + tn + fp + fn),\n",
        "        'Precision': tp / (tp + fp),\n",
        "        'Recall': tp / (tp + fn),\n",
        "        'F1 Score': 2 * tp / (2 * tp + fp + fn)\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "   # Calculate and log metrics\n",
        "    log(\"RNN Decoder Performance Metrics:\")\n",
        "    rnn_metrics = calculate_metrics(y_test.cpu().numpy(), rnn_pred_classes)\n",
        "    for metric, value in rnn_metrics.items():\n",
        "     log(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    log(\"ML Decoder Performance Metrics:\")\n",
        "    ml_metrics = calculate_metrics(y_test.cpu().numpy(), ml_pred_classes)\n",
        "    for metric, value in ml_metrics.items():\n",
        "     log(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    log(\" Confusion Matrices and Performance Metrics Saved\")\n",
        "\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZKhGQrzTnZqP",
        "outputId": "7167abac-4854-4a03-dc58-3314861cf7c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipympl in /usr/local/lib/python3.11/dist-packages (0.9.7)\n",
            "Requirement already satisfied: ipython<10 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.7.1)\n",
            "Requirement already satisfied: matplotlib<4,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (3.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipympl) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (2.8.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<10->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<10->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<10->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.7)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (4.3.7)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.13.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "INFO:  Using Device: cuda\n",
            "INFO:  Polar Code Simulation Initialization\n",
            "INFO: Block Length: 128\n",
            "INFO: Information Bits: 64\n",
            "INFO: Learning Rate: 0.001\n",
            "INFO: Epochs: 100\n",
            "INFO: Batch Size: 64\n",
            "INFO:  Polar Code Generator Initialized\n",
            "INFO: Dataset Prepared: X shape (40000, 128), y shape (40000,)\n",
            "INFO:  Dataset Split Completed\n",
            "INFO: Epoch 1/100, Loss: 0.0267\n",
            "INFO: Epoch 2/100, Loss: 0.0001\n",
            "INFO: Epoch 3/100, Loss: 0.0000\n",
            "INFO: Epoch 4/100, Loss: 0.0000\n",
            "INFO: Epoch 5/100, Loss: 0.0000\n",
            "INFO: Epoch 6/100, Loss: 0.0000\n",
            "INFO: Epoch 7/100, Loss: 0.0000\n",
            "INFO: Epoch 8/100, Loss: 0.0000\n",
            "INFO: Epoch 9/100, Loss: 0.0000\n",
            "INFO: Epoch 10/100, Loss: 0.0000\n",
            "INFO: Epoch 11/100, Loss: 0.0000\n",
            "INFO: Epoch 12/100, Loss: 0.0000\n",
            "INFO: Epoch 13/100, Loss: 0.0000\n",
            "INFO: Epoch 14/100, Loss: 0.0000\n",
            "INFO: Epoch 15/100, Loss: 0.0000\n",
            "INFO: Epoch 16/100, Loss: 0.0000\n",
            "INFO: Epoch 17/100, Loss: 0.0000\n",
            "INFO: Epoch 18/100, Loss: 0.0000\n",
            "INFO: Epoch 19/100, Loss: 0.0000\n",
            "INFO: Epoch 20/100, Loss: 0.0000\n",
            "INFO: Epoch 21/100, Loss: 0.0000\n",
            "INFO: Epoch 22/100, Loss: 0.0000\n",
            "INFO: Epoch 23/100, Loss: 0.0000\n",
            "INFO: Epoch 24/100, Loss: 0.0000\n",
            "INFO: Epoch 25/100, Loss: 0.0000\n",
            "INFO: Epoch 26/100, Loss: 0.0000\n",
            "INFO: Epoch 27/100, Loss: 0.0000\n",
            "INFO: Epoch 28/100, Loss: 0.0000\n",
            "INFO: Epoch 29/100, Loss: 0.0000\n",
            "INFO: Epoch 30/100, Loss: 0.0000\n",
            "INFO: Epoch 31/100, Loss: 0.0000\n",
            "INFO: Epoch 32/100, Loss: 0.0000\n",
            "INFO: Epoch 33/100, Loss: 0.0000\n",
            "INFO: Epoch 34/100, Loss: 0.0000\n",
            "INFO: Epoch 35/100, Loss: 0.0000\n",
            "INFO: Epoch 36/100, Loss: 0.0000\n",
            "INFO: Epoch 37/100, Loss: 0.0000\n",
            "INFO: Epoch 38/100, Loss: 0.0000\n",
            "INFO: Epoch 39/100, Loss: 0.0000\n",
            "INFO: Epoch 40/100, Loss: 0.0000\n",
            "INFO: Epoch 41/100, Loss: 0.0000\n",
            "INFO: Epoch 42/100, Loss: 0.0000\n",
            "INFO: Epoch 43/100, Loss: 0.0000\n",
            "INFO: Epoch 44/100, Loss: 0.0000\n",
            "INFO: Epoch 45/100, Loss: 0.0000\n",
            "INFO: Epoch 46/100, Loss: 0.0000\n",
            "INFO: Epoch 47/100, Loss: 0.0000\n",
            "INFO: Epoch 48/100, Loss: 0.0000\n",
            "INFO: Epoch 49/100, Loss: 0.0000\n",
            "INFO: Epoch 50/100, Loss: 0.0000\n",
            "INFO: Epoch 51/100, Loss: 0.0000\n",
            "INFO: Epoch 52/100, Loss: 0.0000\n",
            "INFO: Epoch 53/100, Loss: 0.0000\n",
            "INFO: Epoch 54/100, Loss: 0.0000\n",
            "INFO: Epoch 55/100, Loss: 0.0000\n",
            "INFO: Epoch 56/100, Loss: 0.0000\n",
            "INFO: Epoch 57/100, Loss: 0.0000\n",
            "INFO: Epoch 58/100, Loss: 0.0000\n",
            "INFO: Epoch 59/100, Loss: 0.0000\n",
            "INFO: Epoch 60/100, Loss: 0.0000\n",
            "INFO: Epoch 61/100, Loss: 0.0000\n",
            "INFO: Epoch 62/100, Loss: 0.0000\n",
            "INFO: Epoch 63/100, Loss: 0.0000\n",
            "INFO: Epoch 64/100, Loss: 0.0000\n",
            "INFO: Epoch 65/100, Loss: 0.0000\n",
            "INFO: Epoch 66/100, Loss: 0.0000\n",
            "INFO: Epoch 67/100, Loss: 0.0000\n",
            "INFO: Epoch 68/100, Loss: 0.0000\n",
            "INFO: Epoch 69/100, Loss: 0.0000\n",
            "INFO: Epoch 70/100, Loss: 0.0000\n",
            "INFO: Epoch 71/100, Loss: 0.0000\n",
            "INFO: Epoch 72/100, Loss: 0.0000\n",
            "INFO: Epoch 73/100, Loss: 0.0000\n",
            "INFO: Epoch 74/100, Loss: 0.0000\n",
            "INFO: Epoch 75/100, Loss: 0.0000\n",
            "INFO: Epoch 76/100, Loss: 0.0000\n",
            "INFO: Epoch 77/100, Loss: 0.0000\n",
            "INFO: Epoch 78/100, Loss: 0.0000\n",
            "INFO: Epoch 79/100, Loss: 0.0000\n",
            "INFO: Epoch 80/100, Loss: 0.0000\n",
            "INFO: Epoch 81/100, Loss: 0.0000\n",
            "INFO: Epoch 82/100, Loss: 0.0000\n",
            "INFO: Epoch 83/100, Loss: 0.0000\n",
            "INFO: Epoch 84/100, Loss: 0.0000\n",
            "INFO: Epoch 85/100, Loss: 0.0000\n",
            "INFO: Epoch 86/100, Loss: 0.0000\n",
            "INFO: Epoch 87/100, Loss: 0.0000\n",
            "INFO: Epoch 88/100, Loss: 0.0000\n",
            "INFO: Epoch 89/100, Loss: 0.0000\n",
            "INFO: Epoch 90/100, Loss: 0.0000\n",
            "INFO: Epoch 91/100, Loss: 0.0000\n",
            "INFO: Epoch 92/100, Loss: 0.0000\n",
            "INFO: Epoch 93/100, Loss: 0.0000\n",
            "INFO: Epoch 94/100, Loss: 0.0000\n",
            "INFO: Epoch 95/100, Loss: 0.0000\n",
            "INFO: Epoch 96/100, Loss: 0.0000\n",
            "INFO: Epoch 97/100, Loss: 0.0000\n",
            "INFO: Epoch 98/100, Loss: 0.0000\n",
            "INFO: Epoch 99/100, Loss: 0.0000\n",
            "INFO: Epoch 100/100, Loss: 0.0000\n",
            "INFO:  RNN Decoder Training Completed\n",
            "INFO: Epoch 1/100, Loss: 0.0279\n",
            "INFO: Epoch 2/100, Loss: 0.0010\n",
            "INFO: Epoch 3/100, Loss: 0.0004\n",
            "INFO: Epoch 4/100, Loss: 0.0002\n",
            "INFO: Epoch 5/100, Loss: 0.0001\n",
            "INFO: Epoch 6/100, Loss: 0.0001\n",
            "INFO: Epoch 7/100, Loss: 0.0000\n",
            "INFO: Epoch 8/100, Loss: 0.0000\n",
            "INFO: Epoch 9/100, Loss: 0.0000\n",
            "INFO: Epoch 10/100, Loss: 0.0000\n",
            "INFO: Epoch 11/100, Loss: 0.0000\n",
            "INFO: Epoch 12/100, Loss: 0.0000\n",
            "INFO: Epoch 13/100, Loss: 0.0000\n",
            "INFO: Epoch 14/100, Loss: 0.0000\n",
            "INFO: Epoch 15/100, Loss: 0.0000\n",
            "INFO: Epoch 16/100, Loss: 0.0000\n",
            "INFO: Epoch 17/100, Loss: 0.0000\n",
            "INFO: Epoch 18/100, Loss: 0.0000\n",
            "INFO: Epoch 19/100, Loss: 0.0000\n",
            "INFO: Epoch 20/100, Loss: 0.0000\n",
            "INFO: Epoch 21/100, Loss: 0.0000\n",
            "INFO: Epoch 22/100, Loss: 0.0000\n",
            "INFO: Epoch 23/100, Loss: 0.0000\n",
            "INFO: Epoch 24/100, Loss: 0.0000\n",
            "INFO: Epoch 25/100, Loss: 0.0000\n",
            "INFO: Epoch 26/100, Loss: 0.0000\n",
            "INFO: Epoch 27/100, Loss: 0.0000\n",
            "INFO: Epoch 28/100, Loss: 0.0000\n",
            "INFO: Epoch 29/100, Loss: 0.0000\n",
            "INFO: Epoch 30/100, Loss: 0.0000\n",
            "INFO: Epoch 31/100, Loss: 0.0000\n",
            "INFO: Epoch 32/100, Loss: 0.0000\n",
            "INFO: Epoch 33/100, Loss: 0.0000\n",
            "INFO: Epoch 34/100, Loss: 0.0000\n",
            "INFO: Epoch 35/100, Loss: 0.0000\n",
            "INFO: Epoch 36/100, Loss: 0.0000\n",
            "INFO: Epoch 37/100, Loss: 0.0000\n",
            "INFO: Epoch 38/100, Loss: 0.0000\n",
            "INFO: Epoch 39/100, Loss: 0.0000\n",
            "INFO: Epoch 40/100, Loss: 0.0000\n",
            "INFO: Epoch 41/100, Loss: 0.0000\n",
            "INFO: Epoch 42/100, Loss: 0.0000\n",
            "INFO: Epoch 43/100, Loss: 0.0000\n",
            "INFO: Epoch 44/100, Loss: 0.0000\n",
            "INFO: Epoch 45/100, Loss: 0.0000\n",
            "INFO: Epoch 46/100, Loss: 0.0000\n",
            "INFO: Epoch 47/100, Loss: 0.0000\n",
            "INFO: Epoch 48/100, Loss: 0.0000\n",
            "INFO: Epoch 49/100, Loss: 0.0000\n",
            "INFO: Epoch 50/100, Loss: 0.0000\n",
            "INFO: Epoch 51/100, Loss: 0.0000\n",
            "INFO: Epoch 52/100, Loss: 0.0000\n",
            "INFO: Epoch 53/100, Loss: 0.0000\n",
            "INFO: Epoch 54/100, Loss: 0.0000\n",
            "INFO: Epoch 55/100, Loss: 0.0000\n",
            "INFO: Epoch 56/100, Loss: 0.0000\n",
            "INFO: Epoch 57/100, Loss: 0.0000\n",
            "INFO: Epoch 58/100, Loss: 0.0000\n",
            "INFO: Epoch 59/100, Loss: 0.0000\n",
            "INFO: Epoch 60/100, Loss: 0.0000\n",
            "INFO: Epoch 61/100, Loss: 0.0000\n",
            "INFO: Epoch 62/100, Loss: 0.0000\n",
            "INFO: Epoch 63/100, Loss: 0.0000\n",
            "INFO: Epoch 64/100, Loss: 0.0000\n",
            "INFO: Epoch 65/100, Loss: 0.0000\n",
            "INFO: Epoch 66/100, Loss: 0.0000\n",
            "INFO: Epoch 67/100, Loss: 0.0000\n",
            "INFO: Epoch 68/100, Loss: 0.0000\n",
            "INFO: Epoch 69/100, Loss: 0.0000\n",
            "INFO: Epoch 70/100, Loss: 0.0000\n",
            "INFO: Epoch 71/100, Loss: 0.0000\n",
            "INFO: Epoch 72/100, Loss: 0.0000\n",
            "INFO: Epoch 73/100, Loss: 0.0000\n",
            "INFO: Epoch 74/100, Loss: 0.0000\n",
            "INFO: Epoch 75/100, Loss: 0.0000\n",
            "INFO: Epoch 76/100, Loss: 0.0000\n",
            "INFO: Epoch 77/100, Loss: 0.0000\n",
            "INFO: Epoch 78/100, Loss: 0.0000\n",
            "INFO: Epoch 79/100, Loss: 0.0000\n",
            "INFO: Epoch 80/100, Loss: 0.0000\n",
            "INFO: Epoch 81/100, Loss: 0.0000\n",
            "INFO: Epoch 82/100, Loss: 0.0000\n",
            "INFO: Epoch 83/100, Loss: 0.0000\n",
            "INFO: Epoch 84/100, Loss: 0.0000\n",
            "INFO: Epoch 85/100, Loss: 0.0000\n",
            "INFO: Epoch 86/100, Loss: 0.0000\n",
            "INFO: Epoch 87/100, Loss: 0.0000\n",
            "INFO: Epoch 88/100, Loss: 0.0000\n",
            "INFO: Epoch 89/100, Loss: 0.0000\n",
            "INFO: Epoch 90/100, Loss: 0.0000\n",
            "INFO: Epoch 91/100, Loss: 0.0000\n",
            "INFO: Epoch 92/100, Loss: 0.0000\n",
            "INFO: Epoch 93/100, Loss: 0.0000\n",
            "INFO: Epoch 94/100, Loss: 0.0000\n",
            "INFO: Epoch 95/100, Loss: 0.0000\n",
            "INFO: Epoch 96/100, Loss: 0.0000\n",
            "INFO: Epoch 97/100, Loss: 0.0000\n",
            "INFO: Epoch 98/100, Loss: 0.0000\n",
            "INFO: Epoch 99/100, Loss: 0.0000\n",
            "INFO: Epoch 100/100, Loss: 0.0000\n",
            "INFO:  ML Decoder Training Completed\n",
            "INFO: Simulation Complete!\n",
            "INFO:  BER/BLER Performance Plots Saved\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
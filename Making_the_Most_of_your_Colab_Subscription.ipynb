{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZFBua2Csaq03",
        "outputId": "b30e493d-6f3d-4205-93f0-8354b0dc6559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "iyB7i06xcV2O",
        "outputId": "887d0a23-717e-4767-e28d-ce0a6bb3d545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b5cc59c2-3db9-453f-92c7-0b7974261e22\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b5cc59c2-3db9-453f-92c7-0b7974261e22\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving making_the_most_of_your_colab_subscription (14).py to making_the_most_of_your_colab_subscription (14).py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive Polar Code Simulation Framework\n",
        "!pip install torch numpy matplotlib scikit-learn\n",
        "# Essential Scientific and Deep Learning Libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Machine Learning and Data Handling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Visualization and Scientific Computing\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.special as sps\n",
        "\n",
        "# System and Utilities\n",
        "import logging\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "# Logging Configuration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Device Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸš€ Using Device: {DEVICE}\")\n",
        "\n",
        "#Part 2: Polar Code Generator with CRC\n",
        "\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N, K, crc_type='CRC-7'):\n",
        "        \"\"\"\n",
        "        Polar Code Generator with CRC support\n",
        "\n",
        "        Args:\n",
        "            N (int): Total code length\n",
        "            K (int): Information bit length\n",
        "            crc_type (str): CRC polynomial type\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc_type = crc_type\n",
        "\n",
        "        # CRC Polynomials\n",
        "        self.crc_polynomials = {\n",
        "            'CRC-7': {\n",
        "                'polynomial': [1, 1, 1, 0, 0, 1, 1],\n",
        "                'length': 7\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        \"\"\"\n",
        "        Generate random information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Random information bits\n",
        "        \"\"\"\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def compute_crc(self, bits):\n",
        "        \"\"\"\n",
        "        Compute CRC checksum using polynomial division\n",
        "\n",
        "        Args:\n",
        "            bits (np.ndarray): Input bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: CRC checksum bits\n",
        "        \"\"\"\n",
        "        poly_info = self.crc_polynomials.get(self.crc_type)\n",
        "        if not poly_info:\n",
        "            raise ValueError(f\"Unsupported CRC type: {self.crc_type}\")\n",
        "\n",
        "        polynomial = poly_info['polynomial']\n",
        "        crc_length = poly_info['length']\n",
        "\n",
        "        # Convert input to list and pad\n",
        "        message = bits.tolist() + [0] * crc_length\n",
        "\n",
        "        # Polynomial long division\n",
        "        for i in range(len(message) - crc_length):\n",
        "            if message[i] == 1:\n",
        "                for j in range(crc_length + 1):\n",
        "                    message[i + j] ^= polynomial[j] if j < len(polynomial) else 0\n",
        "\n",
        "        # Return the last 'crc_length' bits\n",
        "        return np.array(message[-crc_length:], dtype=int)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Polar Code Encoding with CRC\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Append CRC\n",
        "        crc_bits = self.compute_crc(info_bits)\n",
        "        extended_info_bits = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Basic polar encoding (placeholder)\n",
        "        codeword = np.zeros(self.N, dtype=int)\n",
        "        codeword[:len(extended_info_bits)] = extended_info_bits\n",
        "\n",
        "        return codeword\n",
        "\n",
        "    def verify_codeword(self, codeword):\n",
        "        \"\"\"\n",
        "        Verify codeword using CRC\n",
        "\n",
        "        Args:\n",
        "            codeword (np.ndarray): Received codeword\n",
        "\n",
        "        Returns:\n",
        "            bool: True if CRC check passes, False otherwise\n",
        "        \"\"\"\n",
        "        poly_info = self.crc_polynomials.get(self.crc_type)\n",
        "        if not poly_info:\n",
        "            raise ValueError(f\"Unsupported CRC type: {self.crc_type}\")\n",
        "\n",
        "        crc_length = poly_info['length']\n",
        "\n",
        "        # Extract information and CRC bits\n",
        "        info_bits = codeword[:-crc_length]\n",
        "        received_crc = codeword[-crc_length:]\n",
        "\n",
        "        # Compute CRC of information bits\n",
        "        computed_crc = self.compute_crc(info_bits)\n",
        "\n",
        "        # Compare received and computed CRC\n",
        "        return np.array_equal(received_crc, computed_crc)\n",
        "#############################################################\n",
        "#Very latest%%!\n",
        "\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        RNN Decoder with flexible input handling\n",
        "        \"\"\"\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)  # Output single value\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with input shape handling\n",
        "        \"\"\"\n",
        "        # Ensure input is 2D\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Flatten multi-dimensional inputs\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.model(x).squeeze(-1)\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Decoder Trainer with comprehensive tensor handling\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # Use BCEWithLogitsLoss to handle sigmoid internally\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def _preprocess_tensors(self, X, y):\n",
        "        \"\"\"\n",
        "        Comprehensive tensor preprocessing\n",
        "        \"\"\"\n",
        "        # Ensure X and y are PyTorch tensors\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "        if not isinstance(y, torch.Tensor):\n",
        "            y = torch.FloatTensor(y)\n",
        "\n",
        "        # Flatten X if multi-dimensional\n",
        "        if X.dim() > 2:\n",
        "            X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Ensure y is 2D tensor with shape [batch_size, 1]\n",
        "        y = y.float().view(-1, 1)\n",
        "\n",
        "        # Move to device\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        # Diagnostic print\n",
        "        print(\"\\nðŸ” Tensor Preprocessing:\")\n",
        "        print(f\"X shape: {X.shape}\")\n",
        "        print(f\"y shape: {y.shape}\")\n",
        "        print(f\"X dtype: {X.dtype}\")\n",
        "        print(f\"y dtype: {y.dtype}\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, X, y, epochs=50, batch_size=32, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Enhanced training method with built-in validation split\n",
        "        \"\"\"\n",
        "        # Preprocess tensors\n",
        "        X, y = self._preprocess_tensors(X, y)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X))\n",
        "        X_val, y_val = X[train_size:], y[train_size:]\n",
        "        X_train, y_train = X[:train_size], y[:train_size]\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = self._train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = self._validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "                  f\"Train Loss: {train_loss:.4f}, \"\n",
        "                  f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "\n",
        "   #######################\n",
        "   #Latest def train epoch\n",
        "\n",
        "    def _train_epoch(self, dataloader):\n",
        "        \"\"\"\n",
        "        Train for one epoch\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Ensure correct device\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(batch_X)\n",
        "\n",
        "            # Reshape batch_y to match outputs shape\n",
        "            batch_y = batch_y.view_as(outputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "   ######################\n",
        "\n",
        "   #Latest def validate\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        \"\"\"\n",
        "        Validate model performance\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                # Move to device\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "\n",
        "                # Reshape batch_y to match outputs shape\n",
        "                batch_y = batch_y.view_as(outputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "   ##########################\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions with comprehensive error handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Ensure input is a tensor\n",
        "            if not isinstance(X, torch.Tensor):\n",
        "                X = torch.FloatTensor(X)\n",
        "\n",
        "            # Flatten multi-dimensional inputs\n",
        "            if X.dim() > 2:\n",
        "                X = X.view(X.size(0), -1)\n",
        "\n",
        "            # Move to device\n",
        "            X = X.to(self.device)\n",
        "\n",
        "            # Set model to evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(X)\n",
        "\n",
        "            # Apply sigmoid to get probabilities\n",
        "            return torch.sigmoid(outputs).cpu().numpy().flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Prediction Error: {e}\")\n",
        "            return np.zeros(X.size(0))\n",
        "\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type=\"AWGN\"):\n",
        "    \"\"\"\n",
        "    Prepare dataset for Polar Code simulation\n",
        "    \"\"\"\n",
        "    # Create channel simulator\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "\n",
        "    # Initialize storage\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate information bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Encode polar code\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # Channel simulation\n",
        "        received_signal = channel_simulator.simulate(encoded_signal, snr_db)\n",
        "\n",
        "        # Store features and labels\n",
        "        X.append(received_signal)\n",
        "\n",
        "        # Binary classification label\n",
        "        y.append(1 if np.mean(info_bits) > 0.5 else 0)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "#############################################################\n",
        "#part 3\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Advanced Channel Simulator for communication systems\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Channel type ('AWGN' or 'Rayleigh')\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "        logging.info(f\"Initializing {channel_type} Channel Simulator\")\n",
        "\n",
        "    def simulate(self, encoded_signal, snr_db):\n",
        "        \"\"\"\n",
        "        Simulate signal transmission through specified channel\n",
        "\n",
        "        Args:\n",
        "            encoded_signal (np.ndarray): Input encoded signal\n",
        "            snr_db (float): Signal-to-Noise Ratio in decibels\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received signal after channel effects\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert input to numpy array\n",
        "            encoded_signal = np.array(encoded_signal, dtype=float)\n",
        "\n",
        "            # Convert bits {0,1} to BPSK: {+1, -1}\n",
        "            bpsk_signal = 1 - 2 * encoded_signal\n",
        "\n",
        "            # Convert SNR from dB to linear scale\n",
        "            snr_linear = 10 ** (snr_db / 10)\n",
        "\n",
        "            # Compute signal power\n",
        "            signal_power = np.mean(bpsk_signal**2)\n",
        "\n",
        "            # Noise power calculation\n",
        "            noise_power = signal_power / snr_linear\n",
        "            noise_std = np.sqrt(noise_power / 2.0)\n",
        "\n",
        "            # Channel-specific simulation\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # Additive White Gaussian Noise\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = bpsk_signal + noise\n",
        "\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                # Rayleigh Fading Channel\n",
        "                fading = np.random.rayleigh(scale=1.0, size=bpsk_signal.shape)\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = fading * bpsk_signal + noise\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Convert back to binary representation\n",
        "            return (received_signal > 0).astype(float)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Channel simulation error: {e}\")\n",
        "            return encoded_signal\n",
        "\n",
        "    def compute_theoretical_performance(self, block_length, snr_linear):\n",
        "        \"\"\"\n",
        "        Compute theoretical Bit Error Probability (BEP) and Block Error Probability (BLER)\n",
        "\n",
        "        Args:\n",
        "            block_length (int): Length of the code block\n",
        "            snr_linear (np.ndarray): SNR in linear scale\n",
        "\n",
        "        Returns:\n",
        "            tuple: Theoretical BEP and BLER\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # AWGN Channel Theoretical Performance\n",
        "                # Bit Error Probability using Q-function\n",
        "                bep = 0.5 * sps.erfc(np.sqrt(snr_linear))\n",
        "\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                # Rayleigh Fading Channel Theoretical Performance\n",
        "                # Average Bit Error Probability for Rayleigh fading\n",
        "                bep = 0.5 * (1 - np.sqrt(snr_linear / (1 + snr_linear)))\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Block Error Probability (assuming independent bit errors)\n",
        "            bler = 1 - (1 - bep) ** block_length\n",
        "\n",
        "            return bep, bler\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Theoretical performance computation error: {e}\")\n",
        "            # Return default values if computation fails\n",
        "            return np.zeros_like(snr_linear), np.ones_like(snr_linear)\n",
        "\n",
        "    def plot_channel_capacity(self, snr_range):\n",
        "        \"\"\"\n",
        "        Plot channel capacity for the specific channel type\n",
        "\n",
        "        Args:\n",
        "            snr_range (np.ndarray): Range of SNR values in dB\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Compute channel capacities\n",
        "        snr_linear = 10 ** (snr_range / 10)\n",
        "        capacities = [np.log2(1 + snr) for snr in snr_linear]\n",
        "\n",
        "        plt.plot(snr_range, capacities, label=f'{self.channel_type} Channel')\n",
        "        plt.title(f'Channel Capacity - {self.channel_type} Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Capacity (bits/channel use)')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Utility function for dataset preparation\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type='AWGN'):\n",
        "    \"\"\"\n",
        "    Prepare dataset for Polar Code simulation\n",
        "\n",
        "    Args:\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "        num_samples (int): Number of samples to generate\n",
        "        snr_db (float): Signal-to-Noise Ratio in dB\n",
        "        channel_type (str): Channel type\n",
        "\n",
        "    Returns:\n",
        "        tuple: Input features and corresponding labels\n",
        "    \"\"\"\n",
        "    # Create channel simulator\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "\n",
        "    # Initialize storage\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate information bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Encode polar code\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # Channel simulation\n",
        "        received_signal = channel_simulator.simulate(encoded_signal, snr_db)\n",
        "\n",
        "        # Store features and labels\n",
        "        X.append(received_signal)\n",
        "        y.append(info_bits)  # Use original info_bits as target\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "#Part 4\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Advanced Channel Simulator for communication systems\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Channel type ('AWGN' or 'Rayleigh')\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "        logging.info(f\"Initializing {channel_type} Channel Simulator\")\n",
        "\n",
        "    def simulate(self, encoded_signal, snr_db):\n",
        "        \"\"\"\n",
        "        Simulate signal transmission through specified channel\n",
        "\n",
        "        Args:\n",
        "            encoded_signal (np.ndarray): Input encoded signal\n",
        "            snr_db (float): Signal-to-Noise Ratio in decibels\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received signal after channel effects\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert input to numpy array\n",
        "            encoded_signal = np.array(encoded_signal, dtype=float)\n",
        "\n",
        "            # Convert bits {0,1} to BPSK: {+1, -1}\n",
        "            bpsk_signal = 1 - 2 * encoded_signal\n",
        "\n",
        "            # Convert SNR from dB to linear scale\n",
        "            snr_linear = 10 ** (snr_db / 10)\n",
        "\n",
        "            # Compute signal power\n",
        "            signal_power = np.mean(bpsk_signal**2)\n",
        "\n",
        "            # Noise power calculation\n",
        "            noise_power = signal_power / snr_linear\n",
        "            noise_std = np.sqrt(noise_power / 2.0)\n",
        "\n",
        "            # Channel-specific simulation\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # Additive White Gaussian Noise\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = bpsk_signal + noise\n",
        "\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                # Rayleigh Fading Channel\n",
        "                fading = np.random.rayleigh(scale=1.0, size=bpsk_signal.shape)\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = fading * bpsk_signal + noise\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Convert back to binary representation\n",
        "            return (received_signal > 0).astype(float)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Channel simulation error: {e}\")\n",
        "            return encoded_signal\n",
        "\n",
        "    def compute_theoretical_performance(self, block_length, snr_linear):\n",
        "        \"\"\"\n",
        "        Compute theoretical Bit Error Probability (BEP) and Block Error Probability (BLER)\n",
        "\n",
        "        Args:\n",
        "            block_length (int): Length of the code block\n",
        "            snr_linear (np.ndarray): SNR in linear scale\n",
        "\n",
        "        Returns:\n",
        "            tuple: Theoretical BEP and BLER\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # AWGN Channel Theoretical Performance\n",
        "                # Bit Error Probability using Q-function\n",
        "                bep = 0.5 * sps.erfc(np.sqrt(snr_linear))\n",
        "\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                # Rayleigh Fading Channel Theoretical Performance\n",
        "                # Average Bit Error Probability for Rayleigh fading\n",
        "                bep = 0.5 * (1 - np.sqrt(snr_linear / (1 + snr_linear)))\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Block Error Probability (assuming independent bit errors)\n",
        "            bler = 1 - (1 - bep) ** block_length\n",
        "\n",
        "            return bep, bler\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Theoretical performance computation error: {e}\")\n",
        "            # Return default values if computation fails\n",
        "            return np.zeros_like(snr_linear), np.ones_like(snr_linear)\n",
        "\n",
        "    def plot_channel_capacity(self, snr_range):\n",
        "        \"\"\"\n",
        "        Plot channel capacity for the specific channel type\n",
        "\n",
        "        Args:\n",
        "            snr_range (np.ndarray): Range of SNR values in dB\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Compute channel capacities\n",
        "        snr_linear = 10 ** (snr_range / 10)\n",
        "        capacities = [np.log2(1 + snr) for snr in snr_linear]\n",
        "\n",
        "        plt.plot(snr_range, capacities, label=f'{self.channel_type} Channel')\n",
        "        plt.title(f'Channel Capacity - {self.channel_type} Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Capacity (bits/channel use)')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Utility function for dataset preparation\n",
        "##################################################\n",
        "#Very latest\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type=\"AWGN\"):\n",
        "    \"\"\"\n",
        "    Prepare dataset for Polar Code simulation with robust preprocessing\n",
        "\n",
        "    Args:\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "        num_samples (int): Number of samples to generate\n",
        "        snr_db (float): Signal-to-Noise Ratio in dB\n",
        "        channel_type (str): Channel type\n",
        "\n",
        "    Returns:\n",
        "        tuple: Input features and corresponding labels\n",
        "    \"\"\"\n",
        "    # Create channel simulator\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "\n",
        "    # Initialize storage\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate information bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Encode polar code\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # Channel simulation\n",
        "        received_signal = channel_simulator.simulate(encoded_signal, snr_db)\n",
        "\n",
        "        # Store features and labels\n",
        "        X.append(received_signal)\n",
        "\n",
        "        # Binary classification label (based on mean of info_bits)\n",
        "        y.append(1 if np.mean(info_bits) > 0.5 else 0)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Simplified RNN Decoder with robust input handling\n",
        "        \"\"\"\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)  # No sigmoid here\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with flexible input handling\n",
        "        \"\"\"\n",
        "        # Ensure input is 2D\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Flatten if multi-dimensional\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.model(x).squeeze(-1)\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Decoder Trainer with comprehensive tensor handling\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # Use BCEWithLogitsLoss to handle sigmoid internally\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def _preprocess_tensors(self, X, y):\n",
        "        \"\"\"\n",
        "        Comprehensive tensor preprocessing\n",
        "        \"\"\"\n",
        "        # Ensure X and y are PyTorch tensors\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "        if not isinstance(y, torch.Tensor):\n",
        "            y = torch.FloatTensor(y)\n",
        "\n",
        "        # Flatten X if multi-dimensional\n",
        "        if X.dim() > 2:\n",
        "            X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Ensure y is 1D tensor of floats\n",
        "        y = y.float().squeeze()\n",
        "\n",
        "        # Move to device\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        # Diagnostic print\n",
        "        print(\"\\nðŸ” Tensor Preprocessing:\")\n",
        "        print(f\"X shape: {X.shape}\")\n",
        "        print(f\"y shape: {y.shape}\")\n",
        "        print(f\"X dtype: {X.dtype}\")\n",
        "        print(f\"y dtype: {y.dtype}\")\n",
        "\n",
        "        return X, y\n",
        "##############################################\n",
        "#Latest train\n",
        "def train(self, X, y, epochs=50, batch_size=32, validation_split=0.2):\n",
        "    \"\"\"\n",
        "    Enhanced training method with built-in validation split and\n",
        "    explicit target tensor shaping for compatibility.\n",
        "    \"\"\"\n",
        "    # Preprocess tensors\n",
        "    # Ensure y is a 2D tensor from the start\n",
        "    X, y = self._preprocess_tensors(X, y.view(-1, 1))\n",
        "\n",
        "    # Split into train and validation\n",
        "    train_size = int((1 - validation_split) * len(X))\n",
        "    X_val, y_val = X[train_size:], y[train_size:]\n",
        "    X_train, y_train = X[:train_size], y[:train_size]\n",
        "\n",
        "    # Create data loaders\n",
        "    # No need to unsqueeze y here anymore\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        self.model.train()\n",
        "        train_loss = self._train_epoch(train_loader)\n",
        "        self.train_losses.append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        self.model.eval()\n",
        "        val_loss = self._validate(val_loader)\n",
        "        self.val_losses.append(val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        self.scheduler.step(val_loss)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "              f\"Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return self.train_losses, self.val_losses\n",
        "\n",
        "# The _preprocess_tensors, _train_epoch, and _validate methods remain as before\n",
        "# with the batch_y = batch_y.view_as(outputs) line removed.\n",
        "\n",
        "# The _preprocess_tensors, _train_epoch, and _validate methods remain as before\n",
        "# with the batch_y = batch_y.view_as(outputs) line removed.\n",
        "\n",
        "##############################################\n",
        "\n",
        "\n",
        "\n",
        "    def _train_epoch(self, dataloader):\n",
        "        \"\"\"\n",
        "        Train for one epoch\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Ensure correct device\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(batch_X)\n",
        "\n",
        "            # Compute loss (ensure batch_y matches outputs shape)\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        \"\"\"\n",
        "        Validate model performance\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                # Move to device\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions with comprehensive error handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Ensure input is a tensor\n",
        "            if not isinstance(X, torch.Tensor):\n",
        "                X = torch.FloatTensor(X)\n",
        "\n",
        "            # Flatten multi-dimensional inputs\n",
        "            if X.dim() > 2:\n",
        "                X = X.view(X.size(0), -1)\n",
        "\n",
        "            # Move to device\n",
        "            X = X.to(self.device)\n",
        "\n",
        "            # Set model to evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(X)\n",
        "\n",
        "            # Apply sigmoid to get probabilities\n",
        "            return torch.sigmoid(outputs).cpu().numpy().flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Prediction Error: {e}\")\n",
        "            return np.zeros(X.size(0))\n",
        "###################################################\n",
        "#Latest\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Simplified RNN Decoder with robust input handling\n",
        "        \"\"\"\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with flexible input handling\n",
        "        \"\"\"\n",
        "        # Ensure input is 2D\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Flatten if multi-dimensional\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.model(x).squeeze(-1)\n",
        "############################################################\n",
        "#latest Decoder trainer\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Decoder Trainer with enhanced error handling\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def _preprocess_tensors(self, X, y):\n",
        "        \"\"\"\n",
        "        Enhanced tensor preprocessing\n",
        "        \"\"\"\n",
        "        # Ensure X and y are PyTorch tensors\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "        if not isinstance(y, torch.Tensor):\n",
        "            y = torch.FloatTensor(y)\n",
        "\n",
        "        # Flatten X if multi-dimensional\n",
        "        if X.dim() > 2:\n",
        "            X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Ensure y is 2D with correct shape\n",
        "        y = y.view(-1, 1).float()\n",
        "\n",
        "        # Move to device\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, X, y, epochs=50, batch_size=32, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Enhanced training method with built-in validation split\n",
        "        \"\"\"\n",
        "        # Preprocess tensors\n",
        "        X, y = self._preprocess_tensors(X, y)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X))\n",
        "        X_val, y_val = X[train_size:], y[train_size:]\n",
        "        X_train, y_train = X[:train_size], y[:train_size]\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = self._train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = self._validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "                  f\"Train Loss: {train_loss:.4f}, \"\n",
        "                  f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def _train_epoch(self, dataloader):\n",
        "        \"\"\"\n",
        "        Train for one epoch\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Ensure correct device\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(batch_X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        \"\"\"\n",
        "        Validate model performance\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                # Move to device\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions with comprehensive error handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Ensure input is a tensor\n",
        "            if not isinstance(X, torch.Tensor):\n",
        "                X = torch.FloatTensor(X)\n",
        "\n",
        "            # Flatten multi-dimensional inputs\n",
        "            if X.dim() > 2:\n",
        "                X = X.view(X.size(0), -1)\n",
        "\n",
        "            # Move to device\n",
        "            X = X.to(self.device)\n",
        "\n",
        "            # Set model to evaluation mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Predict\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(X)\n",
        "\n",
        "            return outputs.cpu().numpy().flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Prediction Error: {e}\")\n",
        "            return np.zeros(X.size(0))\n",
        "###########################################################\n",
        "\n",
        "#Part 6\n",
        "\n",
        "######################\n",
        "\n",
        "\n",
        "##########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main simulation framework for Polar Code performance analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Simulation Configuration\n",
        "        BLOCK_LENGTH = 32\n",
        "        INFO_BITS = 16\n",
        "        LEARNING_RATE = 1e-3\n",
        "        EPOCHS = 50\n",
        "        BATCH_SIZE = 32\n",
        "        NUM_SAMPLES = 5000\n",
        "\n",
        "        # SNR Ranges\n",
        "        SNR_RANGE_AWGN = np.linspace(0, 5, 10)\n",
        "        SNR_RANGE_RAYLEIGH = np.linspace(0, 10, 10)\n",
        "\n",
        "        # List Sizes\n",
        "        LIST_SIZES = [1, 4, 8]\n",
        "\n",
        "        # Polar Code Generator\n",
        "        polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "\n",
        "        # Results storage\n",
        "        results = {}\n",
        "\n",
        "        # Channel Types\n",
        "        channels = {\n",
        "            'AWGN': EnhancedChannelSimulator(channel_type='AWGN'),\n",
        "            'Rayleigh': EnhancedChannelSimulator(channel_type='Rayleigh')\n",
        "        }\n",
        "\n",
        "        # Diagnostic function for tensor verification\n",
        "        def verify_tensors(X, y, dataset_name):\n",
        "            \"\"\"\n",
        "            Verify tensor shapes and properties\n",
        "            \"\"\"\n",
        "            print(f\"\\nðŸ” Tensor Verification for {dataset_name}:\")\n",
        "            print(f\"Input shape: {X.shape}\")\n",
        "            print(f\"Input dtype: {X.dtype}\")\n",
        "            print(f\"Label shape: {y.shape}\")\n",
        "            print(f\"Label dtype: {y.dtype}\")\n",
        "            print(f\"Label unique values: {np.unique(y)}\")\n",
        "\n",
        "        # Iterate through channel types\n",
        "        for channel_name, channel in channels.items():\n",
        "            logging.info(f\"Analyzing {channel_name} Channel\")\n",
        "\n",
        "            # Prepare Dataset\n",
        "            X, y = prepare_polar_dataset(\n",
        "                polar_code_gen,\n",
        "                num_samples=NUM_SAMPLES,\n",
        "                channel_type=channel_name\n",
        "            )\n",
        "\n",
        "            # Verify initial dataset\n",
        "            verify_tensors(X, y, \"Initial Dataset\")\n",
        "\n",
        "            # Split dataset\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Verify split datasets\n",
        "            verify_tensors(X_train, y_train, \"Training Dataset\")\n",
        "            verify_tensors(X_test, y_test, \"Testing Dataset\")\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            #latest\n",
        "            ############################################################\n",
        "           #  Convert to PyTorch tensors\n",
        "\n",
        "\n",
        "            ###########################################################\n",
        "            # Flatten input features\n",
        "            X_train = torch.FloatTensor(X_train).view(X_train.shape[0], -1)\n",
        "            X_test = torch.FloatTensor(X_test).view(X_test.shape[0], -1)\n",
        "\n",
        "            # Convert labels to binary classification tensor\n",
        "            # Create binary labels based on a threshold (e.g., mean of original labels)\n",
        "            y_train_binary = (torch.FloatTensor(y_train).float() > np.mean(y_train)).float()\n",
        "            y_test_binary = (torch.FloatTensor(y_test).float() > np.mean(y_test)).float()\n",
        "\n",
        "            # Reshape labels to 2D tensor with shape [batch_size, 1]\n",
        "            y_train_binary = y_train_binary.view(-1, 1)\n",
        "            y_test_binary = y_test_binary.view(-1, 1)\n",
        "\n",
        "            # Verify tensor shapes after conversion\n",
        "            print(\"\\nðŸ”¬ Processed Tensor Shapes:\")\n",
        "            print(f\"X_train shape: {X_train.shape}\")\n",
        "            print(f\"y_train shape: {y_train_binary.shape}\")\n",
        "            print(f\"X_test shape: {X_test.shape}\")\n",
        "            print(f\"y_test shape: {y_test_binary.shape}\")\n",
        "\n",
        "            # Enhanced RNN Decoder\n",
        "            rnn_model = EnhancedRNNDecoder(input_size=X_train.size(1))\n",
        "            rnn_trainer = DecoderTrainer(rnn_model)\n",
        "\n",
        "            # Train Decoder\n",
        "            train_losses, val_losses = rnn_trainer.train(\n",
        "                X_train,\n",
        "                y_train_binary,\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            # Determine SNR range\n",
        "            snr_range = SNR_RANGE_AWGN if channel_name == 'AWGN' else SNR_RANGE_RAYLEIGH\n",
        "\n",
        "            # Performance Comparison\n",
        "            performance_results = performance_comparison(\n",
        "                rnn_trainer,\n",
        "                channel,\n",
        "                polar_code_gen,\n",
        "                snr_range,\n",
        "                channel_name,\n",
        "                LIST_SIZES\n",
        "            )\n",
        "\n",
        "            # Comprehensive Analysis Plot\n",
        "            plot_comprehensive_analysis(\n",
        "                rnn_trainer,\n",
        "                X_test,\n",
        "                y_test_binary,\n",
        "                channel_name,\n",
        "                train_losses,\n",
        "                val_losses,\n",
        "                performance_results,\n",
        "                snr_range,\n",
        "                LIST_SIZES\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            results[channel_name] = {\n",
        "                'decoder': rnn_trainer,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'performance': performance_results\n",
        "            }\n",
        "\n",
        "        logging.info(\"ðŸŽ‰ Simulation Complete!\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"ðŸ†˜ Comprehensive Simulation Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mMenq-htcmg3",
        "outputId": "3b9e643c-c66c-4217-91f9-3625b7235cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "ðŸš€ Using Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:ðŸ†˜ Comprehensive Simulation Error: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])) is deprecated. Please ensure they have the same size.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ” Tensor Verification for Initial Dataset:\n",
            "Input shape: (5000, 32)\n",
            "Input dtype: float64\n",
            "Label shape: (5000,)\n",
            "Label dtype: int64\n",
            "Label unique values: [0 1]\n",
            "\n",
            "ðŸ” Tensor Verification for Training Dataset:\n",
            "Input shape: (4000, 32)\n",
            "Input dtype: float64\n",
            "Label shape: (4000,)\n",
            "Label dtype: int64\n",
            "Label unique values: [0 1]\n",
            "\n",
            "ðŸ” Tensor Verification for Testing Dataset:\n",
            "Input shape: (1000, 32)\n",
            "Input dtype: float64\n",
            "Label shape: (1000,)\n",
            "Label dtype: int64\n",
            "Label unique values: [0 1]\n",
            "\n",
            "ðŸ”¬ Processed Tensor Shapes:\n",
            "X_train shape: torch.Size([4000, 32])\n",
            "y_train shape: torch.Size([4000, 1])\n",
            "X_test shape: torch.Size([1000, 32])\n",
            "y_test shape: torch.Size([1000, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-9-1df8caf8d5b4>\", line 1279, in main\n",
            "    train_losses, val_losses = rnn_trainer.train(\n",
            "                               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-1df8caf8d5b4>\", line 1072, in train\n",
            "    train_loss = self._train_epoch(train_loader)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-1df8caf8d5b4>\", line 1107, in _train_epoch\n",
            "    loss = self.criterion(outputs, batch_y)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\", line 699, in forward\n",
            "    return F.binary_cross_entropy(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 3560, in binary_cross_entropy\n",
            "    raise ValueError(\n",
            "ValueError: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])) is deprecated. Please ensure they have the same size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cyQW1fN5fhJ0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
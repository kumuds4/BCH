{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "13fecacc-1aaa-4dc0-c305-539ada1f64f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun 11 04:20:57 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             46W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1G82GuO-tez",
        "outputId": "f35a107f-27f7-491b-b02c-fd8c2946fcd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import logging\n",
        "import pandas as pd\n",
        "import traceback\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import LogFormatterMathtext\n",
        "from matplotlib.ticker import LogFormatterMathtext\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Configuration parameters\n",
        "BLOCK_LENGTH = 128\n",
        "INFO_BITS = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_SAMPLES_TRAIN = 100000\n",
        "NUM_TRIALS_PERF = 1500\n",
        "SNR_RANGE_AWGN = np.linspace(0, 10, 21)\n",
        "LIST_SIZES = [1, 8, 16]\n",
        "snr_db = 5.0     # <----- You can name this however you like!\n",
        "crc_poly = [1, 0, 0, 0, 1, 0, 0, 1]\n",
        "#generator = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS, crc_poly=crc_poly)\n",
        "#G = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS, crc_poly=crc7_poly)\n",
        "#DATASET_SNR_DB = 5.0 # Added configuration for dataset SNR\n",
        "\n",
        "\n",
        "#part 2\n",
        "\n",
        "def compute_crc(data, polynomial):\n",
        "    poly_len = len(polynomial)\n",
        "    crc_length = poly_len - 1\n",
        "    data_with_zeros = np.concatenate((data, np.zeros(crc_length, dtype=int)))\n",
        "    remainder = np.copy(data_with_zeros)\n",
        "    for i in range(len(remainder) - poly_len + 1):\n",
        "        if remainder[i] == 1:\n",
        "            remainder[i:i + poly_len] ^= polynomial\n",
        "    return remainder[-crc_length:]\n",
        "###################################################################################\n",
        "#Latest Polarcode generator\n",
        "#Latest Polarcode generator\n",
        "class PolarCodeGenerator:\n",
        "    \"\"\"\n",
        "    Polar Code Generator for N=128 (with CRC option, reliability sequence for info/frozen placement).\n",
        "    \"\"\"\n",
        "    # Reliability order for N=128 (first is most reliable, last is least reliable)\n",
        "    RELIABILITY_SEQUENCE_128 = [\n",
        "        0, 1, 2, 4, 8, 16, 3, 5, 6, 9, 10, 12, 17, 18, 20, 24,\n",
        "        32, 7, 11, 13, 14, 19, 21, 22, 25, 26, 28, 33, 34, 36, 38, 40,\n",
        "        15, 23, 27, 29, 30, 35, 37, 39, 41, 42, 44, 48, 31, 43, 45, 46,\n",
        "        49, 50, 52, 56, 47, 51, 53, 54, 57, 58, 60, 62, 63, 55, 59, 61,\n",
        "        64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79,\n",
        "        80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95,\n",
        "        96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109,\n",
        "        110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127\n",
        "    ]\n",
        "\n",
        "    def __init__(self, N=128, K=64, crc_poly=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            N (int): Codeword/block length (should match length of RELIABILITY_SEQUENCE_128)\n",
        "            K (int): Number of payload (info) bits (without CRC)\n",
        "            crc_poly (list, optional): CRC polynomial as a binary list, e.g. [1,0,0,0,1,0,0,1] for CRC-7\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc_poly = np.array(crc_poly, dtype=int) if crc_poly is not None else None\n",
        "\n",
        "        # Compute CRC length, total info bits, and info/frozen positions\n",
        "        self.crc_length = 0 if self.crc_poly is None else (len(self.crc_poly) - 1)\n",
        "        self.KwCRC = K + self.crc_length  # Total info bits (with CRC if used)\n",
        "\n",
        "        # Ensure RELIABILITY_SEQUENCE_128 is large enough for N\n",
        "        if N > len(self.RELIABILITY_SEQUENCE_128):\n",
        "             raise ValueError(f\"Reliability sequence only defined for N=128. Got N={N}\")\n",
        "        if self.KwCRC > N:\n",
        "             raise ValueError(f\"K + CRC length ({self.KwCRC}) cannot be greater than N ({N})\")\n",
        "\n",
        "        # Info bits are placed at the self.KwCRC most reliable positions\n",
        "        self.info_set = sorted(self.RELIABILITY_SEQUENCE_128[:self.KwCRC])\n",
        "        self.frozen_set = sorted(set(range(N)) - set(self.info_set))\n",
        "\n",
        "        logging.info(f\"Generator initialized: N={self.N}, K={self.K}, CRC Length={self.crc_length}, KwCRC={self.KwCRC}\")\n",
        "        # logging.info(f\"Info set indices: {self.info_set}\")\n",
        "        # logging.info(f\"Frozen set indices: {self.frozen_set}\")\n",
        "\n",
        "\n",
        "    def encode(self, payload_bits):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            payload_bits (array-like): Length K (payload, i.e. without CRC).\n",
        "        Returns:\n",
        "            codeword (np.array): Encoded polar codeword (length N)\n",
        "            info_bits_with_crc (np.array): Full info vector (payload + CRC if used, length KwCRC)\n",
        "        \"\"\"\n",
        "        payload_bits = np.array(payload_bits, dtype=int)\n",
        "        if len(payload_bits) != self.K:\n",
        "            raise ValueError(f\"Expected {self.K} payload bits, got {len(payload_bits)}\")\n",
        "\n",
        "        # Add CRC bits if needed\n",
        "        if self.crc_poly is not None:\n",
        "            # Use the global compute_crc or define it within the class if preferred\n",
        "            crc_bits = compute_crc(payload_bits, self.crc_poly)\n",
        "            info_bits_with_crc = np.concatenate([payload_bits, crc_bits])\n",
        "        else:\n",
        "            info_bits_with_crc = payload_bits\n",
        "\n",
        "        if len(info_bits_with_crc) != self.KwCRC:\n",
        "             raise RuntimeError(f\"CRC appending resulted in {len(info_bits_with_crc)} bits, expected {self.KwCRC}\")\n",
        "\n",
        "\n",
        "        # Place info bits at most reliable positions, rest frozen (zero)\n",
        "        u = np.zeros(self.N, dtype=int)\n",
        "        # Place info_bits_with_crc into the u vector at info_set indices\n",
        "        u[self.info_set] = info_bits_with_crc\n",
        "        codeword = self._arikan_transform(u)\n",
        "        return codeword, info_bits_with_crc\n",
        "\n",
        "    def _arikan_transform(self, u):\n",
        "        \"\"\"\n",
        "        Fast polar (ArÄ±kan) transform (in-place butterfly).\n",
        "        \"\"\"\n",
        "        N = len(u)\n",
        "        x = u.copy()\n",
        "        n = int(np.log2(N))\n",
        "        for i in range(n):\n",
        "            step = 2 ** i\n",
        "            for j in range(0, N, 2*step):\n",
        "                for k in range(step):\n",
        "                    x[j+k] ^= x[j+k+step]\n",
        "        return x\n",
        "\n",
        "    # Removed redundant compute_crc method here, using the global one\n",
        "\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        # Generates K information bits (payload)\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    # Removed _place_info_bits and _polar_encode_recursive, replaced by encode method\n",
        "\n",
        "    # Removed append_crc here, now handled inside encode method using the global compute_crc\n",
        "\n",
        "\n",
        "# --- Simulation Functions ---\n",
        "\n",
        "def bpsk_modulate(bits):\n",
        "    # Map 0 -> +1, 1 -> -1\n",
        "    bits = np.array(bits, dtype=int)\n",
        "    return 1 - 2*bits\n",
        "\n",
        "def add_awgn_noise(x, snr_db, code_rate):\n",
        "    \"\"\"\n",
        "    Adds AWGN noise. SNR here is Eb/N0, bit energy to noise ratio, in dB.\n",
        "    Requires code_rate (K_info / N or KwCRC / N). Using KwCRC/N for rate.\n",
        "    \"\"\"\n",
        "    snr_linear_eb = 10**(snr_db/10)\n",
        "    # Noise variance sigma^2 = N0/2.\n",
        "    # Eb/N0 = (Es/R) / N0, Es=1 for BPSK\n",
        "    # N0 = 1 / (R * (Eb/N0))\n",
        "    # sigma^2 = 1 / (2 * R * (Eb/N0)_linear)\n",
        "    noise_var = 1 / (2 * code_rate * snr_linear_eb)\n",
        "    noise = np.sqrt(noise_var) * np.random.randn(*x.shape)\n",
        "    return x + noise, noise_var\n",
        "\n",
        "def make_llr(received, noise_var):\n",
        "    return 2*received/noise_var\n",
        "\n",
        "def prepare_polar_dataset(gen, num_samples, snr_db, channel_type='AWGN'):\n",
        "    \"\"\"\n",
        "    Prepares a dataset of noisy channel outputs (LLRs) and corresponding\n",
        "    original information bits (payload + CRC).\n",
        "    \"\"\"\n",
        "    X_data = np.zeros((num_samples, gen.N))\n",
        "    # Correctly use gen.KwCRC for the size of information bits + CRC\n",
        "    y_data = np.zeros((num_samples, gen.KwCRC), dtype=int)\n",
        "\n",
        "    code_rate = gen.KwCRC / gen.N # Use the correct rate including CRC\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # 1. Generate K info bits (payload)\n",
        "        payload = gen.generate_info_bits()\n",
        "\n",
        "        # 2. Encode payload (handles CRC appending and polar transform internally)\n",
        "        codeword, info_bits_with_crc = gen.encode(payload)\n",
        "\n",
        "        # 3. BPSK modulate\n",
        "        x = bpsk_modulate(codeword)\n",
        "\n",
        "        # 4. Add noise and calculate LLRs\n",
        "        if channel_type == 'AWGN':\n",
        "            rx, noise_var = add_awgn_noise(x, snr_db, code_rate)\n",
        "            llr = make_llr(rx, noise_var)\n",
        "        # Add other channel types here if needed\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {channel_type}\")\n",
        "\n",
        "        # Store LLRs (received signal) and the original info_bits_with_crc\n",
        "        X_data[i] = llr\n",
        "        y_data[i] = info_bits_with_crc # y_data should be the bits we want to decode to\n",
        "\n",
        "    return X_data, y_data\n",
        "\n",
        "def save_dataset_to_csv(X, y, filename='dataset.csv'):\n",
        "    # X shape: (num_samples, N)\n",
        "    # y shape: (num_samples, KwCRC)\n",
        "    # Data to save: (num_samples, N + KwCRC)\n",
        "    data = np.hstack((X, y))\n",
        "    columns = [f'received_{i}' for i in range(X.shape[1])] + [f'info_bit_{j}' for j in range(y.shape[1])]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    df.to_csv(filename, index=False)\n",
        "    logging.info(f\"Dataset saved to {filename}\")\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################################################################\n",
        "#part 3\n",
        "#Rewrite\n",
        "######################################################\n",
        "\n",
        "def bpsk_modulate(bits):\n",
        "    bits = np.array(bits, dtype=int)\n",
        "    # Map 0 -> -1, 1 -> +1\n",
        "    return 2 * bits - 1\n",
        "\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def simulate(self, signal, snr_db):\n",
        "        # signal is assumed to be BPSK modulated (+1 or -1)\n",
        "        # Calculate noise variance based on SNR_db and signal power (which is 1 for BPSK)\n",
        "        # SNR_linear = E_s / N0, where E_s = 1 for BPSK\n",
        "        # Noise variance sigma^2 = N0/2.\n",
        "        # SNR_linear = 1 / N0 = 1 / (2 * sigma^2)\n",
        "        # sigma^2 = 1 / (2 * SNR_linear)\n",
        "        # Need to consider code rate R = K/N if SNR is E_b/N0\n",
        "        # E_b/N0 = (E_s/R) / N0 = (1/R) * (E_s/N0) = (1/R) * SNR_linear_Es\n",
        "        # SNR_linear_Es = R * SNR_linear_Eb\n",
        "        # sigma^2 = 1 / (2 * SNR_linear_Es) = 1 / (2 * R * SNR_linear_Eb)\n",
        "        # The PolarCodeGenerator stores K and N, but R is needed here.\n",
        "        # We need the code rate from the generator. Let's pass it or the generator object.\n",
        "\n",
        "        # Assuming snr_db provided is E_b/N0\n",
        "        # We need the code rate K/N used for the encoding. Let's assume the generator object is available or pass R.\n",
        "        # For now, let's assume R is available from a generator instance or a global var if constant.\n",
        "        # If we simulate *per block*, we need the specific R for that block (though N, K are fixed here).\n",
        "        # Let's assume R = INFO_BITS / BLOCK_LENGTH is constant.\n",
        "        R = INFO_BITS / BLOCK_LENGTH # This might need adjustment if K_crc is used for rate calc\n",
        "\n",
        "        snr_linear_eb = 10 ** (snr_db / 10)\n",
        "        # Noise variance for E_b/N0\n",
        "        noise_variance = 1 / (2 * R * snr_linear_eb)\n",
        "        noise_std = np.sqrt(noise_variance)\n",
        "        noise = noise_std * np.random.randn(*signal.shape)\n",
        "        return signal + noise\n",
        "\n",
        "\n",
        "def bpsk_modulate(bits):\n",
        "    # Map 0 -> +1, 1 -> -1\n",
        "    bits = np.array(bits, dtype=int)\n",
        "    return 1 - 2*bits\n",
        "\n",
        "def add_awgn_noise(x, snr_db, code_rate):\n",
        "    \"\"\"\n",
        "    Adds AWGN noise. SNR here is Eb/N0, bit energy to noise ratio, in dB.\n",
        "    Requires code_rate (K_info / N or KwCRC / N). Using KwCRC/N for rate.\n",
        "    \"\"\"\n",
        "    snr_linear_eb = 10**(snr_db/10)\n",
        "    # Noise variance sigma^2 = N0/2.\n",
        "    # Eb/N0 = (Es/R) / N0, Es=1 for BPSK\n",
        "    # N0 = 1 / (R * (Eb/N0))\n",
        "    # sigma^2 = 1 / (2 * R * (Eb/N0)_linear)\n",
        "    noise_var = 1 / (2 * code_rate * snr_linear_eb)\n",
        "    noise = np.sqrt(noise_var) * np.random.randn(*x.shape)\n",
        "    return x + noise, noise_var\n",
        "\n",
        "def make_llr(received, noise_var):\n",
        "    return 2*received/noise_var\n",
        "\n",
        "\n",
        "\n",
        "def prepare_polar_dataset(gen, num_samples, snr_db, channel_type='AWGN'):\n",
        "    \"\"\"\n",
        "    Prepares a dataset of noisy channel outputs (LLRs) and corresponding\n",
        "    original information bits (payload + CRC).\n",
        "    \"\"\"\n",
        "    X_data = np.zeros((num_samples, gen.N))\n",
        "    # Correctly use gen.KwCRC for the size of information bits + CRC\n",
        "    y_data = np.zeros((num_samples, gen.KwCRC), dtype=int)\n",
        "\n",
        "    code_rate = gen.KwCRC / gen.N # Use the correct rate including CRC\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # 1. Generate K info bits (payload)\n",
        "        payload = gen.generate_info_bits()\n",
        "\n",
        "        # 2. Encode payload (handles CRC appending and polar transform internally)\n",
        "        codeword, info_bits_with_crc = gen.encode(payload)\n",
        "\n",
        "        # 3. BPSK modulate\n",
        "        x = bpsk_modulate(codeword)\n",
        "\n",
        "        # 4. Add noise and calculate LLRs\n",
        "        if channel_type == 'AWGN':\n",
        "            rx, noise_var = add_awgn_noise(x, snr_db, code_rate)\n",
        "            llr = make_llr(rx, noise_var)\n",
        "        # Add other channel types here if needed\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {channel_type}\")\n",
        "\n",
        "        # Store LLRs (received signal) and the original info_bits_with_crc\n",
        "        X_data[i] = llr\n",
        "        y_data[i] = info_bits_with_crc # y_data should be the bits we want to decode to\n",
        "\n",
        "    return X_data, y_data\n",
        "\n",
        "\n",
        "def save_dataset_to_csv(X, y, filename='dataset.csv'):\n",
        "    # X shape: (num_samples, N)\n",
        "    # y shape: (num_samples, K_info)\n",
        "    # Data to save: (num_samples, N + K_info)\n",
        "    data = np.hstack((X, y))\n",
        "    columns = [f'received_{i}' for i in range(X.shape[1])] + [f'info_bit_{j}' for j in range(y.shape[1])]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    df.to_csv(filename, index=False)\n",
        "    logging.info(f\"Dataset saved to {filename}\")\n",
        "\n",
        "############################################################################################\n",
        "#latest part 4\n",
        " #----- Model -----\n",
        "#latest part 4\n",
        " #----- Model -----\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=128, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # Dropout is applied after each LSTM (except last)\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: [batch, code_length], reshape to [batch, seq=1, code_length]\n",
        "        x_reshaped = x.unsqueeze(1)  # [B, 1, N]\n",
        "        # LSTM expects sequence, but ours is just 1 timestep with full codeword\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "        out, _ = self.rnn(x_reshaped, (h0, c0))   # [B, 1, H]\n",
        "        out = self.fc(out[:, -1, :])              # [B, output_size]\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "# ----- Trainer -----\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "#latest Train Decoder\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate, early_stop_patience=10):\n",
        "        self.model = model\n",
        "        # Use BCEWithLogitsLoss for numerical stability if output layer was linear,\n",
        "        # but since Sigmoid is used, BCELoss is fine.\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.early_stop_patience = early_stop_patience\n",
        "\n",
        "    def train(self, train_X, train_y, val_X=None, val_y=None, epochs=100, batch_size=128, snr_min=1, snr_max=7, generator=None):\n",
        "        # generator: PolarCodeGenerator used for dynamic noisy batch creation!\n",
        "        train_losses, val_losses = [], []\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Ensure labels are float for BCELoss\n",
        "        train_y = train_y.float()\n",
        "        if val_y is not None:\n",
        "            val_y = val_y.float()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "\n",
        "            # Shuffle training set\n",
        "            indices = torch.randperm(train_X.shape[0])\n",
        "            train_X_shuffled = train_X[indices]\n",
        "            train_y_shuffled = train_y[indices]\n",
        "\n",
        "\n",
        "            self.model.train()\n",
        "            for i in range(0, len(train_X), batch_size):\n",
        "                X_batch = train_X_shuffled[i:i+batch_size]\n",
        "                y_batch = train_y_shuffled[i:i+batch_size]\n",
        "\n",
        "                # --- SNR randomization for each batch (curriculum learning) ---\n",
        "                batch_snr = np.random.uniform(snr_min, snr_max)\n",
        "                # To maximize effectiveness, re-generate batch with noise at selected SNR\n",
        "                # This is crucial for the RNN to learn robustness across SNRs.\n",
        "                # Need to generate the *original* info bits corresponding to the batch indices\n",
        "                # and then encode/noise them using the generator.\n",
        "                # This requires access to the original *payload* bits for these samples,\n",
        "                # which are not stored in train_y (train_y stores info_bits_with_crc).\n",
        "                # A better approach for re-noising is to save original payloads or info+crc bits\n",
        "                # alongside the LLRs in the dataset creation phase.\n",
        "                # Or, prepare_polar_dataset could return payload bits as a 3rd output.\n",
        "\n",
        "                # For now, we have X_batch (LLRs) and y_batch (info+crc).\n",
        "                # We need the *original* info_bits_with_crc corresponding to X_batch\n",
        "                # to re-generate the noisy LLRs. y_batch *is* the info_bits_with_crc.\n",
        "\n",
        "                # Re-generate batch using the generator\n",
        "                # This is computationally heavy, especially with a large dataset.\n",
        "                # An alternative is to pre-generate datasets at multiple SNRs or add\n",
        "                # noise directly to *un-noised* codewords if they were saved.\n",
        "                # Assuming y_batch are the info_bits_with_crc:\n",
        "                if generator is not None:\n",
        "                    # We need to generate 'num_samples=len(X_batch)' batches of noisy data\n",
        "                    # using the info bits from y_batch. The generator needs *payload* bits.\n",
        "                    # This structure is tricky. Let's stick to the simpler re-noising of LLRs for now,\n",
        "                    # acknowledging it's not perfectly simulating new transmissions but rather\n",
        "                    # changing the noise realization on existing data.\n",
        "                    # This approach below requires the original *un-noised* codeword or info+crc bits.\n",
        "                    # Let's assume y_batch are the info_bits_with_crc (0/1) as produced by prepare_polar_dataset.\n",
        "                    # We need the generator to encode these to get the codeword.\n",
        "                    # The generator's `encode` method takes *payload* bits.\n",
        "                    # We need a way to get payload from info_bits_with_crc or encode from info_bits_with_crc.\n",
        "\n",
        "                    # --- Alternative (Simpler but less ideal) ---\n",
        "                    # Just add noise to the LLR batch (treating them like codewords)\n",
        "                    # This doesn't correctly model the channel and encoding.\n",
        "                    # It's better to truly re-generate.\n",
        "\n",
        "                    # --- Correct Re-generation Approach ---\n",
        "                    # Requires saving payload bits in dataset or modifying prepare_polar_dataset\n",
        "                    # to return payload_bits as well.\n",
        "                    # Let's assume for now y_batch are the info_bits_with_crc.\n",
        "                    # We need to get the payload bits from y_batch.\n",
        "                    # y_batch = [payload_bits | crc_bits]\n",
        "                    # payload_batch = y_batch[:, :generator.K] # Requires this structure\n",
        "\n",
        "                    # Let's assume y_batch is info_bits_with_crc and prepare_polar_dataset\n",
        "                    # is modified to return info_bits_with_crc correctly.\n",
        "                    # The generator.encode takes payload, not info_bits_with_crc.\n",
        "\n",
        "                    # --- Revisit prepare_polar_dataset and encoding flow ---\n",
        "                    # prepare_polar_dataset already generates payload, appends CRC,\n",
        "                    # places them into 'u', encodes 'u', then adds noise to the codeword.\n",
        "                    # It returns the noisy LLRs (X_data) and the original info_bits_with_crc (y_data).\n",
        "                    # To re-noise, we need the info_bits_with_crc (which is y_batch) and the generator.\n",
        "                    # We can then re-run the encoding/modulation/noise steps for this batch.\n",
        "\n",
        "                    # Get info_bits_with_crc (already in y_batch)\n",
        "                    info_bits_with_crc_batch = y_batch.cpu().int().numpy() # Convert to numpy int bits\n",
        "\n",
        "                    # Re-generate noisy LLRs for this batch\n",
        "                    X_noisy_batch = np.zeros_like(X_batch.cpu().numpy())\n",
        "                    code_rate = generator.KwCRC / generator.N\n",
        "\n",
        "                    # This loop is slow, should be vectorized if possible\n",
        "                    for k in range(len(info_bits_with_crc_batch)):\n",
        "                         # Need the 'u' vector to encode. Generator needs payload bits for .encode()\n",
        "                         # Let's assume info_bits_with_crc_batch[k] is payload + CRC\n",
        "                         # To use generator.encode(), we need payload_bits_batch[k]\n",
        "                         # This means prepare_polar_dataset *must* return payload_bits as well\n",
        "\n",
        "                         # Simpler: Assume y_batch contains the info+crc bits\n",
        "                         # Place info+crc bits into the u vector\n",
        "                         u_vec = np.zeros(generator.N, dtype=int)\n",
        "                         # Ensure info_bits_with_crc_batch[k] length matches generator.KwCRC\n",
        "                         # This relies on y_batch being the correct KwCRC length\n",
        "                         u_vec[generator.info_set] = info_bits_with_crc_batch[k] # Place info+crc bits\n",
        "\n",
        "                         # Encode u_vec\n",
        "                         codeword = generator._arikan_transform(u_vec) # Directly use transform on u\n",
        "\n",
        "                         # BPSK modulate\n",
        "                         x = bpsk_modulate(codeword)\n",
        "\n",
        "                         # Add noise and calculate LLRs\n",
        "                         rx, noise_var = add_awgn_noise(x, batch_snr, code_rate)\n",
        "                         llr = make_llr(rx, noise_var)\n",
        "                         X_noisy_batch[k] = llr\n",
        "\n",
        "                    X_noisy = torch.tensor(X_noisy_batch, dtype=torch.float32).to(X_batch.device)\n",
        "                else:\n",
        "                    # If no generator provided, use the original batch LLRs (less ideal training)\n",
        "                    logging.warning(\"Generator not provided to Trainer. Using original LLRs without re-noising.\")\n",
        "                    X_noisy = X_batch\n",
        "\n",
        "\n",
        "                outputs = self.model(X_noisy)\n",
        "                loss = self.criterion(outputs, y_batch) # y_batch is already float\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            train_losses.append(epoch_loss / (len(train_X) // batch_size))\n",
        "\n",
        "            # ----- Validation -----\n",
        "            if val_X is not None and val_y is not None:\n",
        "                self.model.eval()\n",
        "                # Evaluate on validation data at fixed SNR (median), or optional noise averaging\n",
        "                val_snr = (snr_min + snr_max) / 2.0\n",
        "\n",
        "                # Re-generate validation batch with noise at val_snr\n",
        "                if generator is not None:\n",
        "                     info_bits_with_crc_val = val_y.cpu().int().numpy() # Convert to numpy int bits\n",
        "                     X_noisy_val_batch = np.zeros_like(val_X.cpu().numpy())\n",
        "                     code_rate = generator.KwCRC / generator.N\n",
        "                     for k in range(len(info_bits_with_crc_val)):\n",
        "                          u_vec = np.zeros(generator.N, dtype=int)\n",
        "                          u_vec[generator.info_set] = info_bits_with_crc_val[k]\n",
        "                          codeword = generator._arikan_transform(u_vec)\n",
        "                          x = bpsk_modulate(codeword)\n",
        "                          rx, noise_var = add_awgn_noise(x, val_snr, code_rate)\n",
        "                          llr = make_llr(rx, noise_var)\n",
        "                          X_noisy_val_batch[k] = llr\n",
        "                     X_noisy_val = torch.tensor(X_noisy_val_batch, dtype=torch.float32).to(val_X.device)\n",
        "                else:\n",
        "                     X_noisy_val = val_X\n",
        "\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    val_output = self.model(X_noisy_val)\n",
        "                    # Ensure val_y is float for criterion\n",
        "                    val_loss = self.criterion(val_output, val_y).item()\n",
        "                val_losses.append(val_loss)\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{epochs}    Train Loss: {train_losses[-1]:.4f}   Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "                # Early stopping\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    patience_counter = 0\n",
        "                    torch.save(self.model.state_dict(), \"best_rnn_decoder.pt\")\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= self.early_stop_patience:\n",
        "                        print(\"Early stopping activated.\")\n",
        "                        break\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}    Train Loss: {train_losses[-1]:.4f}\")\n",
        "\n",
        "        # Load the best model weights if early stopping occurred\n",
        "        if patience_counter >= self.early_stop_patience and val_X is not None:\n",
        "             self.model.load_state_dict(torch.load(\"best_rnn_decoder.pt\"))\n",
        "             print(\"Loaded best model.\")\n",
        "\n",
        "        return train_losses, val_losses if len(val_losses)>0 else None\n",
        "\n",
        "    def evaluate(self, X_test, y_test, threshold=0.5):\n",
        "        self.model.eval()\n",
        "        # Ensure y_test is int for comparison\n",
        "        y_test_int = y_test.int()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_test)\n",
        "            preds = (outputs > threshold).int()\n",
        "            total = y_test_int.numel()\n",
        "            bit_errors = torch.sum(preds != y_test_int).item()\n",
        "            block_errors = torch.sum(torch.any(preds != y_test_int, dim=1)).item()\n",
        "            ber = bit_errors / total\n",
        "            bler = block_errors / X_test.size(0)\n",
        "        return ber, bler\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "        return ber, bler\n",
        "\n",
        "# Removed KwCRC definition here, it's now in the enerator\n",
        "\n",
        " ############################################################################################\n",
        "\n",
        "\n",
        "\n",
        "# Part 5\n",
        "############################################################################\n",
        "#Rewrite\n",
        "class PolarCodeDecoder:\n",
        "    \"\"\"\n",
        "    Polar Code Decoder supporting both SC and SCL (list) decoding with optional CRC checking.\n",
        "    \"\"\"\n",
        "    def __init__(self, N, K, list_size=1, crc_poly=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            N (int): Block length (codeword length)\n",
        "            K (int): Number of information bits (including CRC bits if any)\n",
        "            list_size (int): Number of SCL paths (set to 1 for regular SC)\n",
        "            crc_poly (list or np.array, optional): Binary coefficients of CRC polynomial, e.g. [1,0,1,1]\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.list_size = list_size\n",
        "        self.crc_poly = crc_poly\n",
        "        # By default: frozen set is the last N-K bits (can be changed for specific design)\n",
        "        self.frozen_set = set(range(self.K, self.N))\n",
        "        self.info_set = sorted(set(range(self.N)) - self.frozen_set)\n",
        "        # CRC properties\n",
        "        if crc_poly is not None:\n",
        "            self.crc_length = len(crc_poly) - 1\n",
        "        else:\n",
        "            self.crc_length = 0\n",
        "\n",
        "    def encode_with_crc(self, payload_bits):\n",
        "        \"\"\"\n",
        "        Attach CRC to the payload_bits if CRC polynomial is specified.\n",
        "        Args:\n",
        "            payload_bits (array-like): Length = K - crc_length\n",
        "        Returns:\n",
        "            code_bits (np.array): Length = K (payload + CRC)\n",
        "        \"\"\"\n",
        "        payload_bits = np.array(payload_bits, dtype=int)\n",
        "        if self.crc_poly is not None:\n",
        "            crc_bits = self.compute_crc(payload_bits, self.crc_poly)\n",
        "            code_bits = np.concatenate([payload_bits, crc_bits])\n",
        "            return code_bits\n",
        "        else:\n",
        "            return payload_bits\n",
        "\n",
        "    def decode(self, llr):\n",
        "        \"\"\"\n",
        "        Decodes the input LLRs using SC or SCL decoder.\n",
        "        Args:\n",
        "            llr (np.array): Array of length N (codeword LLRs)\n",
        "        Returns:\n",
        "            info_bits (np.array): Decoded information bits (including CRC bits if present)\n",
        "        \"\"\"\n",
        "        if self.list_size == 1:\n",
        "            # SC decoder\n",
        "            u_hat = self._sc_decode(llr)\n",
        "            info_bits = u_hat[list(self.info_set)]\n",
        "            # CRC check if present\n",
        "            if self.crc_poly is not None:\n",
        "                if not self.crc_check(info_bits):\n",
        "                    print('CRC failed in SC!')\n",
        "            return info_bits\n",
        "        else:\n",
        "            # SCL decoder\n",
        "            info_bits = self._scl_decode(llr)\n",
        "            return info_bits\n",
        "\n",
        "    def _sc_decode(self, llr):\n",
        "        \"\"\"Basic SC decoding.\"\"\"\n",
        "        u_hat = np.zeros(self.N, dtype=int)\n",
        "        for i in range(self.N):\n",
        "            if i in self.frozen_set:\n",
        "                u_hat[i] = 0\n",
        "            else:\n",
        "                u_hat[i] = 0 if llr[i] >= 0 else 1\n",
        "        return u_hat\n",
        "\n",
        "    def _scl_decode(self, llr):\n",
        "        \"\"\"\n",
        "        Simple SCL decoding. For each unfrozen index, splits the path,\n",
        "        keeps most likely list_size paths. CRC is checked on valid codewords.\n",
        "        \"\"\"\n",
        "        paths = [([], 0.0)]  # Each entry: (path_bits, path_metric)\n",
        "        for i in range(self.N):\n",
        "            new_paths = []\n",
        "            for bits, metric in paths:\n",
        "                if i in self.frozen_set:\n",
        "                    # Only extend with 0\n",
        "                    new_bits = bits + [0]\n",
        "                    new_metric = metric + self._metric(llr[i], 0)\n",
        "                    new_paths.append((new_bits, new_metric))\n",
        "                else:\n",
        "                    # Extend with both 0 and 1\n",
        "                    for bit in [0, 1]:\n",
        "                        ext_bits = bits + [bit]\n",
        "                        ext_metric = metric + self._metric(llr[i], bit)\n",
        "                        new_paths.append((ext_bits, ext_metric))\n",
        "            # Prune paths: keep only best list_size\n",
        "            new_paths.sort(key=lambda x: x[1])\n",
        "            paths = new_paths[:self.list_size]\n",
        "\n",
        "        # At the end: check CRC if needed\n",
        "        info_indices = list(self.info_set)\n",
        "        if self.crc_poly is not None:\n",
        "            for bits, metric in paths:\n",
        "                info_bits = np.array([bits[i] for i in info_indices], dtype=int)\n",
        "                if self.crc_check(info_bits):\n",
        "                    return info_bits\n",
        "            # No CRC-valid path found; fall back to best metric path\n",
        "            print(\"Warning: No CRC-valid candidate; using lowest metric path.\")\n",
        "        # Return info bits from best path\n",
        "        best_bits = paths[0][0]\n",
        "        return np.array([best_bits[i] for i in info_indices], dtype=int)\n",
        "\n",
        "    @staticmethod\n",
        "    def _metric(llr, bit):\n",
        "        \"\"\"Path metric increment for bit decision 0/1 given LLR.\"\"\"\n",
        "        # log(1+exp(-llr)) for bit=0, log(1+exp(llr)) for bit=1\n",
        "        return np.log1p(np.exp(-llr)) if bit == 0 else np.log1p(np.exp(llr))\n",
        "\n",
        "    def crc_check(self, info_bits):\n",
        "        \"\"\"\n",
        "        Checks CRC on info_bits (expects last crc_length bits are CRC).\n",
        "        Returns True if CRC matches, False otherwise.\n",
        "        \"\"\"\n",
        "        if self.crc_poly is None or self.crc_length == 0:\n",
        "            return True\n",
        "        poly = np.array(self.crc_poly, dtype=int)\n",
        "        data = np.array(info_bits, dtype=int)\n",
        "        payload = data[:-self.crc_length]\n",
        "        crc_bits = data[-self.crc_length:]\n",
        "        calc_crc = self.compute_crc(payload, poly)\n",
        "        return np.array_equal(calc_crc, crc_bits)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_crc(data, polynomial):\n",
        "        \"\"\"\n",
        "        Computes CRC bits (classic binary modulo-2 division).\n",
        "        Args:\n",
        "            data (np.array): Info bits, shape (payload length,)\n",
        "            polynomial (array-like): Polynomial, e.g., [1,0,1,1]\n",
        "        Returns:\n",
        "            np.array: CRC bits, length len(polynomial)-1\n",
        "        \"\"\"\n",
        "        data = np.array(data, dtype=int)\n",
        "        polynomial = np.array(polynomial, dtype=int)\n",
        "        crc_length = len(polynomial) - 1\n",
        "        data_padded = np.concatenate([data, np.zeros(crc_length, dtype=int)])\n",
        "        for i in range(len(data)):\n",
        "            if data_padded[i] == 1:\n",
        "                data_padded[i:i+len(polynomial)] ^= polynomial\n",
        "        return data_padded[-crc_length:] if crc_length > 0 else np.array([], dtype=int)\n",
        "###############################################################################\n",
        "#Performance comparison\n",
        "def performance_comparison(\n",
        "        rnn_trainer,  # object with .model and .evaluate(X, y) method\n",
        "        polar_code_gen,  # PolarCodeGenerator object\n",
        "        snr_range,  # list/array of SNRs in dB\n",
        "        channel_type,  # e.g., 'AWGN'\n",
        "        list_sizes,  # list of ints (for SCL)\n",
        "        num_trials,  # int\n",
        "    ):\n",
        " #   import numpy as np\n",
        "    device = next(rnn_trainer.model.parameters()).device\n",
        "    BLOCK_LENGTH = polar_code_gen.N\n",
        "    INFO_BITS = polar_code_gen.K\n",
        "    # Initialize results data structures\n",
        "    rnn_results = {'BER_RNN': [], 'BLER_RNN': []}\n",
        "    sc_results = {'BER_SC': [], 'BLER_SC': []}\n",
        "    scl_results = {L: [] for L in list_sizes}\n",
        "\n",
        "    # Loop over SNRs\n",
        "    for snr_db in snr_range:\n",
        "        # === Generate new data for this SNR ===\n",
        "        X_eval, y_eval = prepare_polar_dataset(\n",
        "            polar_code_gen, num_samples=num_trials, snr_db=snr_db, channel_type=channel_type\n",
        "        )\n",
        "        X_tensor = torch.FloatTensor(X_eval).to(device)\n",
        "        y_tensor = torch.FloatTensor(y_eval).to(device)\n",
        "\n",
        "        # --- RNN Decoder ---\n",
        "        ber_rnn, bler_rnn = rnn_trainer.evaluate(X_tensor, y_tensor)\n",
        "        rnn_results['BER_RNN'].append(ber_rnn)\n",
        "        rnn_results['BLER_RNN'].append(bler_rnn)\n",
        "\n",
        "        # --- SC Decoder (list_size=1) ---\n",
        "        sc_decoder = PolarCodeDecoder(polar_code_gen.N, polar_code_gen.K, list_size=1, crc_poly=polar_code_gen.crc_poly)\n",
        "        bit_errors, blk_errors = 0, 0\n",
        "        for i in range(num_trials):\n",
        "            codeword = X_eval[i]  # noisy codeword (received/vector LLR or sysmetric, as needed)\n",
        "            true_bits = y_eval[i]\n",
        "            llr = codeword  # If codeword is already LLR/soft; adjust as needed for your channel\n",
        "            decoded_bits = sc_decoder.decode(llr)\n",
        "            bit_errors += np.sum(decoded_bits != true_bits)\n",
        "            blk_errors += int(np.any(decoded_bits != true_bits))\n",
        "        sc_results['BER_SC'].append(bit_errors / (num_trials * polar_code_gen.K))\n",
        "        sc_results['BLER_SC'].append(blk_errors / num_trials)\n",
        "\n",
        "        # --- SCL Decoders ---\n",
        "        for L in list_sizes:\n",
        "            scl_decoder = PolarCodeDecoder(polar_code_gen.N, polar_code_gen.K, list_size=L, crc_poly=polar_code_gen.crc_poly)\n",
        "            bit_errors, blk_errors = 0, 0\n",
        "            for i in range(num_trials):\n",
        "                codeword = X_eval[i]\n",
        "                true_bits = y_eval[i]\n",
        "                llr = codeword\n",
        "                decoded_bits = scl_decoder.decode(llr)\n",
        "                bit_errors += np.sum(decoded_bits != true_bits)\n",
        "                blk_errors += int(np.any(decoded_bits != true_bits))\n",
        "            scl_results[L].append({'BER': bit_errors / (num_trials * polar_code_gen.K), 'BLER': blk_errors / num_trials})\n",
        "    return rnn_results, scl_results, sc_results\n",
        "#############################################################################\n",
        "#part 6 Plotting fuctions\n",
        "\n",
        "def plot_ber_bler_comparison(snr_range, rnn_results, scl_results, sc_results, list_sizes):\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BER_RNN'], label='RNN')\n",
        "    for size in list_sizes:\n",
        "        plt.plot(snr_range, [result['BER'] for result in scl_results[size]], label=f'SCL, List Size {size}')\n",
        "    plt.plot(snr_range, sc_results['BER_SC'], label='SC')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('Bit Error Rate (BER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BLER_RNN'], label='RNN')\n",
        "    for size in list_sizes:\n",
        "        plt.plot(snr_range, [result['BLER'] for result in scl_results[size]], label=f'SCL, List Size {size}')\n",
        "    plt.plot(snr_range, sc_results['BLER_SC'], label='SC')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('Block Error Rate (BLER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# for RNN and SCL\n",
        "\n",
        "\n",
        "def plot_rnn_ber_bler(snr_range, rnn_results):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BER_RNN'], marker='o', label='RNN BER')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('RNN Bit Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BLER_RNN'], marker='o', label='RNN BLER')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('RNN Block Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "#2. SCL: Specialized BER/BLER Plot (for a single list size at a time)\n",
        "\n",
        "\n",
        "def plot_sc_ber_bler(snr_range, sc_results):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, sc_results['BER_SC'], marker='s', label='SC BER')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('SC Bit Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, sc_results['BLER_SC'], marker='s', label='SC BLER')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('SC Block Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#SCL Plotting Function (ALL list sizes in ONE figure)\n",
        "\n",
        "def plot_scl_ber_bler(snr_range, scl_results, list_sizes):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    for L in list_sizes:\n",
        "        ber_list = [res['BER'] for res in scl_results[L]]\n",
        "        plt.plot(snr_range, ber_list, marker='^', label=f'SCL L={L}')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('SCL Bit Error Rate (All List Sizes)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    for L in list_sizes:\n",
        "        bler_list = [res['BLER'] for res in scl_results[L]]\n",
        "        plt.plot(snr_range, bler_list, marker='^', label=f'SCL L={L}')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('SCL Block Error Rate (All List Sizes)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    ax = plt.gca()\n",
        "    ax.yaxis.set_major_formatter(LogFormatterMathtext())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "######################################################################\n",
        "def plot_training_validation(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    if val_losses:\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_ber_bler_comparison(snr_range, rnn_results, scl_results, sc_results, list_sizes):\n",
        "    plt.figure(figsize=(18, 6))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BER_RNN'], label='RNN')\n",
        "    for size in list_sizes:\n",
        "        plt.plot(snr_range, [result['BER'] for result in scl_results[size]], label=f'SCL, List Size {size}')\n",
        "    plt.plot(snr_range, sc_results['BER_SC'], label='SC')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('Bit Error Rate (BER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BLER_RNN'], label='RNN')\n",
        "    for size in list_sizes:\n",
        "        plt.plot(snr_range, [result['BLER'] for result in scl_results[size]], label=f'SCL, List Size {size}')\n",
        "    plt.plot(snr_range, sc_results['BLER_SC'], label='SC')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('Block Error Rate (BLER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "#Part 7: Main Function\n",
        "###############################################################################\n",
        "#latest main()\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "def main():\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Set up the device\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Configuration parameters\n",
        "        BLOCK_LENGTH = 128\n",
        "        INFO_BITS = 64\n",
        "        LEARNING_RATE = 1e-3\n",
        "        EPOCHS = 100\n",
        "        BATCH_SIZE = 32\n",
        "        NUM_SAMPLES_TRAIN = 100000\n",
        "        NUM_TRIALS_PERF = 1500\n",
        "        SNR_RANGE_AWGN = np.linspace(0, 10, 21)\n",
        "        LIST_SIZES = [1, 8, 16]\n",
        "        snr_db = 5.0     # <----- You can name this however you like!\n",
        "       # crc7_poly = [1, 0, 0, 0, 1, 0, 0, 1]\n",
        "       # G = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS, crc_poly=crc7_poly)\n",
        "       # DATASET_SNR_DB = 5.0 # Added configuration for dataset SNR\n",
        "\n",
        "\n",
        "        # Initialize polar code generator and RNN model/trainer\n",
        "        polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "        rnn_model = EnhancedRNNDecoder(BLOCK_LENGTH, INFO_BITS).to(device)\n",
        "        rnn_trainer = DecoderTrainer(rnn_model, LEARNING_RATE)\n",
        "\n",
        "        # Generate dataset and save (if needed)\n",
        "        X_raw, y_raw = prepare_polar_dataset(\n",
        "            polar_code_gen, num_samples=NUM_SAMPLES_TRAIN, snr_db=5.0, channel_type='AWGN'\n",
        "        )\n",
        "        save_dataset_to_csv(X_raw, y_raw, 'awgn_dataset.csv')\n",
        "\n",
        "        # Tensor conversion and splitting\n",
        "        X_tensor = torch.FloatTensor(X_raw).view(-1, BLOCK_LENGTH).to(device)\n",
        "        y_tensor = torch.FloatTensor(y_raw).view(-1, INFO_BITS).to(device)\n",
        "        train_size = int(0.8 * X_tensor.shape[0])\n",
        "        train_X = X_tensor[:train_size]\n",
        "        train_y = y_tensor[:train_size]\n",
        "        val_X = X_tensor[train_size:]\n",
        "        val_y = y_tensor[train_size:]\n",
        "\n",
        "        # Train RNN model\n",
        "        train_losses, val_losses = rnn_trainer.train(\n",
        "            train_X, train_y, X_val=val_X, y_val=val_y, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # ---- Performance Evaluation ----\n",
        "\n",
        "        # Performance comparison: obtain RNN, SCL, and SC results\n",
        "        # You must make sure your performance_comparison returns these three!\n",
        "        rnn_results, scl_results, sc_results = performance_comparison(\n",
        "            rnn_trainer, polar_code_gen, SNR_RANGE_AWGN, 'AWGN', LIST_SIZES, NUM_TRIALS_PERF\n",
        "        )\n",
        "\n",
        "        # ---- Plot losses ----\n",
        "        plot_training_validation(train_losses, val_losses)\n",
        "\n",
        "        # Plot RNN results\n",
        "        plot_rnn_ber_bler(SNR_RANGE_AWGN, rnn_results)\n",
        "\n",
        "# Plot SC results\n",
        "        plot_sc_ber_bler(SNR_RANGE_AWGN, sc_results)\n",
        "\n",
        "# Plot SCL results (all list sizes in one plot)\n",
        "        plot_scl_ber_bler(SNR_RANGE_AWGN, scl_results, LIST_SIZES)\n",
        "        # ---- (Optional) Plot all for comparison ----\n",
        "        plot_ber_bler_comparison(\n",
        "            SNR_RANGE_AWGN,\n",
        "            rnn_results,\n",
        "            scl_results,\n",
        "            sc_results,\n",
        "            LIST_SIZES\n",
        "        )\n",
        "\n",
        "        # ---- (Optional) Example Confusion Matrix for RNN on validation data ----\n",
        "        from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "        y_true_example = val_y[:100].cpu().numpy()\n",
        "        rnn_input_example = val_X[:100]\n",
        "        rnn_output_prob_example = rnn_trainer.model(rnn_input_example).cpu().detach().numpy()\n",
        "        rnn_output_example = (rnn_output_prob_example > 0.5).astype(int)\n",
        "        y_pred_example = rnn_output_example.squeeze()\n",
        "        plot_confusion_matrix(\n",
        "            y_true_example.flatten(),\n",
        "            y_pred_example.flatten(),\n",
        "            title='RNN Confusion Matrix'\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Simulation Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLE4rmpyGpSC",
        "outputId": "83ae0eed-967a-4c38-be9d-129e06e649af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulation Error: DecoderTrainer.train() got an unexpected keyword argument 'X_val'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-1-550881298>\", line 1050, in main\n",
            "    train_losses, val_losses = rnn_trainer.train(\n",
            "                               ^^^^^^^^^^^^^^^^^^\n",
            "TypeError: DecoderTrainer.train() got an unexpected keyword argument 'X_val'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "kxhQ_d9nQr3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
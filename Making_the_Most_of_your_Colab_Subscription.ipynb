{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jB9yVlla4byD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive Polar Code Simulation Framework\n",
        "#Today May 17, 2025\n",
        "# Essential Scientific and Deep Learning Libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Machine Learning and Data Handling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Visualization and Scientific Computing\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.special as sps\n",
        "\n",
        "# System and Utilities\n",
        "import logging\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "# Logging Configuration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Device Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸš€ Using Device: {DEVICE}\")\n",
        "\n",
        "#Part 2: Polar Code Generator with CRC\n",
        "\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N, K, crc_type='CRC-7'):\n",
        "        \"\"\"\n",
        "        Polar Code Generator with CRC support\n",
        "\n",
        "        Args:\n",
        "            N (int): Total code length\n",
        "            K (int): Information bit length\n",
        "            crc_type (str): CRC polynomial type\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc_type = crc_type\n",
        "\n",
        "        # CRC Polynomials\n",
        "        self.crc_polynomials = {\n",
        "            'CRC-7': {\n",
        "                'polynomial': [1, 1, 1, 0, 0, 1, 1],\n",
        "                'length': 7\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        \"\"\"\n",
        "        Generate random information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Random information bits\n",
        "        \"\"\"\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def compute_crc(self, bits):\n",
        "        \"\"\"\n",
        "        Compute CRC checksum using polynomial division\n",
        "\n",
        "        Args:\n",
        "            bits (np.ndarray): Input bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: CRC checksum bits\n",
        "        \"\"\"\n",
        "        poly_info = self.crc_polynomials.get(self.crc_type)\n",
        "        if not poly_info:\n",
        "            raise ValueError(f\"Unsupported CRC type: {self.crc_type}\")\n",
        "\n",
        "        polynomial = poly_info['polynomial']\n",
        "        crc_length = poly_info['length']\n",
        "\n",
        "        # Convert input to list and pad\n",
        "        message = bits.tolist() + [0] * crc_length\n",
        "\n",
        "        # Polynomial long division\n",
        "        for i in range(len(message) - crc_length):\n",
        "            if message[i] == 1:\n",
        "                for j in range(crc_length + 1):\n",
        "                    message[i + j] ^= polynomial[j] if j < len(polynomial) else 0\n",
        "\n",
        "        # Return the last 'crc_length' bits\n",
        "        return np.array(message[-crc_length:], dtype=int)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Polar Code Encoding with CRC\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Append CRC\n",
        "        crc_bits = self.compute_crc(info_bits)\n",
        "        extended_info_bits = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Basic polar encoding (placeholder)\n",
        "        codeword = np.zeros(self.N, dtype=int)\n",
        "        codeword[:len(extended_info_bits)] = extended_info_bits\n",
        "\n",
        "        return codeword\n",
        "\n",
        "    def verify_codeword(self, codeword):\n",
        "        \"\"\"\n",
        "        Verify codeword using CRC\n",
        "\n",
        "        Args:\n",
        "            codeword (np.ndarray): Received codeword\n",
        "\n",
        "        Returns:\n",
        "            bool: True if CRC check passes, False otherwise\n",
        "        \"\"\"\n",
        "        poly_info = self.crc_polynomials.get(self.crc_type)\n",
        "        if not poly_info:\n",
        "            raise ValueError(f\"Unsupported CRC type: {self.crc_type}\")\n",
        "\n",
        "        crc_length = poly_info['length']\n",
        "\n",
        "        # Extract information and CRC bits\n",
        "        info_bits = codeword[:-crc_length]\n",
        "        received_crc = codeword[-crc_length:]\n",
        "\n",
        "        # Compute CRC of information bits\n",
        "        computed_crc = self.compute_crc(info_bits)\n",
        "\n",
        "        # Compare received and computed CRC\n",
        "        return np.array_equal(received_crc, computed_crc)\n",
        "\n",
        "  #Part 3\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Advanced Channel Simulator for communication systems\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Channel type ('AWGN' or 'Rayleigh')\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "        logging.info(f\"Initializing {channel_type} Channel Simulator\")\n",
        "\n",
        "    def simulate(self, encoded_signal, snr_db):\n",
        "        \"\"\"\n",
        "        Simulate signal transmission through specified channel\n",
        "\n",
        "        Args:\n",
        "            encoded_signal (np.ndarray): Input encoded signal\n",
        "            snr_db (float): Signal-to-Noise Ratio in decibels\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received signal after channel effects\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert input to numpy array\n",
        "            encoded_signal = np.array(encoded_signal, dtype=float)\n",
        "\n",
        "            # Convert bits {0,1} to BPSK: {+1, -1}\n",
        "            bpsk_signal = 1 - 2 * encoded_signal\n",
        "\n",
        "            # Convert SNR from dB to linear scale\n",
        "            snr_linear = 10 ** (snr_db / 10)\n",
        "\n",
        "            # Compute signal power\n",
        "            signal_power = np.mean(bpsk_signal**2)\n",
        "\n",
        "            # Noise power calculation\n",
        "            noise_power = signal_power / snr_linear\n",
        "            noise_std = np.sqrt(noise_power / 2.0)\n",
        "\n",
        "            # Channel-specific simulation\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # Additive White Gaussian Noise\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = bpsk_signal + noise\n",
        "\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                # Rayleigh Fading Channel\n",
        "                fading = np.random.rayleigh(scale=1.0, size=bpsk_signal.shape)\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = fading * bpsk_signal + noise\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Convert back to binary representation\n",
        "            return (received_signal > 0).astype(float)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Channel simulation error: {e}\")\n",
        "            return encoded_signal\n",
        "\n",
        "    def compute_theoretical_performance(self, block_length, snr_linear):\n",
        "        \"\"\"\n",
        "        Compute theoretical Bit Error Probability (BEP) and Block Error Probability (BLER)\n",
        "\n",
        "        Args:\n",
        "            block_length (int): Length of the code block\n",
        "            snr_linear (np.ndarray): SNR in linear scale\n",
        "\n",
        "        Returns:\n",
        "            tuple: Theoretical BEP and BLER\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # AWGN Channel Theoretical Performance\n",
        "                # Bit Error Probability using Q-function\n",
        "                bep = 0.5 * sps.erfc(np.sqrt(snr_linear))\n",
        "\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                # Rayleigh Fading Channel Theoretical Performance\n",
        "                # Average Bit Error Probability for Rayleigh fading\n",
        "                bep = 0.5 * (1 - np.sqrt(snr_linear / (1 + snr_linear)))\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Block Error Probability (assuming independent bit errors)\n",
        "            bler = 1 - (1 - bep) ** block_length\n",
        "\n",
        "            return bep, bler\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Theoretical performance computation error: {e}\")\n",
        "            # Return default values if computation fails\n",
        "            return np.zeros_like(snr_linear), np.ones_like(snr_linear)\n",
        "\n",
        "    def plot_channel_capacity(self, snr_range):\n",
        "        \"\"\"\n",
        "        Plot channel capacity for the specific channel type\n",
        "\n",
        "        Args:\n",
        "            snr_range (np.ndarray): Range of SNR values in dB\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Compute channel capacities\n",
        "        snr_linear = 10 ** (snr_range / 10)\n",
        "        capacities = [np.log2(1 + snr) for snr in snr_linear]\n",
        "\n",
        "        plt.plot(snr_range, capacities, label=f'{self.channel_type} Channel')\n",
        "        plt.title(f'Channel Capacity - {self.channel_type} Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Capacity (bits/channel use)')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "# Utility function for dataset preparation\n",
        "################################################\n",
        "#Latest dataset prep\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type=\"AWGN\"):\n",
        "    \"\"\"\n",
        "    Prepare dataset for Polar Code simulation with robust preprocessing\n",
        "\n",
        "    Args:\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "        num_samples (int): Number of samples to generate\n",
        "        snr_db (float): Signal-to-Noise Ratio in dB\n",
        "        channel_type (str): Channel type\n",
        "\n",
        "    Returns:\n",
        "        tuple: Input features and corresponding labels\n",
        "    \"\"\"\n",
        "    # Create channel simulator\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "\n",
        "    # Initialize storage\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate information bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Encode polar code\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # Channel simulation\n",
        "        received_signal = channel_simulator.simulate(encoded_signal, snr_db)\n",
        "\n",
        "        # Store features and labels\n",
        "        X.append(received_signal)\n",
        "        y.append(info_bits)  # Use original info_bits as target\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Print dataset information\n",
        "    print(\"\\nðŸ“Š Dataset Preparation Diagnostics:\")\n",
        "    print(f\"Input Tensor Shape (X): {X.shape}\")\n",
        "    print(f\"Label Tensor Shape (y): {y.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Decoder Trainer with enhanced tensor handling\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def _preprocess_tensor(self, X, y):\n",
        "        \"\"\"\n",
        "        Comprehensive tensor preprocessing\n",
        "        \"\"\"\n",
        "        # Ensure X is a tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "\n",
        "        # Ensure y is a tensor\n",
        "        if not isinstance(y, torch.Tensor):\n",
        "            y = torch.FloatTensor(y)\n",
        "\n",
        "        # Flatten multi-dimensional inputs\n",
        "        X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Ensure y is 2D with correct shape\n",
        "        y = y.view(-1, 1).float()\n",
        "\n",
        "        # Print diagnostic information\n",
        "        print(\"\\nðŸ” Tensor Preprocessing Diagnostics:\")\n",
        "        print(f\"Processed X shape: {X.shape}\")\n",
        "        print(f\"Processed y shape: {y.shape}\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, X, y, epochs=50, batch_size=32, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Enhanced training method with robust tensor handling\n",
        "        \"\"\"\n",
        "        # Preprocess tensors\n",
        "        X, y = self._preprocess_tensor(X, y)\n",
        "\n",
        "        # Move to device\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X))\n",
        "        X_val, y_val = X[train_size:], y[train_size:]\n",
        "        X_train, y_train = X[:train_size], y[:train_size]\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = self._train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = self._validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "                  f\"Train Loss: {train_loss:.4f}, \"\n",
        "                  f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Enhanced prediction method with tensor preprocessing\n",
        "        \"\"\"\n",
        "        # Ensure input is a tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "\n",
        "        # Flatten multi-dimensional inputs\n",
        "        X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Move to device\n",
        "        X = X.to(self.device)\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "\n",
        "        return outputs.cpu().numpy().flatten()\n",
        "\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Simplified RNN Decoder with robust input handling\n",
        "        \"\"\"\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with flexible input handling\n",
        "        \"\"\"\n",
        "        # Ensure input is 2D\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        # Flatten if multi-dimensional\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.model(x).squeeze(-1)\n",
        "\n",
        "###################################################\n",
        "\n",
        "\n",
        "   #part 4\n",
        "\n",
        "\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Trainer for Neural Network Decoders\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): Neural network model\n",
        "            learning_rate (float): Learning rate for optimization\n",
        "        \"\"\"\n",
        "        # Device configuration\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Model setup\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "        # Loss and Optimization\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        # Training tracking\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train(self, X, y, epochs=50, batch_size=32, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the decoder model\n",
        "\n",
        "        Args:\n",
        "            X (torch.Tensor): Input features\n",
        "            y (torch.Tensor): Target labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Batch size for training\n",
        "            validation_split (float): Proportion of data for validation\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Ensure tensors are on correct device and have correct shape\n",
        "        X = X.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "\n",
        "        # Reshape if needed\n",
        "        if X.dim() == 3:\n",
        "            X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Ensure y is 2D\n",
        "        y = y.view(-1, 1).float()\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X))\n",
        "        X_val, y_val = X[train_size:], y[train_size:]\n",
        "        X_train, y_train = X[:train_size], y[:train_size]\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            train_loss = self._train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = self._validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Print progress\n",
        "            logging.info(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "                         f\"Train Loss: {train_loss:.4f}, \"\n",
        "                         f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def _train_epoch(self, dataloader):\n",
        "        \"\"\"\n",
        "        Train for one epoch\n",
        "\n",
        "        Args:\n",
        "            dataloader (DataLoader): Training data loader\n",
        "\n",
        "        Returns:\n",
        "            float: Average training loss\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Move to device\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.model(batch_X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        \"\"\"\n",
        "        Validate model performance\n",
        "\n",
        "        Args:\n",
        "            dataloader (DataLoader): Validation data loader\n",
        "\n",
        "        Returns:\n",
        "            float: Validation loss\n",
        "        \"\"\"\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                # Move to device\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Accumulate loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray or torch.Tensor): Input data\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities\n",
        "        \"\"\"\n",
        "        # Ensure input is a tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X).to(self.device)\n",
        "        else:\n",
        "            X = X.to(self.device)\n",
        "\n",
        "        # Reshape if needed\n",
        "        if X.dim() == 3:\n",
        "            X = X.view(X.size(0), -1)\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "\n",
        "        return outputs.cpu().numpy().flatten()\n",
        "\n",
        "#Part 5\n",
        "def performance_comparison(decoder, channel_simulator, polar_code_gen, snr_range, channel_type, list_sizes=[1, 4, 8]):\n",
        "    \"\"\"\n",
        "    Comprehensive performance comparison across different list sizes\n",
        "\n",
        "    Args:\n",
        "        decoder (DecoderTrainer): Trained decoder\n",
        "        channel_simulator (EnhancedChannelSimulator): Channel simulator\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "        snr_range (np.ndarray): SNR range for evaluation\n",
        "        channel_type (str): Channel type\n",
        "        list_sizes (list): List sizes for decoding\n",
        "\n",
        "    Returns:\n",
        "        dict: Comprehensive performance results\n",
        "    \"\"\"\n",
        "    # Convert SNR to linear scale\n",
        "    snr_linear = 10 ** (snr_range / 10)\n",
        "\n",
        "    # Theoretical Performance\n",
        "    block_length = polar_code_gen.N\n",
        "    theoretical_bep, theoretical_bler = channel_simulator.compute_theoretical_performance(\n",
        "        block_length, snr_linear\n",
        "    )\n",
        "\n",
        "    # Results storage\n",
        "    results = {\n",
        "        'theoretical': {\n",
        "            'bep': theoretical_bep,\n",
        "            'bler': theoretical_bler\n",
        "        },\n",
        "        'simulated': {}\n",
        "    }\n",
        "\n",
        "    # Performance computation for each list size\n",
        "    for list_size in list_sizes:\n",
        "        # Initialize storage for this list size\n",
        "        results['simulated'][list_size] = {\n",
        "            'ber': [],\n",
        "            'bler': []\n",
        "        }\n",
        "\n",
        "        # Compute performance for each SNR point\n",
        "        for snr in snr_range:\n",
        "            total_bit_errors = 0\n",
        "            total_block_errors = 0\n",
        "            total_trials = 1000\n",
        "\n",
        "            for _ in range(total_trials):\n",
        "                # Generate information bits\n",
        "                info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "                # Encode polar code\n",
        "                encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "                # Channel simulation\n",
        "                received_signal = channel_simulator.simulate(encoded_signal, snr)\n",
        "\n",
        "                # Convert to tensor for decoding\n",
        "                received_tensor = torch.FloatTensor(received_signal).reshape(1, -1)\n",
        "\n",
        "                # Decode with list size simulation\n",
        "                decoded_output = decoder.predict(received_tensor)\n",
        "\n",
        "                # Adjust decoding based on list size\n",
        "                list_threshold = 1 - (list_size / 10)\n",
        "                decoded_bits = (decoded_output > list_threshold).flatten()\n",
        "\n",
        "                # Compute errors\n",
        "                bit_errors = np.sum(np.abs(info_bits - decoded_bits[:len(info_bits)]))\n",
        "                block_error = bit_errors > 0\n",
        "\n",
        "                total_bit_errors += bit_errors\n",
        "                total_block_errors += block_error\n",
        "\n",
        "            # Compute average errors\n",
        "            avg_ber = total_bit_errors / (total_trials * len(info_bits))\n",
        "            avg_bler = total_block_errors / total_trials\n",
        "\n",
        "            # Store results\n",
        "            results['simulated'][list_size]['ber'].append(avg_ber)\n",
        "            results['simulated'][list_size]['bler'].append(avg_bler)\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_comprehensive_analysis(rnn_trainer, X_test, y_test, channel_name, train_losses, val_losses, performance_results, snr_range, list_sizes):\n",
        "    \"\"\"\n",
        "    Create comprehensive performance visualization\n",
        "\n",
        "    Args:\n",
        "        rnn_trainer (DecoderTrainer): Trained decoder\n",
        "        X_test (torch.Tensor): Test input data\n",
        "        y_test (torch.Tensor): Test labels\n",
        "        channel_name (str): Channel type\n",
        "        train_losses (list): Training losses\n",
        "        val_losses (list): Validation losses\n",
        "        performance_results (dict): Performance computation results\n",
        "        snr_range (np.ndarray): SNR range\n",
        "        list_sizes (list): List sizes\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    plt.suptitle(f'Comprehensive Analysis - {channel_name} Channel', fontsize=16)\n",
        "\n",
        "    # 1. Training and Validation Losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "    plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # 2. Confusion Matrix\n",
        "    plt.subplot(2, 2, 2)\n",
        "    predictions = rnn_trainer.predict(X_test)\n",
        "    pred_classes = (predictions > 0.5).astype(int).flatten()\n",
        "    true_classes = y_test.numpy().flatten()\n",
        "\n",
        "    cm = confusion_matrix(true_classes, pred_classes)\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar(im)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "\n",
        "    # 3. BER Performance\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.title(f'Bit Error Rate (BER) - {channel_name} Channel')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER (Log Scale)')\n",
        "\n",
        "    # Theoretical BER\n",
        "    plt.semilogy(snr_range, performance_results['theoretical']['bep'],\n",
        "                 label='Theoretical BER', color='black', linestyle='--')\n",
        "\n",
        "    # Simulated BER for different list sizes\n",
        "    colors = ['red', 'green', 'blue']\n",
        "    for idx, list_size in enumerate(list_sizes):\n",
        "        plt.semilogy(snr_range, performance_results['simulated'][list_size]['ber'],\n",
        "                     label=f'List Size {list_size}', color=colors[idx], marker='o')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True, which='both', ls=':', alpha=0.7)\n",
        "\n",
        "    # 4. BLER Performance\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.title(f'Block Error Rate (BLER) - {channel_name} Channel')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER (Log Scale)')\n",
        "\n",
        "    # Theoretical BLER\n",
        "    plt.semilogy(snr_range, performance_results['theoretical']['bler'],\n",
        "                 label='Theoretical BLER', color='black', linestyle='--')\n",
        "\n",
        "    # Simulated BLER for different list sizes\n",
        "    for idx, list_size in enumerate(list_sizes):\n",
        "        plt.semilogy(snr_range, performance_results['simulated'][list_size]['bler'],\n",
        "                     label=f'List Size {list_size}', color=colors[idx], marker='o')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(True, which='both', ls=':', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Part 6: Main Simulation Framework\n",
        "########################\n",
        "#Latest main()\n",
        "\n",
        "\n",
        "##########################\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main simulation framework for Polar Code performance analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Simulation Configuration\n",
        "        BLOCK_LENGTH = 32\n",
        "        INFO_BITS = 16\n",
        "        LEARNING_RATE = 1e-3\n",
        "        EPOCHS = 50\n",
        "        BATCH_SIZE = 32\n",
        "        NUM_SAMPLES = 5000\n",
        "\n",
        "        # SNR Ranges\n",
        "        SNR_RANGE_AWGN = np.linspace(0, 5, 10)\n",
        "        SNR_RANGE_RAYLEIGH = np.linspace(0, 10, 10)\n",
        "\n",
        "        # List Sizes\n",
        "        LIST_SIZES = [1, 4, 8]\n",
        "\n",
        "        # Polar Code Generator\n",
        "        polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "\n",
        "        # Results storage\n",
        "        results = {}\n",
        "\n",
        "        # Channel Types\n",
        "        channels = {\n",
        "            'AWGN': EnhancedChannelSimulator(channel_type='AWGN'),\n",
        "            'Rayleigh': EnhancedChannelSimulator(channel_type='Rayleigh')\n",
        "        }\n",
        "\n",
        "        # Iterate through channel types\n",
        "        for channel_name, channel in channels.items():\n",
        "            logging.info(f\"Analyzing {channel_name} Channel\")\n",
        "\n",
        "            # Prepare Dataset\n",
        "            X, y = prepare_polar_dataset(\n",
        "                polar_code_gen,\n",
        "                num_samples=NUM_SAMPLES,\n",
        "                channel_type=channel_name\n",
        "            )\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "            # Convert to PyTorch tensors\n",
        "            X_train = torch.FloatTensor(X_train)\n",
        "            X_test = torch.FloatTensor(X_test)\n",
        "            y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
        "            y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
        "\n",
        "            # Enhanced RNN Decoder\n",
        "            rnn_model = EnhancedRNNDecoder(input_size=X_train.size(1))\n",
        "            rnn_trainer = DecoderTrainer(rnn_model)\n",
        "\n",
        "            # Train Decoder\n",
        "            train_losses, val_losses = rnn_trainer.train(\n",
        "                X_train,\n",
        "                y_train,\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            # Determine SNR range\n",
        "            snr_range = SNR_RANGE_AWGN if channel_name == 'AWGN' else SNR_RANGE_RAYLEIGH\n",
        "\n",
        "            # Performance Comparison\n",
        "            performance_results = performance_comparison(\n",
        "                rnn_trainer,\n",
        "                channel,\n",
        "                polar_code_gen,\n",
        "                snr_range,\n",
        "                channel_name,\n",
        "                LIST_SIZES\n",
        "            )\n",
        "\n",
        "            # Comprehensive Analysis Plot\n",
        "            plot_comprehensive_analysis(\n",
        "                rnn_trainer,\n",
        "                X_test,\n",
        "                y_test,\n",
        "                channel_name,\n",
        "                train_losses,\n",
        "                val_losses,\n",
        "                performance_results,\n",
        "                snr_range,\n",
        "                LIST_SIZES\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            results[channel_name] = {\n",
        "                'decoder': rnn_trainer,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'performance': performance_results\n",
        "            }\n",
        "\n",
        "        logging.info(\"ðŸŽ‰ Simulation Complete!\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"ðŸ†˜ Comprehensive Simulation Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Execution\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "############################################\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZysFMv5r4en7",
        "outputId": "fd4977e9-e2eb-4949-907a-1fda04dfb5ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Using Device: cpu\n",
            "\n",
            "ðŸ“Š Dataset Preparation Diagnostics:\n",
            "Input Tensor Shape (X): (5000, 32)\n",
            "Label Tensor Shape (y): (5000, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "ERROR:root:ðŸ†˜ Comprehensive Simulation Error: Size mismatch between tensors\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-19-137dff128298>\", line 902, in main\n",
            "    train_losses, val_losses = rnn_trainer.train(\n",
            "                               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-19-137dff128298>\", line 544, in train\n",
            "    val_dataset = TensorDataset(X_val, y_val)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\", line 205, in __init__\n",
            "    assert all(\n",
            "           ^^^^\n",
            "AssertionError: Size mismatch between tensors\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --- Configuration ---\n",
        "N = 128             # Block length\n",
        "K = 64              # Number of information bits\n",
        "CRC_LENGTH = 8      # CRC bits length\n",
        "CRC_POLY = 0x07     # CRC-8 polynomial (example)\n",
        "\n",
        "LIST_SIZES = [1, 4, 8, 16]    # List sizes for SCL decoder\n",
        "\n",
        "NUM_FRAMES = 50000   # Frames per SNR point for evaluation\n",
        "EPOCHS = 40         # Training epochs for RNN\n",
        "BATCH_SIZE = 64     # Batch size for RNN training\n",
        "\n",
        "snr_range = np.arange(0, 5.5, 0.5)  # SNR range from 0 to 5 dB in steps of 0.5 dB\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- CRC functions ---\n",
        "def crc_encode(bits, poly=CRC_POLY, crc_len=CRC_LENGTH):\n",
        "    g = poly\n",
        "    reg = 0\n",
        "    bits_out = list(bits)\n",
        "    for b in bits:\n",
        "        reg = ((reg << 1) | b) & ((1 << (crc_len + 1)) - 1)\n",
        "        if reg & (1 << crc_len):\n",
        "            reg ^= g\n",
        "    crc = [(reg >> i) & 1 for i in reversed(range(crc_len))]\n",
        "    return np.concatenate((bits_out, crc))\n",
        "\n",
        "def crc_check(bits, poly=CRC_POLY, crc_len=CRC_LENGTH):\n",
        "    reg = 0\n",
        "    for b in bits:\n",
        "        reg = ((reg << 1) | b) & ((1 << (crc_len + 1)) - 1)\n",
        "        if reg & (1 << crc_len):\n",
        "            reg ^= poly\n",
        "    return reg == 0\n",
        "\n",
        "# --- Polar code helpers ---\n",
        "def get_frozen_bits(N, K):\n",
        "    frozen = np.zeros(N, dtype=bool)\n",
        "    frozen[:N-K] = True\n",
        "    return frozen\n",
        "\n",
        "def polar_transform(u):\n",
        "    N = len(u)\n",
        "    if N == 1:\n",
        "        return u\n",
        "    else:\n",
        "        u_even = polar_transform(u[0::2])\n",
        "        u_odd = polar_transform(u[1::2])\n",
        "        return np.concatenate([u_even ^ u_odd, u_odd])\n",
        "\n",
        "def polar_encode(info_bits, frozen_bits_mask):\n",
        "    u = np.zeros(len(frozen_bits_mask), dtype=int)\n",
        "    info_idx = 0\n",
        "    for i in range(len(frozen_bits_mask)):\n",
        "        if not frozen_bits_mask[i]:\n",
        "            u[i] = info_bits[info_idx]\n",
        "            info_idx += 1\n",
        "    x = polar_transform(u)\n",
        "    return x\n",
        "\n",
        "# --- Channel and modulation ---\n",
        "def awgn_noise(sigma, size):\n",
        "    return np.random.normal(0, sigma, size)\n",
        "\n",
        "def bpsk_mod(bits):\n",
        "    return 1 - 2*bits\n",
        "\n",
        "def add_awgn_noise(signal, snr_db):\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1/(2*snr_linear))\n",
        "    noise = awgn_noise(sigma, len(signal))\n",
        "    return signal + noise, sigma\n",
        "\n",
        "def llr_from_awgn(y, sigma):\n",
        "    return 2 * y / (sigma**2)\n",
        "\n",
        "# --- SC Decoder ---\n",
        "def sc_decode(llr, frozen_bits_mask):\n",
        "    N = len(llr)\n",
        "    if N == 1:\n",
        "        return np.array([0]) if frozen_bits_mask[0] else np.array([int(llr[0] < 0)])\n",
        "    else:\n",
        "        half = N // 2\n",
        "        llr_left = f_func(llr[:half], llr[half:])\n",
        "        u_left = sc_decode(llr_left, frozen_bits_mask[:half])\n",
        "        llr_right = g_func(llr[:half], llr[half:], u_left)\n",
        "        u_right = sc_decode(llr_right, frozen_bits_mask[half:])\n",
        "        return np.concatenate([u_left ^ u_right, u_right])\n",
        "\n",
        "def f_func(a, b):\n",
        "    s = np.sign(a)*np.sign(b)\n",
        "    return s * np.minimum(np.abs(a), np.abs(b))\n",
        "\n",
        "def g_func(a, b, c):\n",
        "    return b + (1 - 2*c)*a\n",
        "\n",
        "# --- SCL Decoder ---\n",
        "\n",
        "########################################################\n",
        "\n",
        "#latest SCL decoder\n",
        "class SCLDecoder:\n",
        "    def __init__(self, N, K, frozen_bits_mask, crc_poly=None, list_size=8):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.frozen_bits_mask = frozen_bits_mask  # boolean mask of length N (True for frozen bits)\n",
        "        self.list_size = list_size\n",
        "        self.crc_poly = crc_poly  # e.g. [1,0,0,1,1] for CRC-4 (optional)\n",
        "\n",
        "        self.n = int(np.log2(N))\n",
        "        assert 2**self.n == N, \"N must be a power of 2\"\n",
        "\n",
        "    def _f(self, a, b):\n",
        "        \"\"\"Polar f function (min-sum approx)\"\"\"\n",
        "        return np.sign(a)*np.sign(b)*np.minimum(np.abs(a), np.abs(b))\n",
        "\n",
        "    def _g(self, a, b, c):\n",
        "        \"\"\"Polar g function\"\"\"\n",
        "        return b + (1 - 2*c)*a\n",
        "\n",
        "    def _crc_check(self, bits):\n",
        "        \"\"\"Check CRC if crc_poly provided, else always True\"\"\"\n",
        "        if self.crc_poly is None:\n",
        "            return True\n",
        "        # Simple CRC check: bits is full codeword including CRC bits at end\n",
        "        poly = np.array(self.crc_poly)\n",
        "        code = np.array(bits)\n",
        "        # polynomial division in GF(2)\n",
        "        for i in range(len(code) - len(poly) + 1):\n",
        "            if code[i] == 1:\n",
        "                code[i:i+len(poly)] ^= poly\n",
        "        return not np.any(code[-(len(poly)-1):])  # all zeros in remainder?\n",
        "\n",
        "    def decode(self, llr):\n",
        "        \"\"\"\n",
        "        SCL decoding on input LLR vector of length N.\n",
        "        Returns best estimated info bits of length K.\n",
        "        \"\"\"\n",
        "        N, n = self.N, self.n\n",
        "        L = self.list_size\n",
        "\n",
        "        # Initialize path metrics, paths, partial sums, etc.\n",
        "        PM = np.zeros(L)  # path metrics\n",
        "        paths = np.zeros((L, N), dtype=int)  # decoded bits per path\n",
        "        llr_paths = np.tile(llr, (L, 1))  # LLRs per path\n",
        "\n",
        "        # For each bit index i in [0, N)\n",
        "        for i in range(N):\n",
        "            # Determine if current bit is frozen or info\n",
        "            frozen = self.frozen_bits_mask[i]\n",
        "\n",
        "            # For all current paths: compute LLR for bit i from partial decoding info\n",
        "            # Here we simplify by treating llr_paths as current LLRs.\n",
        "            # (Full recursive LLR update is complex, omitted for brevity)\n",
        "\n",
        "            # For frozen bit, bit value is zero; update path metrics accordingly\n",
        "            if frozen:\n",
        "                # Update PM for each path assuming bit=0\n",
        "                pm_update = np.log1p(np.exp(-llr_paths[:, i]))  # penalty for bit=0\n",
        "                PM += pm_update\n",
        "                paths[:, i] = 0\n",
        "            else:\n",
        "                # For info bit, split paths with bit=0 and bit=1\n",
        "                pm0 = PM + np.log1p(np.exp(-llr_paths[:, i]))\n",
        "                pm1 = PM + np.log1p(np.exp(llr_paths[:, i]))\n",
        "\n",
        "                # Create candidate paths doubling the number\n",
        "                cand_PM = np.concatenate([pm0, pm1])\n",
        "                cand_paths = np.vstack([np.hstack([paths[j, :i], 0, paths[j, i+1:]]) for j in range(L)] +\n",
        "                                       [np.hstack([paths[j, :i], 1, paths[j, i+1:]]) for j in range(L)])\n",
        "\n",
        "                # Select best L paths\n",
        "                idx = np.argsort(cand_PM)[:L]\n",
        "                PM = cand_PM[idx]\n",
        "                paths = cand_paths[idx]\n",
        "\n",
        "        # At the end, select best path that passes CRC\n",
        "        for i in np.argsort(PM):\n",
        "            candidate_bits = paths[i]\n",
        "            info_bits = candidate_bits[~self.frozen_bits_mask]\n",
        "            if self._crc_check(candidate_bits):\n",
        "                return info_bits[:self.K]  # return info bits excluding CRC bits\n",
        "\n",
        "        # If none pass CRC, return best PM path anyway\n",
        "        best_bits = paths[np.argmin(PM)]\n",
        "        return best_bits[~self.frozen_bits_mask][:self.K]\n",
        "##########################################################\n",
        "#latest RNN decoder\n",
        "\n",
        "# ----- RNN Decoder Definition -----\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size=128, hidden_size=64, num_layers=1):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, 1, input_size)\n",
        "        out, _ = self.rnn(x)\n",
        "        logits = self.fc(out[:, -1, :])  # (batch_size, input_size)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs\n",
        "\n",
        "# ----- Data Generation Function -----\n",
        "def generate_training_data(N, K, frozen_bits_mask, num_samples=10000, snr_db=2.0):\n",
        "    info_bits = np.random.randint(0, 2, size=(num_samples, K))\n",
        "    codewords = np.zeros((num_samples, N))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        codeword = np.zeros(N, dtype=int)\n",
        "        codeword[~frozen_bits_mask] = info_bits[i]\n",
        "        # Polar transform (basic version)\n",
        "        for stage in range(int(np.log2(N))):\n",
        "            step = 2 ** (stage + 1)\n",
        "            for j in range(0, N, step):\n",
        "                for k in range(step // 2):\n",
        "                    u1 = codeword[j + k]\n",
        "                    u2 = codeword[j + k + step // 2]\n",
        "                    codeword[j + k] = u1 ^ u2\n",
        "        codewords[i] = codeword\n",
        "\n",
        "    # BPSK modulation\n",
        "    x = 1 - 2 * codewords\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1 / (2 * snr))\n",
        "    noise = sigma * np.random.randn(*x.shape)\n",
        "    y = x + noise\n",
        "    llr = 2 * y / (sigma ** 2)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X = torch.tensor(llr, dtype=torch.float32).reshape(num_samples, 1, N)  # (B, 1, 128)\n",
        "    Y = torch.tensor(info_bits, dtype=torch.float32)\n",
        "    return X, Y\n",
        "\n",
        "# ----- Training Function -----\n",
        "def train_rnn_decoder(model, train_loader, val_loader, num_epochs=10, lr=0.001):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, Y_batch in train_loader:\n",
        "            X_batch, Y_batch = X_batch.cuda(), Y_batch.cuda()\n",
        "            output = model(X_batch)\n",
        "            loss = criterion(output, Y_batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X_val, Y_val in val_loader:\n",
        "                X_val, Y_val = X_val.cuda(), Y_val.cuda()\n",
        "                output = model(X_val)\n",
        "                val_loss = criterion(output, Y_val)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "###########################################################\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        " # BPSK modulation\n",
        "    x = 1 - 2 * codewords\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1 / (2 * snr))\n",
        "    noise = sigma * np.random.randn(*x.shape)\n",
        "    y = x + noise\n",
        "    llr = 2 * y / (sigma ** 2)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X = torch.tensor(llr, dtype=torch.float32).reshape(num_samples, 1, N)  # (B, 1, 128)\n",
        "    Y = torch.tensor(info_bits, dtype=torch.float32)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "\n",
        "############################################################\n",
        "\n",
        "\n",
        "\n",
        "# --- Evaluation ---\n",
        "def evaluate_decoder(decoder_func, frozen_bits_mask, snr_db, num_frames):\n",
        "    total_bits = 0\n",
        "    total_bit_errors = 0\n",
        "    total_blocks = 0\n",
        "    total_block_errors = 0\n",
        "    for _ in range(num_frames):\n",
        "        info_bits = np.random.randint(0, 2, K)\n",
        "        tx_bits = crc_encode(info_bits)\n",
        "        tx_bits = tx_bits[:K+CRC_LENGTH]\n",
        "        coded_bits = polar_encode(tx_bits, frozen_bits_mask)\n",
        "        modulated = bpsk_mod(coded_bits)\n",
        "        noisy, sigma = add_awgn_noise(modulated, snr_db)\n",
        "        llr = llr_from_awgn(noisy, sigma)\n",
        "        decoded = decoder_func(llr)\n",
        "        decoded_info = decoded[~frozen_bits_mask]\n",
        "        total_bits += K\n",
        "        total_bit_errors += np.sum(decoded_info != tx_bits)\n",
        "        total_blocks += 1\n",
        "        total_block_errors += int(np.any(decoded_info != tx_bits))\n",
        "    ber = total_bit_errors / total_bits\n",
        "    bler = total_block_errors / total_blocks\n",
        "    return ber, bler\n",
        "\n",
        "# --- Main ---\n",
        "def main():\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    frozen_bits_mask = get_frozen_bits(N, K)\n",
        "\n",
        "    scl_decoders = {L: SCLDecoder(N, K, frozen_bits_mask, L) for L in LIST_SIZES}\n",
        "\n",
        "    rnn = RNNDecoder().to(device)\n",
        "    optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    print(\"Generating training data...\")\n",
        "  #  X_train, Y_train = generate_training_data(50000, frozen_bits_mask)\n",
        "    X_train, Y_train = generate_training_data(N, K, frozen_bits_mask, num_samples=50000, snr_db=2.0)\n",
        "\n",
        "    print(\"Training RNN...\")\n",
        "    train_losses, val_losses = train_rnn(rnn, optimizer, criterion, X_train, Y_train, EPOCHS, BATCH_SIZE)\n",
        "\n",
        "    ber_sc_list = []\n",
        "    bler_sc_list = []\n",
        "    ber_scl_lists = {L: [] for L in LIST_SIZES}\n",
        "    bler_scl_lists = {L: [] for L in LIST_SIZES}\n",
        "    ber_rnn_list = []\n",
        "    bler_rnn_list = []\n",
        "\n",
        "    rnn.eval()\n",
        "    with torch.no_grad():\n",
        "        for snr_db in snr_range:\n",
        "            def sc_dec(llr):\n",
        "                return sc_decode(llr, frozen_bits_mask)\n",
        "\n",
        "            def rnn_dec(llr):\n",
        "                inp = torch.tensor(llr, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "                out = rnn(inp)\n",
        "                bits = (out.cpu().numpy() > 0.5).astype(int).flatten()\n",
        "                return bits\n",
        "\n",
        "            ber_sc, bler_sc = evaluate_decoder(sc_dec, frozen_bits_mask, snr_db, NUM_FRAMES)\n",
        "            ber_sc_list.append(ber_sc)\n",
        "            bler_sc_list.append(bler_sc)\n",
        "\n",
        "            ber_rnn, bler_rnn = evaluate_decoder(rnn_dec, frozen_bits_mask, snr_db, NUM_FRAMES)\n",
        "            ber_rnn_list.append(ber_rnn)\n",
        "            bler_rnn_list.append(bler_rnn)\n",
        "\n",
        "            for L in LIST_SIZES:\n",
        "                def scl_dec(llr):\n",
        "                    return scl_decoders[L].decode(llr)\n",
        "                ber_scl, bler_scl = evaluate_decoder(scl_dec, frozen_bits_mask, snr_db, NUM_FRAMES)\n",
        "                ber_scl_lists[L].append(ber_scl)\n",
        "                bler_scl_lists[L].append(bler_scl)\n",
        "\n",
        "            print(f\"SNR={snr_db:.1f} dB | BER SC={ber_sc:.4e}, BER RNN={ber_rnn:.4e}\")\n",
        "            for L in LIST_SIZES:\n",
        "                print(f\"          | BER SCL(L={L})={ber_scl_lists[L][-1]:.4e}\")\n",
        "            print(f\"          | BLER SC={bler_sc:.4e}, BLER RNN={bler_rnn:.4e}\")\n",
        "            for L in LIST_SIZES:\n",
        "                print(f\"          | BLER SCL(L={L})={bler_scl_lists[L][-1]:.4e}\")\n",
        "\n",
        "    # Plot Training Loss\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    if val_losses:\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('RNN Training Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BER\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-6, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate (BER)')\n",
        "    plt.title('BER vs SNR')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.plot(snr_range, ber_sc_list, 'o-', label='SC')\n",
        "    for L in LIST_SIZES:\n",
        "        plt.plot(snr_range, ber_scl_lists[L], label=f'SCL L={L}')\n",
        "    plt.plot(snr_range, ber_rnn_list, 's--', label='RNN')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BLER\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-6, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate (BLER)')\n",
        "    plt.title('BLER vs SNR')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.plot(snr_range, bler_sc_list, 'o-', label='SC')\n",
        "    for L in LIST_SIZES:\n",
        "        plt.plot(snr_range, bler_scl_lists[L], label=f'SCL L={L}')\n",
        "    plt.plot(snr_range, bler_rnn_list, 's--', label='RNN')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KxXELOYM8etv",
        "outputId": "4b652096-9758-473e-c6fd-2db3cf35c714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Generating training data...\n",
            "Training RNN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-3636049996.py:231: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1),\n",
            "/tmp/ipython-input-1-3636049996.py:232: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(Y_train, dtype=torch.float32))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "GRU: Expected input to be 2D or 3D, got 4D instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3934786501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4-3934786501.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training RNN...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mber_sc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1-3636049996.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, optimizer, criterion, X_train, Y_train, epochs, batch_size)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4-3934786501.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# x: (batch_size, 1, input_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, input_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1357\u001b[0m                     \u001b[0;34mf\"GRU: Expected input to be 2D or 3D, got {input.dim()}D instead\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: GRU: Expected input to be 2D or 3D, got 4D instead"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
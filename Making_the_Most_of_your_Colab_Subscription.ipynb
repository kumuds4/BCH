{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#latest plot functions\n",
        "# Updates code script\n",
        "# Comprehensive Polar Code Simulation Framework\n",
        "# Clean up and install necessary libraries in one go\n",
        "# This helps avoid potential conflicts from multiple installations\n",
        "# Explicitly uninstall pandas and install a compatible version first\n",
        "# Google Colab requires pandas==2.2.2 as of the last error message.\n",
        "# Comprehensive Uninstall\n",
        "!pip uninstall -y torch torchvision torchaudio fastai numba tensorflow tensorflow-decision-forests tensorflow-text tf-keras numpy pandas scikit-learn scipy cuml-cu12 cudf-cu12 dask-cuda distributed-ucxx-cu12 ml-dtypes tensorboard dask-cudf-cu12 raft-dask-cu12 sklearn-pandas\n",
        "\n",
        "# Install core libraries with --upgrade to force versions\n",
        "!pip install numpy==1.26.4 --upgrade --no-cache-dir\n",
        "#!pip install scipy==1.14.0 --upgrade --no-cache-dir\n",
        "!pip install pandas==2.2.2 --upgrade --no-cache-dir\n",
        "#!pip install scikit-learn==1.4.1.post1 --upgrade --no-cache-dir\n",
        "#!pip install sklearn-pandas==2.2.8 --upgrade --no-cache-dir\n",
        "# Rest of your installations (commented out for this test)\n",
        "# !pip install scipy==1.14.0 --no-cache-dir\n",
        "# !pip install scikit-learn==1.4.1.post1 --no-cache-dir\n",
        "# !pip install sklearn-pandas==2.2.8 --no-cache-dir # Explicitly install sklearn-pandas after pandas\n",
        "# !pip install torch==2.6.0 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --extra-index-url https://download.pytorch.org/whl/cu124 --no-cache-dir\n",
        "# !pip install fastai==2.7.19 --no-cache-dir\n",
        "# !pip install matplotlib --no-cache-dir\n",
        "# !pip install tensorflow==2.18.0 tensorflow-decision-forests tensorflow-text tf-keras --no-cache-dir\n",
        "# !pip install numba==0.60.0 --no-cache-dir\n",
        "# Install other libraries with --upgrade\n",
        "#!pip install torch==2.6.0 torchvision==0.21.0+cu124 torchaudio==2.6.0+cu124 --extra-index-url https://download.pytorch.org/whl/cu124 --upgrade --no-cache-dir\n",
        "#!pip install fastai==2.7.19 --upgrade --no-cache-dir\n",
        "#!pip install matplotlib --upgrade --no-cache-dir\n",
        "#!pip install tensorflow==2.18.0 tensorflow-decision-forests tensorflow-text tf-keras --upgrade --no-cache-dir\n",
        "#!pip install numba==0.60.0 --upgrade --no-cache-dir\n",
        "\n",
        "# Install RAPIDS libraries if you need them, making sure to match versions and Numba/CUDA compatibility\n",
        "# !pip install cuml-cu12==25.2.1 cudf-cu12==25.2.1 dask-cuda==25.2.0 distributed-ucxx-cu12==0.42.0 dask-cudf-cu12==25.2.2 raft-dask-cu12==25.2.0 --extra-index-url=https://pypi.nvidia.com --upgrade --no-cache-dir\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.special as sps # Added for theoretical performance\n",
        "import traceback # Added for error tracing\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "# Install RAPIDS libraries if you need them, making sure to match versions and Numba/CUDA compatibility\n",
        "# !pip install cuml-cu12==25.2.1 cudf-cu12==25.2.1 dask-cuda==25.2.0 distributed-ucxx-cu12==0.42.0 dask-cudf-cu12==25.2.2 raft-dask-cu12==25.2.0 --extra-index-url=https://pypi.nvidia.com --no-cache-dir\n",
        "# Install RAPIDS libraries if you need them, making sure to match versions and Numba/CUDA compatibility\n",
        "# !pip install cuml-cu12==25.2.1 cudf-cu12==25.2.1 dask-cuda==25.2.0 distributed-ucxx-cu12==0.42.0 dask-cudf-cu12==25.2.2 raft-dask-cu12==25.2.0 --extra-index-url=https://pypi.nvidia.com --no-cache-dir\n",
        "# You may need to install RAPIDS libraries separately if you are using them:\n",
        "# !pip install cuml-cu12==25.2.1 cudf-cu12==25.2.1 dask-cuda==25.2.0 distributed-ucxx-cu12==0.42.0\n",
        "# Now install/upgrade other essential libraries to potentially compatible versions\n",
        "#!pip install -q --upgrade numpy matplotlib scikit-learn torch\n",
        "#!pip install -q --upgrade pandas numpy matplotlib scikit-learn torch\n",
        "\n",
        "\n",
        "# Fix: Corrected format string for datefmt\n",
        "# Fix: Corrected format string for datefmt\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "\n",
        "# Device Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"🚀 Using Device: {DEVICE}\")\n",
        "\n",
        "#part two\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N, K, crc_type='CRC-7'):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc_type = crc_type\n",
        "        self.crc_polynomials = {\n",
        "            'CRC-7': {'polynomial': [1, 1, 1, 0, 0, 1, 1], 'length': 7}\n",
        "        }\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def compute_crc(self, bits):\n",
        "        poly_info = self.crc_polynomials.get(self.crc_type)\n",
        "        if not poly_info:\n",
        "            raise ValueError(f\"Unsupported CRC type: {self.crc_type}\")\n",
        "\n",
        "        polynomial = poly_info['polynomial']\n",
        "        crc_length = poly_info['length']\n",
        "        message = bits.tolist() + [0] * crc_length\n",
        "        for i in range(len(message) - crc_length):\n",
        "            if message[i] == 1:\n",
        "                for j in range(crc_length + 1):\n",
        "                    message[i + j] ^= polynomial[j] if j < len(polynomial) else 0\n",
        "\n",
        "        return np.array(message[-crc_length:], dtype=int)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        crc_bits = self.compute_crc(info_bits)\n",
        "        extended_info_bits = np.concatenate([info_bits, crc_bits])\n",
        "        codeword = np.zeros(self.N, dtype=int)\n",
        "        codeword[:len(extended_info_bits)] = extended_info_bits\n",
        "        return codeword\n",
        "\n",
        "    def verify_codeword(self, codeword):\n",
        "        poly_info = self.crc_polynomials[self.crc_type]\n",
        "        crc_length = poly_info['length']\n",
        "        info_bits = codeword[:-crc_length]\n",
        "        received_crc = codeword[-crc_length:]\n",
        "        computed_crc = self.compute_crc(info_bits)\n",
        "        return np.array_equal(received_crc, computed_crc)\n",
        "\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        self.channel_type = channel_type\n",
        "        logging.info(f\"Initializing {channel_type} Channel Simulator\")\n",
        "\n",
        "    def simulate(self, encoded_signal, snr_db):\n",
        "        try:\n",
        "            encoded_signal = np.array(encoded_signal, dtype=float)\n",
        "            bpsk_signal = 1 - 2 * encoded_signal\n",
        "            snr_linear = 10 ** (snr_db / 10)\n",
        "            signal_power = np.mean(bpsk_signal**2)\n",
        "            noise_power = signal_power / snr_linear\n",
        "            noise_std = np.sqrt(noise_power / 2.0)\n",
        "\n",
        "            if self.channel_type == 'AWGN':\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = bpsk_signal + noise\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                fading = np.random.rayleigh(scale=1.0, size=bpsk_signal.shape)\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = fading * bpsk_signal + noise\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Return the raw received signal instead of hard decisions for RNN input\n",
        "            return received_signal\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Channel simulation error: {e}\")\n",
        "            # Return the original signal if simulation fails\n",
        "            return bpsk_signal\n",
        "\n",
        "    def compute_theoretical_performance(self, block_length, snr_linear):\n",
        "        try:\n",
        "            if self.channel_type == 'AWGN':\n",
        "                # Theoretical BER for BPSK in AWGN\n",
        "                bep = 0.5 * sps.erfc(np.sqrt(snr_linear))\n",
        "            elif self.channel_type == 'Rayleigh':\n",
        "                 # Theoretical BER for BPSK in Rayleigh (assuming ideal channel estimation)\n",
        "                 bep = 0.5 * (1 - np.sqrt(snr_linear / (1 + snr_linear)))\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            # Theoretical BLER is complex for Polar codes; using a simple bound might be misleading.\n",
        "            # Using a very loose upper bound (Union Bound)\n",
        "            bler = 1 - (1 - bep) ** block_length\n",
        "            return bep, bler\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Theoretical performance computation error: {e}\")\n",
        "            return np.zeros_like(snr_linear), np.ones_like(snr_linear)\n",
        "\n",
        "\n",
        "#part three\n",
        "\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type='AWGN'):\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "    X, y = [], []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "        # Simulate the channel and get the received signal (soft values)\n",
        "        received_signal = channel_simulator.simulate(encoded_signal, snr_db)\n",
        "        X.append(received_signal)\n",
        "        y.append(info_bits) # Keep the original info bits as labels\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "#part four\n",
        "\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, output_size), # Output size is number of info bits (K)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1) # Flatten if input is not already 2D\n",
        "        return self.model(x)\n",
        "###########################################################\n",
        "#add latest version for DecodertRainer\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "            # Remove 'verbose'\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        " # Add other methods as defined in your code\n",
        "\n",
        "\n",
        "    def train(self, X, y, epochs=100, batch_size=32, validation_split=0.2):\n",
        "        X_tensor = X.to(self.device)\n",
        "        y_tensor = y.to(self.device)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        train_size = int((1 - validation_split) * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            train_loss = self._train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = self._validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def _train_epoch(self, dataloader):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(batch_X)\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "                total_loss += loss.item()\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "        if X.dim() > 2:\n",
        "            X = X.view(X.size(0), -1)\n",
        "        X = X.to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "        return (outputs > 0.5).cpu().numpy().astype(int)\n",
        "\n",
        "# Add traditional decoder\n",
        "class SCLDecoder:\n",
        "    def __init__(self, N, K, list_size, crc_type='CRC-7'):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.list_size = list_size\n",
        "        self.crc_type = crc_type\n",
        "\n",
        "    def decode(self, received_signal):\n",
        "        # Simplified pseudo-code for SCL decoding\n",
        "        # Initialize list paths and metrics\n",
        "        paths = [np.zeros(self.N, dtype=int)] * self.list_size\n",
        "        path_metrics = [0] * self.list_size\n",
        "\n",
        "        # Successive cancellation with list keeping\n",
        "        for i in range(self.N):\n",
        "            # Update each path with new bit decision\n",
        "            for l in range(self.list_size):\n",
        "                # Example decision logic (to be replaced with actual SCL logic)\n",
        "                paths[l][i] = self.make_decision(received_signal[i], path_metrics[l])\n",
        "\n",
        "                # Update path metric\n",
        "                path_metrics[l] += self.calculate_metric(paths[l][i], received_signal[i])\n",
        "\n",
        "            # Sort and prune paths based on metrics (keep best `list_size` paths)\n",
        "            best_indices = np.argsort(path_metrics)[:self.list_size]\n",
        "            paths = [paths[i] for i in best_indices]\n",
        "            path_metrics = [path_metrics[i] for i in best_indices]\n",
        "\n",
        "        # Extract and return the best path\n",
        "        best_path = paths[np.argmin(path_metrics)]\n",
        "        return best_path[:self.K]  # Return the first K bits\n",
        "\n",
        "    def make_decision(self, received_signal, path_metric):\n",
        "        # Placeholder logic for bit decision\n",
        "        return 0 if received_signal < 0.5 else 1\n",
        "\n",
        "    def calculate_metric(self, bit_decision, received_signal):\n",
        "        # Example metric computation (Hamming distance, etc.)\n",
        "        return np.abs(bit_decision - received_signal)\n",
        "##########################################################################\n",
        "#latest SCL decoder\n",
        "def run_scl_decoder(polar_code_gen, SNRS, list_size, channel_type, num_trials):\n",
        "    results = []\n",
        "    for snr_db in SNRS:\n",
        "        X, y = prepare_polar_dataset(channel_type, polar_code_gen, num_samples=num_trials, snr_db=snr_db)\n",
        "\n",
        "        decoder = SCLDecoder(N=polar_code_gen.N, K=polar_code_gen.K, list_size=list_size)\n",
        "        decoded_bits = np.array([decoder.decode(x) for x in X])\n",
        "\n",
        "        ber = np.sum(np.abs(decoded_bits - y)) / (num_trials * polar_code_gen.K)\n",
        "        bler = np.mean(np.any(decoded_bits != y, axis=1))\n",
        "\n",
        "        results.append({'SNR': snr_db, 'BER': ber, 'BLER': bler})\n",
        "        print(f\"SCL Decoder - SNR: {snr_db:.1f} dB, List Size: {list_size}, BER: {ber:.4f}, BLER: {bler:.4f}\") # Add this line\n",
        "\n",
        "    return results\n",
        "##########################################################################\n",
        "\n",
        "# part five\n",
        "\n",
        "# Modified performance comparison to evaluate multi-bit predictions\n",
        "# This function will now evaluate the same RNN decoder but store results keyed by 'list_size' labels.\n",
        "#def performance_comparison(rnn_trainer, polar_code_gen, snr_range, channel_name, list_sizes, num_trials):\n",
        "    # Initialize performance results dictionary to store results for each list size label\n",
        " #   performance_results = {list_size: {'BER': [], 'BLER': []} for list_size in list_sizes}\n",
        "  #  channel_simulator = EnhancedChannelSimulator(channel_type=channel_name)\n",
        "\n",
        "   # for snr_db in snr_range:\n",
        "        # Generate data for performance evaluation\n",
        "    #    X, y = prepare_polar_dataset(polar_code_gen, num_samples=num_trials, snr_db=snr_db, channel_type=channel_name)\n",
        "\n",
        "     #   predictions = rnn_trainer.predict(X) # predictions shape: [num_trials, K]\n",
        "      #  actual_labels = y # actual_labels shape: [num_trials, K]\n",
        "\n",
        "        # Calculate BER: Total number of bit errors / Total number of bits\n",
        "       # ber = np.sum(np.abs(predictions - actual_labels)) / (num_trials * polar_code_gen.K)\n",
        "\n",
        "        # Calculate BLER: Number of blocks with at least one bit error / Total number of blocks\n",
        "        #block_errors = np.sum(np.any(predictions != actual_labels, axis=1))\n",
        "        #bler = block_errors / num_trials\n",
        "\n",
        "        # Store the calculated BER and BLER for EACH specified list size label.\n",
        "        # Note: The values are the same because it's the same RNN performance being measured.\n",
        "        #for list_size in list_sizes:\n",
        "         #   performance_results[list_size]['BER'].append(ber)\n",
        "          #  performance_results[list_size]['BLER'].append(bler)\n",
        "\n",
        "    #return performance_results\n",
        "\n",
        "# Modified plot function to use the updated performance results structure\n",
        "def plot_comprehensive_analysis(train_losses, val_losses, performance_results, snr_range, channel_name):\n",
        "    plt.figure(figsize=(12, 15)) # Increased figure size\n",
        "\n",
        "    # Plot Training and Validation Loss\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title(f'{channel_name} Channel - Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    # Plot BER (from performance_results)\n",
        "    plt.subplot(3, 1, 2)\n",
        "    # Iterate through the decoder types (which are now just the list size labels)\n",
        "    for list_size, results in performance_results.items():\n",
        "        # Use the list_size as the label\n",
        "        plt.plot(snr_range, results['BER'], label=f'RNN Decoder (List size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_name} Channel - BER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.yscale('log')\n",
        "    # Adjust ylim for BER to show better detail at lower error rates if needed\n",
        "    plt.ylim(1e-4, 1) # Example adjustment\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\") # Add grid\n",
        "\n",
        "    # Plot BLER (from performance_results)\n",
        "    plt.subplot(3, 1, 3)\n",
        "    # Iterate through the decoder types (which are now just the list size labels)\n",
        "    for list_size, results in performance_results.items():\n",
        "        # Use the list_size as the label\n",
        "        plt.plot(snr_range, results['BLER'], label=f'RNN Decoder (List size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_name} Channel - BLER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.yscale('log')\n",
        "     # Adjust ylim for BLER to show better detail at lower error rates if needed\n",
        "    plt.ylim(1e-4, 1) # Example adjustment\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\") # Add grid\n",
        "\n",
        "\n",
        "    plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
        "    plt.show()\n",
        "####################################################\n",
        "# another trial to have all plots\n",
        "\n",
        "def performance_comparison(rnn_trainer, polar_code_gen, snr_range, channel_name, list_sizes, num_trials):\n",
        "    performance_results = {list_size: {'BER': [], 'BLER': []} for list_size in list_sizes}\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_name)\n",
        "\n",
        "    for list_size in list_sizes:\n",
        "        for snr_db in snr_range:\n",
        "            # Here, introduce any logic specific to list_size\n",
        "            X, y = prepare_polar_dataset(polar_code_gen, num_samples=num_trials, snr_db=snr_db, channel_type=channel_name)\n",
        "\n",
        "            # Example: Adjust how predictions are made or interpreted based on list size\n",
        "            predictions = rnn_trainer.predict(X)\n",
        "\n",
        "            actual_labels = y\n",
        "            ber = np.sum(np.abs(predictions - actual_labels)) / (num_trials * polar_code_gen.K)\n",
        "            block_errors = np.sum(np.any(predictions != actual_labels, axis=1))\n",
        "            bler = block_errors / num_trials\n",
        "\n",
        "            performance_results[list_size]['BER'].append(ber)\n",
        "            performance_results[list_size]['BLER'].append(bler)\n",
        "            print(f\"List Size: {list_size}, SNR: {snr_db}, BER: {ber}, BLER: {bler}\")\n",
        "\n",
        "    return performance_results\n",
        "\n",
        "\n",
        "#####################################################\n",
        "# with two SNR separately teh comparison of SCL tradional and ML decoder\n",
        "######################################################################\n",
        "\n",
        "##############################################################\n",
        "def plot_ber_bler(results, channel_type, decoder_type, snr_range):\n",
        "    plt.figure(figsize=(12, 16))\n",
        "\n",
        "    # Plot BER\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for i, list_size in enumerate(LIST_SIZES):\n",
        "        ber = [res['BER'] for res in results[channel_type][decoder_type][i]]\n",
        "        plt.plot(snr_range, ber, label=f'{decoder_type} Decoder (List Size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_type} Channel - {decoder_type} BER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(10**0, 10**-4)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # Plot BLER\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for i, list_size in enumerate(LIST_SIZES):\n",
        "        bler = [res['BLER'] for res in results[channel_type][decoder_type][i]]\n",
        "        plt.plot(snr_range, bler, label=f'{decoder_type} Decoder (List Size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_type} Channel - {decoder_type} BLER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(10**0, 10**-4)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "##############################################################\n",
        "#again plot results\n",
        "###################################################\n",
        "#latest plot functions\n",
        "LIST_SIZES = [1, 8, 16]\n",
        "SNR_AWGN     = np.linspace(0, 5, 11)\n",
        "SNR_RAYLEIGH = np.linspace(0, 10, 11)\n",
        "####################################################################\n",
        "# Latest\n",
        "def compare_decoders(polar_code_gen):\n",
        "    results = {}\n",
        "    for channel in ['AWGN','Rayleigh']:\n",
        "        snr_grid = SNR_AWGN if channel=='AWGN' else SNR_RAYLEIGH\n",
        "\n",
        "        ml_dict = {}\n",
        "        scl_dict = {}\n",
        "\n",
        "        # ML-decoder part (already working)\n",
        "\n",
        "        # SCL-decoder\n",
        "        for L in LIST_SIZES:\n",
        "            scl_list = run_scl_decoder(\n",
        "                polar_code_gen, snr_grid, L, channel, NUM_TRIALS_PERF\n",
        "            )\n",
        "            ber_list  = [x['BER']  for x in scl_list]\n",
        "            bler_list = [x['BLER'] for x in scl_list]\n",
        "            scl_dict[L] = {'BER':ber_list, 'BLER':bler_list}\n",
        "            print(f\"Compare Decoders - Channel: {channel}, SCL Results for L={L}: {scl_dict[L]}\") # Add this line\n",
        "\n",
        "        results[channel] = {'ML': ml_dict, 'SCL': scl_dict}\n",
        "    return results\n",
        "####################################################################\n",
        "\n",
        "\n",
        "def plot_channel(results, channel, decoder_key, snr_grid):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    # BER\n",
        "    plt.subplot(2,1,1)\n",
        "    for L in LIST_SIZES:\n",
        "        y = results[channel][decoder_key][L]['BER']\n",
        "        plt.plot(snr_grid, y, label=f\"{decoder_key} L={L}\")\n",
        "    plt.title(f\"{channel} – {decoder_key} BER\")\n",
        "    plt.yscale('log'); plt.ylim(1,1e-4)\n",
        "    plt.xlabel(\"SNR (dB)\"); plt.ylabel(\"BER\")\n",
        "    plt.grid(True,which='both'); plt.legend()\n",
        "\n",
        "    # BLER\n",
        "    plt.subplot(2,1,2)\n",
        "    for L in LIST_SIZES:\n",
        "        y = results[channel][decoder_key][L]['BLER']\n",
        "        plt.plot(snr_grid, y, label=f\"{decoder_key} L={L}\")\n",
        "    plt.title(f\"{channel} – {decoder_key} BLER\")\n",
        "    plt.yscale('log'); plt.ylim(1,1e-4)\n",
        "    plt.xlabel(\"SNR (dB)\"); plt.ylabel(\"BLER\")\n",
        "    plt.grid(True,which='both'); plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "###################################################\n",
        " # part six\n",
        "#latest main\n",
        "#Add on traditional decoder comprison\n",
        "#latest main\n",
        "#####################################################\n",
        "def plot_results(results, channel_type, decoder_key, decoder_label):\n",
        "    snr_range = SNR_AWGN if channel_type == 'AWGN' else SNR_RAYLEIGH\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Plot BER\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for i, list_size in enumerate(LIST_SIZES):\n",
        "        ber = [res['BER'] for res in results[channel_type][decoder_key][i]]\n",
        "        plt.plot(snr_range, ber, label=f'{decoder_label} Decoder (List Size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_type} Channel - {decoder_label} BER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(10**0, 10**-4)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # Plot BLER\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for i, list_size in enumerate(LIST_SIZES):\n",
        "        bler = [res['BLER'] for res in results[channel_type][decoder_key][i]]\n",
        "        plt.plot(snr_range, bler, label=f'{decoder_label} Decoder (List Size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_type} Channel - {decoder_label} BLER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(10**0, 10**-4)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#####################################################\n",
        "def main():\n",
        "    try:\n",
        "        BLOCK_LENGTH = 32\n",
        "        INFO_BITS = 16\n",
        "        LEARNING_RATE = 1e-3\n",
        "        EPOCHS = 50\n",
        "        BATCH_SIZE = 32\n",
        "        NUM_SAMPLES_TRAIN = 10000 # Increased training samples\n",
        "        NUM_TRIALS_PERF = 1000  # Number of trials (blocks) for performance comparison at each SNR\n",
        "        SNR_RANGE_AWGN = np.linspace(0, 5, 11) # More points for smoother curve\n",
        "        SNR_RANGE_RAYLEIGH = np.linspace(0, 10, 11) # More points for smoother curve\n",
        "        LIST_SIZES = [1, 8, 16] # List sizes to use for plotting labels\n",
        "\n",
        "\n",
        "        polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "        results = {}\n",
        "        channels = {\n",
        "            'AWGN': EnhancedChannelSimulator(channel_type='AWGN'),\n",
        "            'Rayleigh': EnhancedChannelSimulator(channel_type='Rayleigh')\n",
        "        }\n",
        "\n",
        "        for channel_name, channel in channels.items():\n",
        "            logging.info(f\"Analyzing {channel_name} Channel\")\n",
        "            # Prepare dataset for training and validation\n",
        "            logging.info(f\"Generating training data ({NUM_SAMPLES_TRAIN} samples) for {channel_name} at SNR=5dB\")\n",
        "            # Train at a fixed moderate SNR, evaluate performance across a range\n",
        "            X, y = prepare_polar_dataset(polar_code_gen, num_samples=NUM_SAMPLES_TRAIN, snr_db=5.0, channel_type=channel_name)\n",
        "\n",
        "            # Convert numpy arrays to PyTorch tensors\n",
        "            X_tensor = torch.FloatTensor(X)\n",
        "            y_tensor = torch.FloatTensor(y) # y_tensor shape: [num_samples, K]\n",
        "\n",
        "            # Flatten input features for the FCNN-based decoder\n",
        "            X_tensor_flat = X_tensor.view(X_tensor.shape[0], -1) # Shape [num_samples, N]\n",
        "\n",
        "            # No need to split y_tensor into binary labels, keep its original shape [num_samples, K]\n",
        "            # The BCELoss will expect predictions of shape [batch_size, K] and targets of shape [batch_size, K]\n",
        "\n",
        "            # Verify tensor shapes before training\n",
        "            print(\"\\n🔬 Processed Tensor Shapes (Training):\")\n",
        "            print(f\"X_tensor_flat shape: {X_tensor_flat.shape}\")\n",
        "            print(f\"y_tensor shape: {y_tensor.shape}\")\n",
        "\n",
        "            # Calculate the input size for the RNN based on the flattened data\n",
        "            input_feature_size = X_tensor_flat.size(1) # This will be N (BLOCK_LENGTH)\n",
        "            output_size = INFO_BITS # The RNN should output K bits\n",
        "            print(f\"Calculated input feature size: {input_feature_size}\")\n",
        "            print(f\"Calculated output size (info bits): {output_size}\")\n",
        "\n",
        "\n",
        "            # Enhanced RNN Decoder (now correctly outputs K bits)\n",
        "            rnn_model = EnhancedRNNDecoder(input_size=input_feature_size, output_size=output_size)\n",
        "            rnn_trainer = DecoderTrainer(rnn_model)\n",
        "\n",
        "            logging.info(f\"Starting training for {channel_name} Channel RNN Decoder\")\n",
        "            # Train the RNN Decoder with multi-bit labels\n",
        "            # Pass the flattened X and original y tensors\n",
        "            train_losses, val_losses = rnn_trainer.train(X_tensor_flat, y_tensor, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "            logging.info(f\"Finished training for {channel_name} Channel RNN Decoder\")\n",
        "\n",
        "\n",
        "            # Perform performance comparison across SNR range\n",
        "            snr_range = SNR_RANGE_AWGN if channel_name == 'AWGN' else SNR_RANGE_RAYLEIGH\n",
        "            logging.info(f\"Evaluating performance for {channel_name} Channel across SNR range: {snr_range}\")\n",
        "\n",
        "            # Call the modified performance_comparison\n",
        "            # This will run the RNN decoder performance once and store results under multiple list_size keys.\n",
        "            performance_results = performance_comparison(\n",
        "                rnn_trainer, polar_code_gen, snr_range, channel_name, LIST_SIZES, NUM_TRIALS_PERF\n",
        "            )\n",
        "            logging.info(f\"Finished performance evaluation for {channel_name} Channel\")\n",
        "\n",
        "\n",
        "            # Plotting Confusion Matrix for the test set\n",
        "            # First, prepare a separate test set for confusion matrix visualization\n",
        "            # Use a moderate SNR, e.g., 3dB, and a reasonable number of samples\n",
        "            logging.info(f\"Generating test data ({NUM_TRIALS_PERF} samples) for Confusion Matrix at SNR=3dB for {channel_name}\")\n",
        "            X_test_cm, y_test_cm = prepare_polar_dataset(polar_code_gen, num_samples=NUM_TRIALS_PERF, snr_db=3.0, channel_type=channel_name)\n",
        "            X_test_cm_tensor = torch.FloatTensor(X_test_cm).view(X_test_cm.shape[0], -1)\n",
        "            y_test_cm_tensor = torch.FloatTensor(y_test_cm) # Keep original shape [num_samples, K]\n",
        "\n",
        "\n",
        "            predictions_test = rnn_trainer.predict(X_test_cm_tensor) # predictions_test shape: [num_samples, K]\n",
        "            actual_labels_test = y_test_cm_tensor.numpy() # actual_labels_test shape: [num_samples, K]\n",
        "\n",
        "            # To plot a single confusion matrix, we need to flatten the predictions and actual labels\n",
        "            # This treats each predicted bit as an independent classification outcome.\n",
        "            predictions_flat = predictions_test.flatten()\n",
        "            actual_labels_flat = actual_labels_test.flatten()\n",
        "\n",
        "            # Calculate and display confusion matrix\n",
        "            logging.info(f\"Plotting Confusion Matrix for {channel_name} Channel Test Set\")\n",
        "            cm = confusion_matrix(actual_labels_flat, predictions_flat)\n",
        "            ConfusionMatrixDisplay(cm, display_labels=[0, 1]).plot() # Specify display_labels\n",
        "            plt.title(f'Confusion Matrix - {channel_name}')\n",
        "            plt.xlabel('Predicted label (All Info Bits)')\n",
        "            plt.ylabel('True label (All Info Bits)')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            # Plot comprehensive analysis (training loss, BER, BLER)\n",
        "            logging.info(f\"Plotting performance analysis for {channel_name} Channel\")\n",
        "            plot_comprehensive_analysis(\n",
        "                 train_losses, val_losses, performance_results, snr_range, channel_name\n",
        "            )\n",
        "\n",
        "\n",
        "            results[channel_name] = {\n",
        "                'decoder': rnn_trainer,\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'performance': performance_results\n",
        "            }\n",
        "\n",
        "        logging.info(\"🎉 Simulation Complete!\")\n",
        "        return results\n",
        "        results = compare_decoders()  # Call the comparison function\n",
        "        # results = compare_decoders()\n",
        "        # Plot for each channel\n",
        "        for ch in ['AWGN','Rayleigh']:\n",
        "           plot_channel(results, ch, 'ML',     SNR_AWGN if ch=='AWGN' else SNR_RAYLEIGH)\n",
        "           plot_channel(results, ch, 'SCL',    SNR_AWGN if ch=='AWGN' else SNR_RAYLEIGH)\n",
        "        return results\n",
        "     #   for channel_type in ['AWGN', 'Rayleigh']:\n",
        "      #      snr_range = SNR_AWGN if channel_type == 'AWGN' else SNR_RAYLEIGH\n",
        "       #     plot_ber_bler(results, channel_type, 'ML', snr_range)\n",
        "        #    plot_ber_bler(results, channel_type, 'SCL', snr_range)\n",
        "\n",
        "       # for channel_type in ['AWGN', 'Rayleigh']:\n",
        "          #  plot_results(results, channel_type)\n",
        "\n",
        "        # Analyze and plot results as desired\n",
        "    except Exception as e:\n",
        "        logging.error(f\"🆘 Comprehensive Simulation Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "RkCo-6B2Ai8u",
        "outputId": "f7446d2f-6da0-4e03-fa63-b0a0ec23a3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping numba as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-decision-forests as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tf-keras as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scipy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cuml-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dask-cuda as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping distributed-ucxx-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ml-dtypes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorboard as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dask-cudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping raft-dask-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping sklearn-pandas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m332.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "keras-hub 0.18.1 requires tensorflow-text; platform_system != \"Darwin\", which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "bqplot 0.12.44 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "datasets 2.14.4 requires pandas, which is not installed.\n",
            "yfinance 0.2.61 requires pandas>=1.3.0, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "keras 3.8.0 requires ml-dtypes, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "pymc 5.22.0 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "accelerate 1.6.0 requires torch>=2.0.0, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "osqp 1.0.4 requires scipy>=0.13.2, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "bigquery-magics 0.9.0 requires pandas>=1.1.0, which is not installed.\n",
            "albumentations 2.0.6 requires scipy>=1.10.0, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "holoviews 1.20.2 requires pandas>=1.3, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "tensorflow-hub 0.16.1 requires tf-keras>=2.14.1, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "bigframes 2.4.0 requires pandas>=1.5.3, which is not installed.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, which is not installed.\n",
            "jaxlib 0.5.1 requires ml-dtypes>=0.2.0, which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "xarray 2025.3.1 requires pandas>=2.1, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "panel 1.6.3 requires pandas>=1.2, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "pandas-gbq 0.28.1 requires pandas>=1.1.4, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "peft 0.15.2 requires torch>=1.13.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "pytensor 2.30.3 requires scipy<2,>=1, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "prophet 1.1.6 requires pandas>=1.0.4, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m167.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-17dfb81c58df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;31m# These were moved in 1.25 and may be deprecated eventually:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;34m\"ModuleDeprecationWarning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VisibleDeprecationWarning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;34m\"ComplexWarning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TooHardError\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AxisError\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
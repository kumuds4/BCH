{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pH4U1sst6dqN",
        "outputId": "96221afb-6f4c-4d67-af3d-da3a813f98fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "uKoUJd1C67GN",
        "outputId": "67de0e84-d932-4e21-f75c-05a132f494e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2fd903a8-f3ca-4ea2-8593-897b693d9099\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2fd903a8-f3ca-4ea2-8593-897b693d9099\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 610ltstpolarML.py to 610ltstpolarML.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#latest from Gemini evening /night 11:40 PM\n",
        "# Polar transform\n",
        "#Latest done early morning 06/09/25\n",
        "#no doing during evening 06/08/25\n",
        "#plots need be fixed. They are flat.\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import logging\n",
        "import pandas as pd\n",
        "import traceback # Import the traceback module\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Configuration parameters\n",
        "BLOCK_LENGTH = 128\n",
        "INFO_BITS = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "NUM_SAMPLES_TRAIN = 50000\n",
        "NUM_TRIALS_PERF = 1500\n",
        "SNR_RANGE_AWGN = np.linspace(0, 5, 11)\n",
        "LIST_SIZES = [1, 8, 16]\n",
        "\n",
        "#Part 2: Polar Code Generator and Modulation\n",
        "####################################################\n",
        "\n",
        "\n",
        "#######################################\n",
        "#latest on evening of 06/10/25\n",
        "\n",
        "# Centralized CRC Calculation Function (for 3GPP style)\n",
        "def compute_crc(data, polynomial):\n",
        "    \"\"\"\n",
        "    Computes CRC for data using the given polynomial (3GPP style).\n",
        "    Args:\n",
        "        data: Numpy array of binary data bits (0s and 0s).\n",
        "        polynomial: Numpy array of binary polynomial coefficients (e.g., [1, 0, 0, 0, 1, 0, 0, 1] for x^7 + x^3 + 1).\n",
        "    Returns:\n",
        "        Numpy array of CRC bits.\n",
        "    \"\"\"\n",
        "    poly_len = len(polynomial)\n",
        "    crc_length = poly_len - 1 # CRC length is polynomial degree\n",
        "    # Append crc_length zeros\n",
        "    data_with_zeros = np.concatenate((data, np.zeros(crc_length, dtype=int)))\n",
        "\n",
        "    remainder = np.copy(data_with_zeros)\n",
        "\n",
        "    # Perform polynomial division from MSB\n",
        "    for i in range(len(remainder) - poly_len + 1):\n",
        "        if remainder[i] == 1:\n",
        "            remainder[i : i + poly_len] ^= polynomial\n",
        "\n",
        "    return remainder[-crc_length:]\n",
        "\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N, K, channel_snr_db, crc_type='CRC-7'):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.R = K / N  # Add code rate\n",
        "        self.channel_snr_db = channel_snr_db # SNR for channel reliability calculation\n",
        "        self.crc_type = crc_type\n",
        "        self.crc_polynomials = {'CRC-7': (np.array([1, 0, 0, 0, 1, 0, 0, 1], dtype=int), 7)}\n",
        "\n",
        "        if crc_type in self.crc_polynomials:\n",
        "            self._crc_polynomial = self.crc_polynomials[crc_type][0]\n",
        "            self._crc_length = self.crc_polynomials[crc_type][1]\n",
        "        else:\n",
        "            self._crc_polynomial = None\n",
        "            self._crc_length = 0\n",
        "\n",
        "        self.K_crc = self.K + self._crc_length\n",
        "\n",
        "        # Determine frozen/info sets using mutual information based method\n",
        "        self.frozen_set, self.info_set = self._get_frozen_and_info_sets_mi()\n",
        "\n",
        "        logging.info(f\"PolarCodeGenerator: N={self.N}, K={self.K}, R={self.R:.4f}, CRC Length={self._crc_length}, K_crc={self.K_crc}\")\n",
        "        logging.info(f\"Info Set (first 5, last 5): {self.info_set[:5]} ... {self.info_set[-5:]}\")\n",
        "        logging.info(f\"Frozen Set (first 5, last 5): {self.frozen_set[:5]} ... {self.frozen_set[-5:]}\")\n",
        "\n",
        "    def _get_frozen_and_info_sets_mi(self):\n",
        "        # Simplified mutual information approximation for reliability\n",
        "        # This is a basic approach and more sophisticated methods exist.\n",
        "        # Reliability is inversely related to the Bhattacharyya parameter I(W).\n",
        "        # For BEC, I(W) = p (erasure probability)\n",
        "        # For AWGN, calculating exact mutual information is complex.\n",
        "        # A common approximation relates reliability to channel capacity or similar metrics.\n",
        "        # For a fixed N and channel type, the relative reliability order is constant.\n",
        "        # We can use a pre-computed order or a more involved recursive calculation.\n",
        "\n",
        "        # Let's use a simple recursive method to get the reliability order\n",
        "        # based on the channel combination operation.\n",
        "\n",
        "        def get_reliability_order(n):\n",
        "            if n == 1:\n",
        "                return [0]\n",
        "            half_n = n // 2\n",
        "            order_half = get_reliability_order(half_n)\n",
        "            order = []\n",
        "            # Indices for the combined channels\n",
        "            order.extend([2 * i for i in order_half]) # Less reliable channels\n",
        "            order.extend([2 * i + 1 for i in order_half]) # More reliable channels\n",
        "            return order\n",
        "\n",
        "        reliability_order = get_reliability_order(self.N)\n",
        "\n",
        "        # Sort indices based on a metric (e.g., Bhattacharyya parameter).\n",
        "        # Since calculating the exact Bhattacharyya parameter recursively for AWGN\n",
        "        # is involved, we'll use the known relative order of channels for AWGN.\n",
        "        # The 'better' channels have lower Bhattacharyya parameter values (or higher capacity/reliability).\n",
        "        # The recursive function above generates indices in a specific order.\n",
        "        # To get the reliability order, we need to know which index corresponds\n",
        "        # to a more reliable channel after the polarization step.\n",
        "        # The standard polarization construction for AWGN results in the channels\n",
        "        # formed from (u_i + u_{i+N/2}) being less reliable than u_i + u_{i+N/2} given u_i.\n",
        "\n",
        "        # A more correct way to get the reliability order for AWGN is using a\n",
        "        # recursive calculation of the Bhattacharyya parameter or similar metric.\n",
        "        # For practical purposes and for N=128, the 3GPP sequence is derived\n",
        "        # from these principles.\n",
        "\n",
        "        # Let's revert to using the 3GPP sequence approach for N=128 for accuracy,\n",
        "        # but keep the method name general (`_get_frozen_and_info_sets_mi`) to\n",
        "        # indicate the underlying principle. For other N, a proper recursive\n",
        "        # calculation would be needed.\n",
        "\n",
        "        if self.N == 128:\n",
        "             reliability_sequence = self._get_3gpp_reliability_sequence_128()\n",
        "             if len(reliability_sequence) >= self.K_crc:\n",
        "                 info_channel_indices = sorted(reliability_sequence[-self.K_crc:])\n",
        "             else:\n",
        "                 logging.error(f\"Reliability sequence (length {len(reliability_sequence)}) is shorter than K_crc ({self.K_crc}). Cannot determine info set correctly.\")\n",
        "                 info_channel_indices = sorted(reliability_sequence[-len(reliability_sequence):])\n",
        "\n",
        "             frozen_channel_indices = sorted(list(set(range(self.N)) - set(info_channel_indices)))\n",
        "\n",
        "        else:\n",
        "            # For N != 128, a proper recursive calculation of reliability indices is needed.\n",
        "            # This is a placeholder for a more general implementation.\n",
        "            # For this exercise, we'll stick to N=128 or raise an error.\n",
        "            raise NotImplementedError(f\"Mutual information-based frozen set calculation is not implemented for N={self.N}. Only N=128 is supported with the 3GPP sequence.\")\n",
        "\n",
        "\n",
        "        if len(info_channel_indices) != self.K_crc:\n",
        "            logging.warning(f\"Mismatch: Expected {self.K_crc} info indices, but got {len(info_channel_indices)}\")\n",
        "\n",
        "        if len(frozen_channel_indices) != self.N - self.K_crc:\n",
        "             logging.warning(f\"Mismatch: Expected {self.N - self.K_crc} frozen indices, but got {len(frozen_channel_indices)}\")\n",
        "\n",
        "\n",
        "        return frozen_channel_indices, info_channel_indices\n",
        "\n",
        "\n",
        "    def _get_3gpp_reliability_sequence_128(self):\n",
        "        # The pre-computed reliability sequence for N=128, AWGN (from 3GPP TS 38.212).\n",
        "        # This sequence lists bit indices in increasing order of reliability\n",
        "        # (least reliable first).\n",
        "        return [\n",
        "            0, 1, 2, 4, 8, 16, 3, 5, 9, 6, 17, 10, 18, 32, 12, 33,\n",
        "            20, 24, 34, 36, 40, 7, 11, 19, 21, 13, 22, 25, 26, 28,\n",
        "            48, 35, 37, 38, 41, 42, 44, 56, 14, 15, 23, 27, 29, 30,\n",
        "            31, 39, 43, 45, 46, 49, 50, 52, 57, 58, 60, 63, 47, 51,\n",
        "            53, 54, 59, 61, 62, 65, 66, 67, 68, 70, 72, 73, 74, 75,\n",
        "            76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 77,\n",
        "            79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 102, 103,\n",
        "            104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
        "            116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127\n",
        "        ]\n",
        "\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def polar_encode(self, info_bits, verbose=False):\n",
        "        info_bits_with_crc = self.append_crc(info_bits)\n",
        "\n",
        "        if verbose:\n",
        "             logging.info(f\"polar_encode: info_bits_with_crc (len {len(info_bits_with_crc)}): {info_bits_with_crc[:10]}...\")\n",
        "\n",
        "        if len(info_bits_with_crc) != len(self.info_set):\n",
        "            raise ValueError(f\"Length of info_bits_with_crc ({len(info_bits_with_crc)}) must match length of info_set ({len(self.info_set)})\")\n",
        "\n",
        "        u = np.zeros(self.N, dtype=int)\n",
        "        u[np.array(list(self.info_set))] = info_bits_with_crc\n",
        "\n",
        "        encoded = self._polar_transform(u)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def _polar_transform(self, u):\n",
        "        N = len(u)\n",
        "        if N == 1:\n",
        "            return u\n",
        "        else:\n",
        "            half_N = N // 2\n",
        "            u_np = np.array(u)\n",
        "            x_upper = self._polar_transform(u_np[:half_N])\n",
        "            x_lower = self._polar_transform(u_np[half_N:])\n",
        "            codeword = np.concatenate([(x_upper + x_lower) % 2, x_lower])\n",
        "            return codeword\n",
        "\n",
        "    def append_crc(self, info_bits):\n",
        "        if self.crc_type not in self.crc_polynomials:\n",
        "            return info_bits\n",
        "        polynomial, length = self.crc_polynomials[self.crc_type]\n",
        "        data_for_crc = np.copy(info_bits)\n",
        "        crc_bits = compute_crc(data_for_crc, polynomial)\n",
        "        if len(crc_bits) != length:\n",
        "             logging.warning(f\"Calculated CRC length ({len(crc_bits)}) does not match expected length ({length})\")\n",
        "        return np.concatenate((info_bits, crc_bits))\n",
        "############################################\n",
        "\n",
        "\n",
        "\n",
        "#Latest Polar Code Generator\n",
        "\n",
        "\n",
        "####################################################\n",
        "#Latest Polar Code Generator\n",
        "# ========== CRC Polynomial (CRC-8) ==========\n",
        "#crc_poly = np.array([1, 0, 0, 0, 1, 1, 0, 1, 1], dtype=int)  # x^8 + x^4 + x^3 + x + 1\n",
        "\n",
        "# ========== Functions ==========\n",
        "#######################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def bpsk_modulate(bits):\n",
        "    return 1 - 2 * bits\n",
        "\n",
        "def add_awgn(signal, snr_db):\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    noise_var = 1 / (2 * snr_linear)\n",
        "    noise = np.sqrt(noise_var) * np.random.randn(len(signal))\n",
        "    return signal + noise, noise_var\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################\n",
        "\n",
        "\n",
        "#############################################\n",
        "\n",
        "# Keep the rest of your code (BPSK modulation, Channel Simulation, RNN Decoder, Trainer, SCL Decoder, Plotting, Performance Comparison, and main function) as is for now\n",
        "####################################################\n",
        "\n",
        "\n",
        "#Part 3: Dataset Preparation and Channel Simulation\n",
        "############################################################\n",
        "#latest add bpsk_modulate\n",
        "def bpsk_modulate(codeword):\n",
        "  \"\"\"\n",
        "  Performs BPSK modulation on a binary codeword.\n",
        "\n",
        "  Args:\n",
        "    codeword: A numpy array of binary bits (0s and 1s).\n",
        "\n",
        "  Returns:\n",
        "    A numpy array of BPSK symbols (+1s and -1s).\n",
        "  \"\"\"\n",
        "  return 1 - 2 * codeword\n",
        "\n",
        "###########################################################\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def simulate(self, signal, snr_db):\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        noise_std = np.sqrt(1 / (2 * snr_linear))\n",
        "        noise = noise_std * np.random.randn(*signal.shape)\n",
        "        return signal + noise\n",
        "\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type='AWGN'):\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "    X, y = [], []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "        modulated_signal = bpsk_modulate(encoded_signal)\n",
        "        received_signal = channel_simulator.simulate(modulated_signal, snr_db)\n",
        "        X.append(received_signal)\n",
        "        y.append(info_bits)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def save_dataset_to_csv(X, y, filename='dataset.csv'):\n",
        "    data = np.hstack((X, y))\n",
        "    columns = [f'received_{i}' for i in range(X.shape[1])] + [f'bit_{j}' for j in range(y.shape[1])]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    df.to_csv(filename, index=False)\n",
        "    logging.info(f\"Dataset saved to {filename}\")\n",
        "\n",
        "#Part 4\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size=128, num_layers=2):\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers  # Ensure this is defined\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_reshaped = x.unsqueeze(1)  # Assuming sequence_length = 1\n",
        "\n",
        "        # Ensure h0 and c0 are created with the correct device and shape\n",
        "        # Use self.num_layers for multiple RNN layers\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.rnn(x_reshaped, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate):\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=50, batch_size=32):\n",
        "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        train_losses, val_losses = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            self.model.train()\n",
        "\n",
        "            for X_batch, y_batch in loader:\n",
        "                X_batch = X_batch.view(-1, BLOCK_LENGTH)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(X_batch)\n",
        "                loss = self.criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            train_loss = epoch_loss / len(loader)\n",
        "            train_losses.append(train_loss)\n",
        "            logging.info(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "            if X_val is not None and y_val is not None:\n",
        "                self.model.eval()\n",
        "                with torch.no_grad():\n",
        "                    val_output = self.model(X_val.view(-1, BLOCK_LENGTH))\n",
        "                    val_loss = self.criterion(val_output, y_val).item()\n",
        "                    val_losses.append(val_loss)\n",
        "                    logging.info(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses if X_val is not None else None\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_test.view(-1, BLOCK_LENGTH))\n",
        "            predicted = (outputs > 0.5).int()\n",
        "            correct = (predicted == y_test).sum().item()\n",
        "            total = y_test.numel()\n",
        "            accuracy = correct / total\n",
        "            # Calculate BER and BLER\n",
        "            bit_errors = torch.sum(predicted != y_test).item()\n",
        "            block_errors = torch.sum(torch.any(predicted != y_test, dim=1)).item()\n",
        "            ber = bit_errors / total\n",
        "            bler = block_errors / X_test.size(0)\n",
        "\n",
        "        return ber, bler\n",
        "\n",
        "\n",
        "#Part 5 and 6: SCL Decoder\n",
        "#########################################################\n",
        "#latest Polarcode decoder\n",
        "#It's on 06/08/25 evening/night\n",
        "#on 06/10/25 more changes for SCL decoder\n",
        "#latest Polarcode decoder\n",
        "#It's on 06/08/25 evening/night\n",
        "###########################################\n",
        "#SC decoder\n",
        "\n",
        "class SCDecoder:\n",
        "    def __init__(self, N, K, info_set, frozen_set):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.info_set = set(info_set)  # Convert to set for faster lookups\n",
        "        self.frozen_set = set(frozen_set) # Convert to set for faster lookups\n",
        "        self.u_hat = np.zeros(N, dtype=int) # Decoded bits\n",
        "        self.llrs = None # LLRs will be initialized at decode time\n",
        "\n",
        "    def decode(self, received_llrs):\n",
        "        if len(received_llrs) != self.N:\n",
        "            raise ValueError(f\"Input LLR length ({len(received_llrs)}) does not match code length N ({self.N}).\")\n",
        "\n",
        "        self.llrs = np.copy(received_llrs)\n",
        "        self._recursive_decode(0, self.N)\n",
        "\n",
        "        # Extract the information bits\n",
        "        decoded_info_bits = self.u_hat[list(sorted(self.info_set))]\n",
        "\n",
        "        return decoded_info_bits\n",
        "\n",
        "    def _recursive_decode(self, bit_index, block_size):\n",
        "        if block_size == 1:\n",
        "            # Decision step at the leaf nodes\n",
        "            if bit_index in self.frozen_set:\n",
        "                self.u_hat[bit_index] = 0 # Frozen bit is fixed to 0\n",
        "            else:\n",
        "                # Information bit - make a hard decision based on LLR\n",
        "                self.u_hat[bit_index] = 0 if self.llrs[bit_index] >= 0 else 1\n",
        "            return\n",
        "\n",
        "        half_size = block_size // 2\n",
        "\n",
        "        # Split step - compute LLRs for the first half (u_1)\n",
        "        # LLR(u1) approx sign(L1)*sign(L2)*min(|L1|,|L2|)\n",
        "        llr_f = np.sign(self.llrs[bit_index : bit_index + half_size]) * \\\n",
        "                np.sign(self.llrs[bit_index + half_size : bit_index + block_size]) * \\\n",
        "                np.minimum(np.abs(self.llrs[bit_index : bit_index + half_size]), \\\n",
        "                           np.abs(self.llrs[bit_index + half_size : bit_index + block_size]))\n",
        "\n",
        "        # Update LLRs for the first half sub-block\n",
        "        self.llrs[bit_index : bit_index + half_size] = llr_f\n",
        "\n",
        "        # Recursively decode the first half\n",
        "        self._recursive_decode(bit_index, half_size)\n",
        "\n",
        "        # Combine step - prepare information for the second half (u_2)\n",
        "        # Need decisions from the first half (u_1) for the G operation\n",
        "        u1_decisions = self.u_hat[bit_index : bit_index + half_size]\n",
        "        # LLR(u2) = L2 + (1 - 2*u1) * L1\n",
        "        # The L1 here refers to the LLRs of the *first* half *after* the f operation\n",
        "        llr_g = self.llrs[bit_index + half_size : bit_index + block_size] + \\\n",
        "                (1 - 2 * u1_decisions) * self.llrs[bit_index : bit_index + half_size]\n",
        "\n",
        "        # Update LLRs for the second half sub-block\n",
        "        self.llrs[bit_index + half_size : bit_index + block_size] = llr_g\n",
        "\n",
        "        # Recursively decode the second half\n",
        "        self._recursive_decode(bit_index + half_size, half_size)\n",
        "\n",
        "    # The _f and _g functions are implicitly used within the recursive function\n",
        "    # but can be defined separately for clarity if needed.\n",
        "    # def _f(self, L1, L2):\n",
        "    #     return np.sign(L1) * np.sign(L2) * np.minimum(np.abs(L1), np.abs(L2))\n",
        "\n",
        "    # def _g(self, L1, L2, u1):\n",
        "    #     return L2 + (1 - 2 * u1) * L1\n",
        "#############################################\n",
        "\n",
        "#Latest SCL DECODER\n",
        "class PolarCodeDecoder:\n",
        "    def __init__(self, N, K, list_size, crc_poly=None):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.list_size = list_size\n",
        "        self._crc_polynomial = None\n",
        "        self._crc_length = 0\n",
        "\n",
        "        if crc_poly is not None and isinstance(crc_poly, tuple) and len(crc_poly) == 2:\n",
        "             self._crc_polynomial = crc_poly[0]\n",
        "             self._crc_length = crc_poly[1]\n",
        "             self.K_crc = self.K + self._crc_length\n",
        "        else:\n",
        "             self.K_crc = self.K\n",
        "\n",
        "        # Get frozen and info sets using the 3GPP sequence method\n",
        "        # Corrected method call name\n",
        "        self.frozen_set, self.info_set = self._get_frozen_and_info_sets_from_3gpp_sequence()\n",
        "\n",
        "        # Debug prints for info and frozen sets in Decoder\n",
        "        logging.info(f\"PolarCodeDecoder: N={self.N}, K={self.K}, CRC Length={self._crc_length}, K_crc={self.K_crc}\")\n",
        "        logging.info(f\"Decoder Info Set (first 5, last 5): {self.info_set[:5]} ... {self.info_set[-5:]}\")\n",
        "        logging.info(f\"Decoder Frozen Set (first 5, last 5): {self.frozen_set[:5]} ... {self.frozen_set[-5:]}\")\n",
        "\n",
        "\n",
        "    def _get_3gpp_reliability_sequence_128(self):\n",
        "        # The pre-computed reliability sequence for N=128, AWGN (from 3GPP TS 38.212).\n",
        "        # This sequence lists bit indices in increasing order of reliability\n",
        "        # (least reliable first).\n",
        "        return [\n",
        "            0, 1, 2, 4, 8, 16, 3, 5, 9, 6, 17, 10, 18, 32, 12, 33,\n",
        "            20, 24, 34, 36, 40, 7, 11, 19, 21, 13, 22, 25, 26, 28,\n",
        "            48, 35, 37, 38, 41, 42, 44, 56, 14, 15, 23, 27, 29, 30,\n",
        "            31, 39, 43, 45, 46, 49, 50, 52, 57, 58, 60, 63, 47, 51,\n",
        "            53, 54, 59, 61, 62, 65, 66, 67, 68, 70, 72, 73, 74, 75,\n",
        "            76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 77,\n",
        "            79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 102, 103,\n",
        "            104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115,\n",
        "            116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127\n",
        "        ]\n",
        "\n",
        "\n",
        "    def _get_frozen_and_info_sets_from_3gpp_sequence(self):\n",
        "        reliability_sequence = self._get_3gpp_reliability_sequence_128()\n",
        "\n",
        "        if self.N != 128:\n",
        "             raise ValueError(f\"Code length N={self.N} is not supported by the hardcoded 3GPP sequence.\")\n",
        "\n",
        "        # The most reliable channels are used for information bits (including CRC)\n",
        "        # The sequence is ordered from least reliable to most reliable,\n",
        "        # so we take the last K_crc elements for the info set.\n",
        "        if len(reliability_sequence) >= self.K_crc:\n",
        "            info_channel_indices = sorted(reliability_sequence[-self.K_crc:])\n",
        "        else:\n",
        "             logging.error(f\"Reliability sequence (length {len(reliability_sequence)}) is shorter than K_crc ({self.K_crc}). Cannot determine info set correctly.\")\n",
        "             info_channel_indices = sorted(reliability_sequence[-len(reliability_sequence):])\n",
        "\n",
        "        # The remaining channels are frozen\n",
        "        frozen_channel_indices = sorted(list(set(range(self.N)) - set(info_channel_indices)))\n",
        "\n",
        "        if len(info_channel_indices) != self.K_crc:\n",
        "            logging.warning(f\"Mismatch: Expected {self.K_crc} info indices, but got {len(info_channel_indices)}\")\n",
        "\n",
        "        if len(frozen_channel_indices) != self.N - self.K_crc:\n",
        "             logging.warning(f\"Mismatch: Expected {self.N - self.K_crc} frozen indices, but got {len(frozen_channel_indices)}\")\n",
        "\n",
        "        return frozen_channel_indices, info_channel_indices\n",
        "\n",
        "\n",
        "    def decode(self, received_llrs):\n",
        "        # Initialize list of active path indices. Initially, only one path (index 0) is active.\n",
        "        active_path_indices = [0]\n",
        "        # Initialize lists to store LLRs, hard decisions, and path metrics for each path.\n",
        "        # We start with one path, which is a copy of the input LLRs and an empty hard decision array.\n",
        "        self.llrs = [np.copy(received_llrs)]\n",
        "        self.hard_decisions = [np.zeros(self.N, dtype=int)]\n",
        "        # Initialize path metrics. A common initialization is 0.0.\n",
        "        self.path_metrics = [0.0]\n",
        "\n",
        "        # Start the recursive decoding process.\n",
        "        final_active_path_indices = self._recursive_decode(active_path_indices, 0, self.N)\n",
        "\n",
        "        # Select the best path among the final active paths.\n",
        "        # If CRC is used, we prioritize paths with valid CRC.\n",
        "        if self._crc_polynomial is not None:\n",
        "             valid_paths = []\n",
        "             # logging.info(f\"Checking CRC for {len(final_active_path_indices)} paths.\") # Optional logging\n",
        "             for path_idx in final_active_path_indices:\n",
        "                 # Extract the bits corresponding to info set indices for CRC check\n",
        "                 decoded_bits_at_info_indices = self.hard_decisions[path_idx][list(sorted(self.info_set))]\n",
        "\n",
        "                 # Ensure the extracted bits have the expected length before checking CRC\n",
        "                 if len(decoded_bits_at_info_indices) == self.K_crc and self._check_crc(decoded_bits_at_info_indices):\n",
        "                     valid_paths.append(path_idx)\n",
        "                     # logging.info(f\"Path {path_idx} has a valid CRC.\") # Optional logging\n",
        "                 # else: # Optional logging for CRC failures\n",
        "                     # logging.info(f\"Path {path_idx} CRC check failed or length mismatch.\")\n",
        "                     # if len(decoded_bits_at_info_indices) != self.K_crc:\n",
        "                     #    logging.warning(f\"Path {path_idx}: Length mismatch for CRC check. Got {len(decoded_bits_at_info_indices)}, Expected {self.K_crc}\")\n",
        "\n",
        "\n",
        "             if valid_paths:\n",
        "                 # If valid paths exist, choose the one with the minimum path metric.\n",
        "                 logging.info(f\"Found {len(valid_paths)} paths with valid CRC. Selecting the one with the best metric.\")\n",
        "                 best_path_index_in_valid = np.argmin([self.path_metrics[i] for i in valid_paths])\n",
        "                 best_path_index = valid_paths[best_path_index_in_valid]\n",
        "                 # logging.info(f\"Selected valid path {best_path_index} with metric {self.path_metrics[best_path_index]:.4f}\") # Optional logging\n",
        "             else:\n",
        "                 # If no valid CRC path, choose the path with the overall best metric among the final active paths.\n",
        "                 # This indicates a decoding failure where no path satisfies the CRC,\n",
        "                 # but we return the \"most likely\" result according to the path metric.\n",
        "                 logging.warning(\"No valid CRC path found among the best paths. Choosing the path with the best metric.\")\n",
        "                 best_path_index_in_final = np.argmin([self.path_metrics[i] for i in final_active_path_indices])\n",
        "                 best_path_index = final_active_path_indices[best_path_index_in_final]\n",
        "                 # logging.warning(f\"Selected path {best_path_index} with metric {self.path_metrics[best_path_index]:.4f}\") # Optional logging\n",
        "\n",
        "        else:\n",
        "            # If no CRC is used, simply choose the path with the minimum path metric among the final active paths.\n",
        "            logging.info(f\"No CRC used. Selecting the path with the best metric among {len(final_active_path_indices)} final paths.\")\n",
        "            best_path_index_in_final = np.argmin([self.path_metrics[i] for i in final_active_path_indices])\n",
        "            best_path_index = final_active_path_indices[best_path_index_in_final]\n",
        "            # logging.info(f\"Selected path {best_path_index} with best metric {self.path_metrics[best_path_index]:.4f}.\") # Optional logging\n",
        "\n",
        "\n",
        "        # Extract and return the decoded information bits (first K bits of the K_crc bits)\n",
        "        decoded_info_bits_with_crc = self.hard_decisions[best_path_index][list(sorted(self.info_set))]\n",
        "\n",
        "        # Ensure we return exactly K information bits\n",
        "        if len(decoded_info_bits_with_crc) >= self.K:\n",
        "             return decoded_info_bits_with_crc[:self.K]\n",
        "        else:\n",
        "             logging.error(f\"Decoded bits at info indices (length {len(decoded_info_bits_with_crc)}) is shorter than K ({self.K}). Returning truncated bits.\")\n",
        "             return decoded_info_bits_with_crc[:min(len(decoded_info_bits_with_crc), self.K)] # Return up to K bits\n",
        "\n",
        "\n",
        "    def _recursive_decode(self, active_path_indices, bit_index, block_size):\n",
        "        # Base Case: Arrived at a single bit\n",
        "        if block_size == 1:\n",
        "            next_active_path_indices = []\n",
        "            for path_idx in active_path_indices:\n",
        "                llr = self.llrs[path_idx][bit_index]\n",
        "\n",
        "                # Check if this is an information bit index (including CRC)\n",
        "                is_info_bit = bit_index in self.info_set\n",
        "\n",
        "                # If it's a frozen bit, the decision is fixed to 0 for all paths.\n",
        "                if bit_index in self.frozen_set:\n",
        "                     hard_decision = 0\n",
        "                     # For a frozen bit, there is no alternative decision to consider for path splitting.\n",
        "                     # The path metric contribution for a frozen bit is typically considered 0\n",
        "                     # because the decision is not based on the channel observation.\n",
        "                     self.hard_decisions[path_idx][bit_index] = hard_decision\n",
        "                     next_active_path_indices.append(path_idx)\n",
        "                else: # If it's an information bit (or CRC bit within the info set)\n",
        "                    # For information bits, we consider two possible hard decisions (0 and 1).\n",
        "                    # We need to calculate the path metric for both decisions and potentially\n",
        "                    # create new paths if the list size allows.\n",
        "\n",
        "                    # Calculate path metric for decision 0\n",
        "                    path_metric_0 = self.path_metrics[path_idx] + np.log(1 + np.exp(-llr))\n",
        "\n",
        "                    # Calculate path metric for decision 1\n",
        "                    path_metric_1 = self.path_metrics[path_idx] + np.log(1 + np.exp(llr))\n",
        "\n",
        "                    # If the number of active paths is less than the list size,\n",
        "                    # we can potentially branch and create a new path for the alternative decision.\n",
        "                    if len(active_path_indices) < self.list_size:\n",
        "                         # Create a new path by copying the current path's state\n",
        "                         new_path_idx = len(self.paths) # Index for the new path\n",
        "                         self.llrs.append(np.copy(self.llrs[path_idx]))\n",
        "                         self.hard_decisions.append(np.copy(self.hard_decisions[path_idx]))\n",
        "                         self.path_metrics.append(0.0) # Will update below\n",
        "\n",
        "                         # Assign one decision to the original path and the other to the new path.\n",
        "                         # It's common to assign the decision with the lower path metric to the original path\n",
        "                         # and the higher path metric to the new path, but the order doesn't fundamentally\n",
        "                         # change the set of paths considered, just which index they get.\n",
        "                         # Let's assign decision 0 to the original path and decision 1 to the new path for simplicity here.\n",
        "                         hard_decision_orig = 0\n",
        "                         hard_decision_new = 1\n",
        "\n",
        "                         self.hard_decisions[path_idx][bit_index] = hard_decision_orig\n",
        "                         self.path_metrics[path_idx] = path_metric_0\n",
        "\n",
        "                         self.hard_decisions[new_path_idx][bit_index] = hard_decision_new\n",
        "                         self.path_metrics[new_path_idx] = path_metric_1\n",
        "\n",
        "                         # Add both paths to the list of next active paths\n",
        "                         next_active_path_indices.extend([path_idx, new_path_idx])\n",
        "                    else:\n",
        "                         # If we are already at the list size limit, we must choose only one decision\n",
        "                         # for the current path. We choose the decision with the smaller path metric.\n",
        "                         if path_metric_0 <= path_metric_1:\n",
        "                             hard_decision = 0\n",
        "                             self.path_metrics[path_idx] = path_metric_0\n",
        "                         else:\n",
        "                             hard_decision = 1\n",
        "                             self.path_metrics[path_idx] = path_metric_1\n",
        "\n",
        "                         self.hard_decisions[path_idx][bit_index] = hard_decision\n",
        "                         next_active_path_indices.append(path_idx)\n",
        "\n",
        "            # After processing all active paths for the current bit, prune the list\n",
        "            # if the number of potential next paths exceeds the list size.\n",
        "            if len(next_active_path_indices) > self.list_size:\n",
        "                 # Sort paths by their metric and keep only the best 'list_size' paths\n",
        "                 sorted_indices = sorted(next_active_path_indices, key=lambda i: self.path_metrics[i])\n",
        "                 return sorted_indices[:self.list_size]\n",
        "            else:\n",
        "                return next_active_path_indices\n",
        "\n",
        "\n",
        "        # Recursive Step: Process a block\n",
        "        else:\n",
        "            half_size = block_size // 2\n",
        "            # Split step - compute LLRs for the first half (u_1) for each active path\n",
        "            for path_idx in active_path_indices:\n",
        "                llr_f = self._f(self.llrs[path_idx][bit_index:bit_index + half_size], self.llrs[path_idx][bit_index + half_size:bit_index + block_size])\n",
        "                self.llrs[path_idx][bit_index:bit_index + half_size] = llr_f\n",
        "\n",
        "            # Recursively decode the first half for the current active paths\n",
        "            active_paths_after_u1 = self._recursive_decode(active_path_indices, bit_index, half_size)\n",
        "\n",
        "            # Combine step - prepare information for the second half (u_2) for the remaining active paths\n",
        "            # Need decisions from the first half (u_1) for the G operation\n",
        "            next_active_path_indices = []\n",
        "            for path_idx in active_paths_after_u1:\n",
        "                u1_decisions = self.hard_decisions[path_idx][bit_index:bit_index + half_size]\n",
        "                llr_g = self._g(self.llrs[path_idx][bit_index:bit_index + half_size], self.llrs[path_idx][bit_index + half_size:bit_index + block_size], u1_decisions)\n",
        "                self.llrs[path_idx][bit_index + half_size:bit_index + block_size] = llr_g\n",
        "                next_active_path_indices.append(path_idx) # These paths remain active for the next recursive call\n",
        "\n",
        "            # Recursively decode the second half for the remaining active paths\n",
        "            active_paths_after_u2 = self._recursive_decode(next_active_path_indices, bit_index + half_size, half_size)\n",
        "\n",
        "            # Pruning after finishing a sub-block (this is a common point for pruning in SCL)\n",
        "            if len(active_paths_after_u2) > self.list_size:\n",
        "                sorted_indices = sorted(active_paths_after_u2, key=lambda i: self.path_metrics[i])\n",
        "                return sorted_indices[:self.list_size]\n",
        "            else:\n",
        "                return active_paths_after_u2\n",
        "\n",
        "\n",
        "    def _f(self, L1, L2):\n",
        "        # More accurate LLR combining for F operation\n",
        "        # Formula: sign(L1)*sign(L2)*min(|L1|,|L2|) + log(1+exp(-|L1+L2|)) - log(1+exp(-|L1-L2|))\n",
        "        # Handle potential overflow/underflow for large LLRs\n",
        "        # A numerically stable approximation often used:\n",
        "        # return np.sign(L1) * np.sign(L2) * np.minimum(np.abs(L1), np.abs(L2))\n",
        "        # Let's use the more standard approximation that is numerically stable:\n",
        "        # min(|L1|, |L2|) with the sign of L1*L2, adding a correction term.\n",
        "        # This is also sometimes approximated as sign(L1*L2) * min(|L1|, |L2|) for simplicity\n",
        "        # when exact LLRs aren't critical.\n",
        "        # A more robust approximation:\n",
        "        return np.sign(L1) * np.sign(L2) * np.minimum(np.abs(L1), np.abs(L2))\n",
        "\n",
        "    def _g(self, L1, L2, u1):\n",
        "        # LLR combining for G operation\n",
        "        # Formula: LLR(u2) = L2 + (1 - 2*u1) * L1\n",
        "        return L2 + (1 - 2 * u1) * L1\n",
        "\n",
        "    def _check_crc(self, bits):\n",
        "         \"\"\"\n",
        "         Checks the CRC for the given bits (assumed to be info bits + CRC).\n",
        "         Uses the centralized compute_crc function.\n",
        "         \"\"\"\n",
        "         if self._crc_polynomial is None or self._crc_length == 0:\n",
        "             return True # No CRC to check or CRC length is 0\n",
        "\n",
        "         # Ensure the input bits have the expected length (K + crc_length)\n",
        "         if len(bits) != self.K_crc:\n",
        "             # logging.warning(f\"_check_crc: Input bits length ({len(bits)}) does not match expected K_crc ({self.K_crc}). CRC check skipped.\") # Optional logging\n",
        "             return False # Cannot check CRC if the length is wrong\n",
        "\n",
        "         # Extract data and received CRC bits\n",
        "         data_bits = bits[:self.K]\n",
        "         received_crc = bits[self.K:]\n",
        "\n",
        "         # Compute CRC for the data bits\n",
        "         computed_crc = compute_crc(data_bits, self._crc_polynomial)\n",
        "\n",
        "         # Check if computed CRC matches the received CRC\n",
        "         return np.array_equal(computed_crc, received_crc)\n",
        "\n",
        "####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Part 6: Plotting Functions\n",
        "\n",
        "def plot_training_validation(train_losses, val_losses):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    if val_losses:\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_ber_bler_comparison(snr_range, rnn_results, scl_results_all, list_sizes):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BER_RNN'], label='RNN')\n",
        "    for size, scl_results in scl_results_all.items():\n",
        "        plt.plot(snr_range, [result['BER'] for result in scl_results], label=f'SCL, List Size {size}')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('Bit Error Rate (BER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.plot(snr_range, rnn_results['BLER_RNN'], label='RNN')\n",
        "    for size, scl_results in scl_results_all.items():\n",
        "        plt.plot(snr_range, [result['BLER'] for result in scl_results], label=f'SCL, List Size {size}')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('Block Error Rate (BLER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "def plot_ber_bler_scl(snr_range, scl_results_all, list_sizes):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # BER Plot for SCL\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)  # Set y-limits\n",
        "    for size in list_sizes:\n",
        "        if size not in scl_results_all or not scl_results_all[size]:\n",
        "            continue\n",
        "        plt.plot(snr_range, [result['BER'] for result in scl_results_all[size]], label=f'SCL, List Size {size}', marker='x', linestyle='--')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('SCL Bit Error Rate (BER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # BLER Plot for SCL\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)  # Set y-limits\n",
        "    for size in list_sizes:\n",
        "        if size not in scl_results_all or not scl_results_all[size]:\n",
        "            continue\n",
        "        plt.plot(snr_range, [result['BLER'] for result in scl_results_all[size]], label=f'SCL, List Size {size}', marker='x', linestyle='--')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('SCL Block Error Rate (BLER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    #for RNN decoder plots\n",
        "def plot_ber_bler_rnn(snr_range, rnn_results):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # BER Plot for RNN\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)  # Set y-limits\n",
        "    plt.plot(snr_range, rnn_results['BER_RNN'], label='RNN', marker='o', linestyle='-')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('RNN Bit Error Rate (BER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    # BLER Plot for RNN\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)  # Set y-limits\n",
        "    plt.plot(snr_range, rnn_results['BLER_RNN'], label='RNN', marker='o', linestyle='-')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('RNN Block Error Rate (BLER)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Part 6 continued: Performance Evaluation Function\n",
        "\n",
        "#Latest performance compariso from ChatGPT 06/09/25 evening\n",
        "\n",
        "def performance_comparison(\n",
        "    rnn_trainer, polar_code_gen, snr_range_db, channel_type, list_sizes, num_trials, device, info_bits\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate performance (BER and BLER) of RNN and SCL decoders over a range of SNR values.\n",
        "\n",
        "    Args:\n",
        "        rnn_trainer: Trained RNN decoder trainer object.\n",
        "        polar_code_gen: Polar code generator object.\n",
        "        snr_range_db: Iterable of SNR values in dB.\n",
        "        channel_type: Channel model type string (e.g., 'AWGN').\n",
        "        list_sizes: List of integers for SCL decoder list sizes.\n",
        "        num_trials: Number of random trials per SNR.\n",
        "        device: PyTorch device (cpu or cuda).\n",
        "        info_bits: Number of info bits in code.\n",
        "\n",
        "    Returns:\n",
        "        rnn_perf_results: dict with 'BER_RNN' and 'BLER_RNN' lists.\n",
        "        scl_perf_results: dict keyed by list size with list of dicts {'BER', 'BLER'}.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting performance comparison...\")\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "    rnn_perf_results = {'BER_RNN': [], 'BLER_RNN': []}\n",
        "    scl_perf_results = {size: [] for size in list_sizes}\n",
        "\n",
        "    rnn_model = rnn_trainer.model.to(device)\n",
        "    rnn_model.eval()\n",
        "\n",
        "    # Initialize one PolarCodeDecoder per list size outside trials loop (assuming stateless)\n",
        "    scl_decoders = {\n",
        "        size: PolarCodeDecoder(\n",
        "            N=polar_code_gen.N,\n",
        "            K=polar_code_gen.K,\n",
        "            list_size=size,\n",
        "            crc_poly=polar_code_gen.crc_polynomials.get(polar_code_gen.crc_type)\n",
        "        ) for size in list_sizes\n",
        "    }\n",
        "\n",
        "    for snr_db in snr_range_db:\n",
        "        logging.info(f\"Simulating at SNR: {snr_db} dB\")\n",
        "\n",
        "        # Initialize counters\n",
        "        total_bits_rnn = 0\n",
        "        bit_errors_rnn = 0\n",
        "        total_blocks_rnn = 0\n",
        "        block_errors_rnn = 0\n",
        "\n",
        "        total_bits_scl = {size: 0 for size in list_sizes}\n",
        "        bit_errors_scl = {size: 0 for size in list_sizes}\n",
        "        total_blocks_scl = {size: 0 for size in list_sizes}\n",
        "        block_errors_scl = {size: 0 for size in list_sizes}\n",
        "\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        noise_variance = 1 / (2 * snr_linear)  # Assumes unit signal power for BPSK\n",
        "\n",
        "        for trial in range(num_trials):\n",
        "            # Generate and encode bits\n",
        "            info_bits_array = polar_code_gen.generate_info_bits()\n",
        "            encoded_signal = polar_code_gen.polar_encode(info_bits_array)\n",
        "            modulated_signal = bpsk_modulate(encoded_signal)\n",
        "            received_signal = channel_simulator.simulate(modulated_signal, snr_db)\n",
        "\n",
        "            # --- RNN Decoding ---\n",
        "            with torch.no_grad():\n",
        "                received_tensor = torch.FloatTensor(received_signal).view(1, -1).to(device)\n",
        "                rnn_output_prob = rnn_model(received_tensor).cpu().numpy().squeeze()\n",
        "                rnn_decoded_bits = (rnn_output_prob > 0.5).astype(int)\n",
        "\n",
        "            # Update RNN stats\n",
        "            total_bits_rnn += info_bits\n",
        "            bit_errors_rnn += np.sum(rnn_decoded_bits != info_bits_array)\n",
        "            total_blocks_rnn += 1\n",
        "            if not np.array_equal(rnn_decoded_bits, info_bits_array):\n",
        "                block_errors_rnn += 1\n",
        "\n",
        "            # --- SCL Decoding ---\n",
        "            received_llrs = (2 * received_signal) / noise_variance\n",
        "\n",
        "            for size in list_sizes:\n",
        "                scl_decoder = scl_decoders[size]\n",
        "                scl_decoded_bits = scl_decoder.decode(received_llrs)\n",
        "\n",
        "                total_bits_scl[size] += info_bits\n",
        "                bit_errors_scl[size] += np.sum(scl_decoded_bits != info_bits_array)\n",
        "                total_blocks_scl[size] += 1\n",
        "                if not np.array_equal(scl_decoded_bits, info_bits_array):\n",
        "                    block_errors_scl[size] += 1\n",
        "\n",
        "        # Calculate RNN BER and BLER\n",
        "        rnn_ber = bit_errors_rnn / total_bits_rnn if total_bits_rnn > 0 else 0\n",
        "        rnn_bler = block_errors_rnn / total_blocks_rnn if total_blocks_rnn > 0 else 0\n",
        "        rnn_perf_results['BER_RNN'].append(rnn_ber)\n",
        "        rnn_perf_results['BLER_RNN'].append(rnn_bler)\n",
        "        logging.info(f\"SNR: {snr_db} dB, RNN BER: {rnn_ber:.4e}, BLER: {rnn_bler:.4e}\")\n",
        "\n",
        "        # Calculate SCL BER and BLER for each list size\n",
        "        for size in list_sizes:\n",
        "            scl_ber = bit_errors_scl[size] / total_bits_scl[size] if total_bits_scl[size] > 0 else 0\n",
        "            scl_bler = block_errors_scl[size] / total_blocks_scl[size] if total_blocks_scl[size] > 0 else 0\n",
        "            scl_perf_results[size].append({'BER': scl_ber, 'BLER': scl_bler})\n",
        "            logging.info(f\"SNR: {snr_db} dB, SCL List Size {size} BER: {scl_ber:.4e}, BLER: {scl_bler:.4e}\")\n",
        "\n",
        "    return rnn_perf_results, scl_perf_results\n",
        "\n",
        "\n",
        "\n",
        "#Part 7\n",
        "#main() function\n",
        "\n",
        "\n",
        "\n",
        " #####################################################################\n",
        "\n",
        " #latest main() from ChatGPT 06/09/25 evening\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main workflow for training RNN decoder and comparing with SCL decoder.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        logging.info(f\"Using device: {device}\")\n",
        "\n",
        "        # Constants\n",
        "        BLOCK_LENGTH = 128\n",
        "        INFO_BITS = 64\n",
        "        LEARNING_RATE = 1e-3\n",
        "        EPOCHS = 50\n",
        "        BATCH_SIZE = 32\n",
        "        NUM_SAMPLES_TRAIN = 50000\n",
        "        NUM_TRIALS_PERF = 1500\n",
        "        SNR_RANGE_AWGN = np.linspace(0, 5, 11)\n",
        "        LIST_SIZES = [1, 8, 16]\n",
        "\n",
        "        # Initialize Polar code generator and RNN model\n",
        "        polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "        rnn_model = EnhancedRNNDecoder(BLOCK_LENGTH, INFO_BITS).to(device)\n",
        "        rnn_trainer = DecoderTrainer(rnn_model, LEARNING_RATE)\n",
        "\n",
        "        logging.info(f\"Polar code rate: {polar_code_gen.R}\")\n",
        "\n",
        "        # Data structures to store BER and BLER results\n",
        "        sc_ber_results = []\n",
        "        sc_bler_results = []\n",
        "# Use a dictionary to store results for each list size\n",
        "        scl_ber_results = {ls: [] for ls in LIST_SIZES}\n",
        "        scl_bler_results = {ls: [] for ls in LIST_SIZES}\n",
        "\n",
        "# Define the CRC polynomial and length (assuming CRC-7 is used)\n",
        "# Make sure this matches what's used in PolarCodeGenerator and PolarCodeDecoder\n",
        "        crc_polynomial = np.array([1, 0, 0, 0, 1, 0, 0, 1], dtype=int)\n",
        "        crc_length = 7\n",
        "        crc_poly_tuple = (crc_polynomial, crc_length) # Tuple format for PolarCodeDecoder\n",
        "\n",
        "for snr_db in SNR_RANGE_AWGN:\n",
        "    logging.info(f\"Simulating at SNR = {snr_db:.2f} dB\")\n",
        "\n",
        "    # Instantiate the generator for the current SNR.\n",
        "    # While reliability order for N=128 AWGN is fixed,\n",
        "    # the generator still needs the SNR to be initialized.\n",
        "    polar_code_gen = PolarCodeGenerator(BLOCK_LENGTH, INFO_BITS, snr_db, crc_type='CRC-7')\n",
        "\n",
        "    # Get the frozen and info sets from the generator.\n",
        "    # These sets are determined during generator initialization based on the channel model/reliability sequence.\n",
        "    frozen_set = polar_code_gen.frozen_set\n",
        "    info_set = polar_code_gen.info_set\n",
        "\n",
        "    # Instantiate the SC Decoder.\n",
        "    # It uses the info and frozen sets to know which bits to decode.\n",
        "    sc_decoder = SCDecoder(BLOCK_LENGTH, INFO_BITS, info_set, frozen_set)\n",
        "\n",
        "    # Instantiate the SCL Decoders for each list size.\n",
        "    # They also need the info/frozen sets (derived internally using the same logic as the generator)\n",
        "    # and the CRC polynomial for checking.\n",
        "    scl_decoders = {ls: PolarCodeDecoder(BLOCK_LENGTH, INFO_BITS, ls, crc_poly=crc_poly_tuple) for ls in LIST_SIZES}\n",
        "\n",
        "\n",
        "    total_info_bits = 0 # Total number of information bits transmitted\n",
        "    total_block_errors_sc = 0 # Number of blocks where SC decoder made at least one error\n",
        "    total_bit_errors_sc = 0 # Total number of bit errors by SC decoder\n",
        "\n",
        "    total_block_errors_scl = {ls: 0 for ls in LIST_SIZES} # Block errors for each SCL list size\n",
        "    total_bit_errors_scl = {ls: 0 for ls in LIST_SIZES} # Bit errors for each SCL list size\n",
        "\n",
        "    total_simulations = 0 # Total number of blocks simulated at this SNR\n",
        "\n",
        "    # Run simulation trials for performance evaluation\n",
        "    for trial in range(NUM_TRIALS_PERF):\n",
        "        # Generate random information bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Polar encode the information bits (includes CRC if configured)\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # BPSK modulate the encoded signal\n",
        "        modulated_signal = bpsk_modulate(encoded_signal)\n",
        "\n",
        "        # Simulate the AWGN channel\n",
        "        channel_simulator = EnhancedChannelSimulator(channel_type='AWGN')\n",
        "        received_signal = channel_simulator.simulate(modulated_signal, snr_db)\n",
        "\n",
        "        # Convert received signal to LLRs (Log-Likelihood Ratios)\n",
        "        # For BPSK over AWGN, LLR = (2 * received_signal) / noise_variance\n",
        "        snr_linear = 10**(snr_db/10)\n",
        "        # Assuming Es = 1 (symbol energy)\n",
        "        noise_var = 1 / (2 * snr_linear)\n",
        "        # Handle potential division by zero if snr_linear is very close to 0 (e.g., -inf dB)\n",
        "        if noise_var > 0:\n",
        "            received_llrs = (2 * received_signal) / noise_var\n",
        "        else:\n",
        "            # For very low/negative SNR, LLRs can be effectively 0 (pure noise)\n",
        "            received_llrs = np.zeros_like(received_signal)\n",
        "\n",
        "\n",
        "        # --- Decode with SC ---\n",
        "        try:\n",
        "            decoded_info_sc = sc_decoder.decode(received_llrs)\n",
        "            if not np.array_equal(decoded_info_sc, info_bits):\n",
        "                total_block_errors_sc += 1\n",
        "                total_bit_errors_sc += np.sum(decoded_info_sc != info_bits)\n",
        "        except Exception as e:\n",
        "             logging.error(f\"SC Decoder Error during trial {trial} at SNR {snr_db:.2f} dB: {e}\")\n",
        "             traceback.print_exc() # Print traceback for debugging\n",
        "             total_block_errors_sc += 1 # Count as a block error if decoding fails\n",
        "             total_bit_errors_sc += INFO_BITS # Assume all bits wrong in case of decoding failure\n",
        "\n",
        "        total_info_bits += INFO_BITS # Add INFO_BITS for each trial\n",
        "\n",
        "\n",
        "        # --- Decode with SCL for each list size ---\n",
        "        for ls in LIST_SIZES:\n",
        "            try:\n",
        "                decoded_info_scl = scl_decoders[ls].decode(received_llrs)\n",
        "                if not np.array_equal(decoded_info_scl, info_bits):\n",
        "                    total_block_errors_scl[ls] += 1\n",
        "                    total_bit_errors_scl[ls] += np.sum(decoded_info_scl != info_bits)\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"SCL Decoder (L={ls}) Error during trial {trial} at SNR {snr_db:.2f} dB: {e}\")\n",
        "                 traceback.print_exc() # Print traceback for debugging\n",
        "                 total_block_errors_scl[ls] += 1 # Count as a block error if decoding fails\n",
        "                 total_bit_errors_scl[ls] += INFO_BITS # Assume all bits wrong\n",
        "\n",
        "        total_simulations += 1\n",
        "\n",
        "    # Calculate BER and BLER for SC at this SNR\n",
        "        sc_ber = total_bit_errors_sc / total_info_bits if total_info_bits > 0 else 0\n",
        "        sc_bler = total_block_errors_sc / total_simulations if total_simulations > 0 else 0\n",
        "        sc_ber_results.append(sc_ber)\n",
        "        sc_bler_results.append(sc_bler)\n",
        "\n",
        "        logging.info(f\"  SC Decoder: BER = {sc_ber:.6f}, BLER = {sc_bler:.6f} (based on {total_simulations} trials)\")\n",
        "\n",
        "\n",
        "    # Calculate BER and BLER for SCL for each list size at this SNR\n",
        "    for ls in LIST_SIZES:\n",
        "        scl_ber = total_bit_errors_scl[ls] / total_info_bits if total_info_bits > 0 else 0\n",
        "        scl_bler = total_block_errors_scl[ls] / total_simulations if total_simulations > 0 else 0\n",
        "        scl_ber_results[ls].append(scl_ber)\n",
        "        scl_bler_results[ls].append(scl_bler)\n",
        "\n",
        "        logging.info(f\"  SCL Decoder (L={ls}): BER = {scl_ber:.6f}, BLER = {scl_bler:.6f} (based on {total_simulations} trials)\")\n",
        "\n",
        "\n",
        "         logging.info(\"Performance evaluation finished.\")\n",
        "\n",
        "# --- Plotting ---\n",
        "# You will need plotting code here, using the collected results:\n",
        "# sc_ber_results, sc_bler_results, scl_ber_results, scl_bler_results\n",
        "# Example (assuming matplotlib is imported as plt):\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.semilogy(SNR_RANGE_AWGN, sc_ber_results, marker='o', linestyle='-', label='SC Decoder')\n",
        "        for ls in LIST_SIZES:\n",
        "            plt.semilogy(SNR_RANGE_AWGN, scl_ber_results[ls], marker='x', linestyle='--', label=f'SCL Decoder (L={ls})')\n",
        "\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Bit Error Rate (BER)')\n",
        "        plt.title('BER Performance Comparison (Polar Code)')\n",
        "        plt.grid(True, which=\"both\", linestyle='--')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.semilogy(SNR_RANGE_AWGN, sc_bler_results, marker='o', linestyle='-', label='SC Decoder')\n",
        "for ls in LIST_SIZES:\n",
        "    plt.semilogy(SNR_RANGE_AWGN, scl_bler_results[ls], marker='x', linestyle='--', label=f'SCL Decoder (L={ls})')\n",
        "\n",
        "plt.xlabel('SNR (dB)')\n",
        "plt.ylabel('Block Error Rate (BLER)')\n",
        "plt.title('BLER Performance Comparison (Polar Code)')\n",
        "plt.grid(True, which=\"both\", linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Note: RNN decoder evaluation would require loading the trained model\n",
        "# and running it on test data for each SNR, then plotting its results as well.\n",
        "# This rewrite focuses on SC and SCL.\n",
        "\n",
        "\n",
        "        # Dataset generation & saving\n",
        "        X_raw, y_raw = prepare_polar_dataset(\n",
        "            polar_code_gen, num_samples=NUM_SAMPLES_TRAIN, snr_db=5.0, channel_type='AWGN'\n",
        "        )\n",
        "        save_dataset_to_csv(X_raw, y_raw, 'awgn_dataset.csv')\n",
        "\n",
        "        # Prepare tensors and split into train/val sets\n",
        "        X_tensor = torch.FloatTensor(X_raw).view(-1, BLOCK_LENGTH).to(device)\n",
        "        y_tensor = torch.FloatTensor(y_raw).view(-1, INFO_BITS).to(device)\n",
        "        train_size = int(0.8 * X_tensor.shape[0])\n",
        "        train_X, train_y = X_tensor[:train_size], y_tensor[:train_size]\n",
        "        val_X, val_y = X_tensor[train_size:], y_tensor[train_size:]\n",
        "\n",
        "        # Train RNN model\n",
        "        train_losses, val_losses = rnn_trainer.train(\n",
        "            train_X, train_y, X_val=val_X, y_val=val_y, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # Evaluate performance\n",
        "        rnn_perf_results, scl_perf_results = performance_comparison(\n",
        "            rnn_trainer, polar_code_gen, SNR_RANGE_AWGN, 'AWGN', LIST_SIZES, NUM_TRIALS_PERF, device, INFO_BITS\n",
        "        )\n",
        "\n",
        "        # Plot training and evaluation results\n",
        "        plot_training_validation(train_losses, val_losses)\n",
        "        plot_ber_bler_rnn(SNR_RANGE_AWGN, rnn_perf_results)\n",
        "        plot_ber_bler_comparison(SNR_RANGE_AWGN, rnn_perf_results, scl_perf_results, LIST_SIZES)\n",
        "        plot_ber_bler_scl(SNR_RANGE_AWGN, scl_perf_results, LIST_SIZES)\n",
        "\n",
        "        # Confusion matrix example for first 100 samples in training data\n",
        "        y_true_example = train_y[:100].cpu().numpy()\n",
        "        rnn_input_example = train_X[:100]\n",
        "        with torch.no_grad():\n",
        "            rnn_output_prob_example = rnn_trainer.model(rnn_input_example).cpu().numpy()\n",
        "        rnn_output_example = (rnn_output_prob_example > 0.5).astype(int)\n",
        "        plot_confusion_matrix(y_true_example.flatten(), rnn_output_example.flatten(), title='Confusion Matrix')\n",
        "\n",
        "        logging.info(\" AWGN Channel Simulation Complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Simulation Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "-3SdRVSn7PAW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-neJgdNazFT",
        "outputId": "ce1f7571-678f-4e01-a941-775a95aefd95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# polar_code_generator.py\n",
        "!pip install ipympl\n",
        "!pip install scikit-learn\n",
        "!pip install numpy torch matplotlib scikit-learn\n",
        "!pip install -U matplotlib\n",
        "import numpy as np\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib\n",
        "#import ipympl\n",
        "matplotlib.use('Agg')  # Or try 'TkAgg', 'Qt5Agg'\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Add this line\n",
        "import logging\n",
        "import traceback\n",
        "# Import custom modules\n",
        "#from polar_code_generator import PolarCodeGenerator\n",
        "#from channel_simulator import ChannelSimulator\n",
        "#from neural_decoder import NeuralDecoder\n",
        "#from rnn_trainer import RNNTrainer\n",
        "#from ml_trainer import MLTrainer\n",
        "#from dataset_preparation import (\n",
        " #   prepare_polar_dataset,\n",
        "  #  prepare_dataset_for_training,\n",
        " #   normalize_features\n",
        "#)\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N=128, K=64):\n",
        "        \"\"\"\n",
        "        Initialize Polar Code Generator with CRC-7 Polynomial\n",
        "\n",
        "        Args:\n",
        "            N (int): Total block length\n",
        "            K (int): Information bit length\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.R = K / N  # Code rate\n",
        "\n",
        "        # CRC-7 Polynomial (Standard polynomial for communication)\n",
        "        # x^7 + x^6 + x^5 + x^2 + x^0\n",
        "        self.crc_polynomial = 0b10100011  # CRC-7 polynomial\n",
        "        self.crc_order = 7  # 7-bit CRC\n",
        "\n",
        "    def crc_generate(self, data):\n",
        "        \"\"\"\n",
        "        Generate CRC-7 checksum\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Input data bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: CRC checksum bits\n",
        "        \"\"\"\n",
        "        # Convert input to numpy array\n",
        "        data = np.asarray(data)\n",
        "\n",
        "        # Create data with zero padding for CRC\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.crc_order, dtype=int)])\n",
        "\n",
        "        # CRC calculation\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.crc_order + 1):\n",
        "                    data_with_zeros[i+j] ^= ((self.crc_polynomial >> j) & 1)\n",
        "\n",
        "        # Return the last 'crc_order' bits as CRC\n",
        "        return data_with_zeros[-self.crc_order:]\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        \"\"\"\n",
        "        Generate random information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Random information bits\n",
        "        \"\"\"\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Systematic Polar Encoding with CRC\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits to encode\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Generate CRC\n",
        "        crc_bits = self.crc_generate(info_bits)\n",
        "\n",
        "        # Combine info bits and CRC\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Initialize codeword\n",
        "        encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Place information bits\n",
        "        encoded_bits[:len(full_info)] = full_info\n",
        "\n",
        "        return encoded_bits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def crc_verify(self, data, received_crc):\n",
        "        \"\"\"\n",
        "        Verify CRC-7 checksum\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): Original data bits\n",
        "            received_crc (np.ndarray): Received CRC checksum\n",
        "\n",
        "        Returns:\n",
        "            bool: True if CRC is valid, False otherwise\n",
        "        \"\"\"\n",
        "        # Combine data and received CRC\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "\n",
        "        # CRC verification\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.crc_order + 1):\n",
        "                    full_data[i+j] ^= ((self.crc_polynomial >> j) & 1)\n",
        "\n",
        "        # Check if the last 'crc_order' bits are zero\n",
        "        return np.all(full_data[-self.crc_order:] == 0)\n",
        "\n",
        "    def bhattacharyya_parameter(self, W, n):\n",
        "        \"\"\"\n",
        "        Compute Bhattacharyya parameter for channel polarization\n",
        "\n",
        "        Args:\n",
        "            W (float): Initial channel crossover probability\n",
        "            n (int): Recursion depth\n",
        "\n",
        "        Returns:\n",
        "            float: Bhattacharyya parameter\n",
        "        \"\"\"\n",
        "        if n == 0:\n",
        "            return W\n",
        "\n",
        "        # Recursive Bhattacharyya parameter computation\n",
        "        W_used = self.bhattacharyya_parameter(W, n-1)\n",
        "        W_transform = 2 * (W_used ** 2) - (W_used ** 4)\n",
        "\n",
        "        return W_transform\n",
        "\n",
        "def generate_polar_code_matrix(self):\n",
        "    \"\"\"\n",
        "    Generate polar code matrix using Bhattacharyya parameter\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Indices of information bit positions\n",
        "    \"\"\"\n",
        "    # Initial channel crossover probability (Binary Symmetric Channel)\n",
        "    W = 0.5\n",
        "\n",
        "    # Compute channel capacities\n",
        "    channel_capacities = []\n",
        "    for _ in range(self.N):\n",
        "        # Compute Bhattacharyya parameter\n",
        "        capacity = self.bhattacharyya_parameter(W, int(math.log2(self.N)))\n",
        "        channel_capacities.append(capacity)\n",
        "\n",
        "    # Sort channel capacities\n",
        "    sorted_indices = np.argsort(channel_capacities)\n",
        "\n",
        "    # Select best channels for information bits\n",
        "    info_indices = sorted_indices[self.N - self.K:]\n",
        "\n",
        "    return info_indices\n",
        "\n",
        "def polar_encode(self, info_bits):\n",
        "    \"\"\"\n",
        "    Systematic Polar Encoding with CRC\n",
        "\n",
        "    Args:\n",
        "        info_bits (np.ndarray): Information bits to encode\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Encoded codeword\n",
        "    \"\"\"\n",
        "    # Generate CRC\n",
        "    crc_bits = self.crc_generate(info_bits)\n",
        "\n",
        "    # Combine info bits and CRC\n",
        "    full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "    # Initialize codeword\n",
        "    encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "    # Get indices for information bits\n",
        "    info_indices = self.generate_polar_code_matrix()\n",
        "\n",
        "    # Ensure we don't exceed available indices\n",
        "    max_info_length = min(len(full_info), len(info_indices))\n",
        "\n",
        "    # Place information bits at selected indices\n",
        "    encoded_bits[info_indices[:max_info_length]] = full_info[:max_info_length]\n",
        "\n",
        "    return encoded_bits\n",
        "\n",
        "def bhattacharyya_parameter(self, W, n):\n",
        "    \"\"\"\n",
        "    Compute Bhattacharyya parameter for channel polarization\n",
        "\n",
        "    Args:\n",
        "        W (float): Initial channel crossover probability\n",
        "        n (int): Recursion depth\n",
        "\n",
        "    Returns:\n",
        "        float: Bhattacharyya parameter\n",
        "    \"\"\"\n",
        "    if n == 0:\n",
        "        return W\n",
        "\n",
        "    # Recursive Bhattacharyya parameter computation\n",
        "    W_used = self.bhattacharyya_parameter(W, n-1)\n",
        "    W_transform = 2 * (W_used ** 2) - (W_used ** 4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        \"\"\"\n",
        "        Generate random information bits\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Random information bits\n",
        "        \"\"\"\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Systematic Polar Encoding with CRC\n",
        "\n",
        "        Args:\n",
        "            info_bits (np.ndarray): Information bits to encode\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Encoded codeword\n",
        "        \"\"\"\n",
        "        # Generate CRC\n",
        "        crc_bits = self.crc_generate(info_bits)\n",
        "\n",
        "        # Combine info bits and CRC\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Initialize codeword\n",
        "        encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Get indices for information bits\n",
        "        info_indices = self.generate_polar_code_matrix()\n",
        "        info_indices = info_indices[:len(full_info)]\n",
        "\n",
        "        # Place information bits at selected indices\n",
        "        encoded_bits[info_indices] = full_info\n",
        "\n",
        "        return encoded_bits\n",
        "\n",
        "# Part 1: Channel Simulator\n",
        "class ChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Initialize Channel Simulator\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Type of channel (AWGN or Rayleigh)\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def transmit(self, signal, snr):\n",
        "        \"\"\"\n",
        "        Transmit signal through channel\n",
        "\n",
        "        Args:\n",
        "            signal (np.ndarray): Input signal\n",
        "            snr (float): Signal-to-Noise Ratio in dB\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received noisy signal\n",
        "        \"\"\"\n",
        "        # Convert SNR to linear scale\n",
        "        snr_linear = 10 ** (snr / 10)\n",
        "\n",
        "        # Noise standard deviation\n",
        "        noise_std = np.sqrt(1 / (2 * snr_linear))\n",
        "\n",
        "        # Generate noise\n",
        "        if self.channel_type == 'AWGN':\n",
        "            # Additive White Gaussian Noise\n",
        "            noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        elif self.channel_type == 'Rayleigh':\n",
        "            # Rayleigh Fading Channel\n",
        "            fading = np.random.rayleigh(scale=1, size=signal.shape)\n",
        "            noise = fading * np.random.normal(0, noise_std, signal.shape)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "        return signal + noise\n",
        "\n",
        "\n",
        "# channel_simulator.py\n",
        "\n",
        "\n",
        "class ChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        \"\"\"\n",
        "        Initialize Channel Simulator\n",
        "\n",
        "        Args:\n",
        "            channel_type (str): Type of channel (AWGN or Rayleigh)\n",
        "        \"\"\"\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def transmit(self, signal, snr):\n",
        "        \"\"\"\n",
        "        Transmit signal through channel\n",
        "\n",
        "        Args:\n",
        "            signal (np.ndarray): Input signal\n",
        "            snr (float): Signal-to-Noise Ratio in dB\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Received noisy signal\n",
        "        \"\"\"\n",
        "        # Convert SNR to linear scale\n",
        "        snr_linear = 10 ** (snr / 10)\n",
        "\n",
        "        # Noise standard deviation\n",
        "        noise_std = np.sqrt(1 / (2 * snr_linear))\n",
        "\n",
        "        # Generate noise\n",
        "        if self.channel_type == 'AWGN':\n",
        "            # Additive White Gaussian Noise\n",
        "            noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        elif self.channel_type == 'Rayleigh':\n",
        "            # Rayleigh Fading Channel\n",
        "            fading = np.random.rayleigh(scale=1, size=signal.shape)\n",
        "            noise = fading * np.random.normal(0, noise_std, signal.shape)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "        return signal + noise\n",
        "\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize Trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): Neural network model\n",
        "            learning_rate (float): Optimization learning rate\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20, batch_size=64):\n",
        "        \"\"\"\n",
        "        Train the model\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features\n",
        "            y_train (np.ndarray): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Training batch size\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Convert to torch tensors\n",
        "        X_train = torch.FloatTensor(X_train)\n",
        "        y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            outputs = self.model(X_train)\n",
        "            loss = self.criterion(outputs, y_train)\n",
        "\n",
        "            # Backward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        return train_losses\n",
        "\n",
        "    def plot_training_performance(self, train_losses, title='Training Performance'):\n",
        "        \"\"\"\n",
        "        Plot training performance\n",
        "\n",
        "        Args:\n",
        "            train_losses (list): Training losses\n",
        "            title (str): Plot title\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(train_losses)\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "class RNNTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize RNN Trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): RNN neural network model\n",
        "            learning_rate (float): Optimization learning rate\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20, batch_size=64, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the RNN model\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features\n",
        "            y_train (np.ndarray): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Training batch size\n",
        "            validation_split (float): Proportion of data for validation\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Convert to torch tensors and move to device\n",
        "        X_train = torch.FloatTensor(X_train).to(self.device)\n",
        "        y_train = torch.FloatTensor(y_train).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # Reshape input for RNN if needed\n",
        "        if X_train.dim() == 2:\n",
        "            X_train = X_train.unsqueeze(2)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X_train))\n",
        "        X_val, y_val = X_train[train_size:], y_train[train_size:]\n",
        "        X_train, y_train = X_train[:train_size], y_train[:train_size]\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_train_losses = []\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[i:i+batch_size]\n",
        "                batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_train_losses.append(loss.item())\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val)\n",
        "                val_loss = self.criterion(val_outputs, y_val)\n",
        "\n",
        "            # Record losses\n",
        "            avg_train_loss = np.mean(epoch_train_losses)\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def plot_training_performance(self, train_losses, val_losses=None, title='RNN Training Performance'):\n",
        "        \"\"\"\n",
        "        Plot RNN training performance\n",
        "\n",
        "        Args:\n",
        "            train_losses (list): Training losses\n",
        "            val_losses (list, optional): Validation losses\n",
        "            title (str): Plot title\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Training Loss Plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.title(f'{title} - Training Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Validation Loss Plot\n",
        "        if val_losses:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "            plt.title(f'{title} - Validation Loss')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features for prediction\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities\n",
        "        \"\"\"\n",
        "        # Ensure input is a torch tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X).to(self.device)\n",
        "\n",
        "        # Reshape input for RNN if needed\n",
        "        if X.dim() == 2:\n",
        "            X = X.unsqueeze(2)\n",
        "\n",
        "        # Prediction\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X)\n",
        "\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "class MLTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize ML (MLP) Trainer\n",
        "\n",
        "        Args:\n",
        "            model (nn.Module): Multi-Layer Perceptron model\n",
        "            learning_rate (float): Optimization learning rate\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=20, batch_size=64, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the ML model\n",
        "\n",
        "        Args:\n",
        "            X_train (np.ndarray): Training features\n",
        "            y_train (np.ndarray): Training labels\n",
        "            epochs (int): Number of training epochs\n",
        "            batch_size (int): Training batch size\n",
        "            validation_split (float): Proportion of data for validation\n",
        "\n",
        "        Returns:\n",
        "            tuple: Training and validation losses\n",
        "        \"\"\"\n",
        "        # Convert to torch tensors and move to device\n",
        "        X_train = torch.FloatTensor(X_train).to(self.device)\n",
        "        y_train = torch.FloatTensor(y_train).to(self.device).unsqueeze(1)\n",
        "\n",
        "        # Split into train and validation\n",
        "        train_size = int((1 - validation_split) * len(X_train))\n",
        "        X_val, y_val = X_train[train_size:], y_train[train_size:]\n",
        "        X_train, y_train = X_train[:train_size], y_train[:train_size]\n",
        "\n",
        "        # Training loop\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            self.model.train()\n",
        "            epoch_train_losses = []\n",
        "\n",
        "            for i in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[i:i+batch_size]\n",
        "                batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass and optimize\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_train_losses.append(loss.item())\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val)\n",
        "                val_loss = self.criterion(val_outputs, y_val)\n",
        "\n",
        "            # Record losses\n",
        "            avg_train_loss = np.mean(epoch_train_losses)\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        return train_losses, val_losses\n",
        "\n",
        "    def plot_training_performance(self, train_losses, val_losses=None, title='ML Training Performance'):\n",
        "        \"\"\"\n",
        "        Plot ML training performance\n",
        "\n",
        "        Args:\n",
        "            train_losses (list): Training losses\n",
        "            val_losses (list, optional): Validation losses\n",
        "            title (str): Plot title\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Training Loss Plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.title(f'{title} - Training Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Validation Loss Plot\n",
        "        if val_losses:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "            plt.title(f'{title} - Validation Loss')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features for prediction\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities\n",
        "        \"\"\"\n",
        "        # Ensure input is a torch tensor\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X).to(self.device)\n",
        "\n",
        "        # Prediction\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = self.model(X)\n",
        "\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, feature_type='codeword'):\n",
        "    \"\"\"\n",
        "    Advanced dataset preparation for Polar Codes\n",
        "\n",
        "    Args:\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "        num_samples (int): Number of samples to generate\n",
        "        feature_type (str): Type of feature extraction\n",
        "\n",
        "    Returns:\n",
        "        tuple: Features and labels\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate info bits\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "        # Encode\n",
        "        codeword = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "        # Feature extraction\n",
        "        if feature_type == 'codeword':\n",
        "            # Use full codeword as features\n",
        "            features = codeword\n",
        "        elif feature_type == 'statistical':\n",
        "            # Statistical features\n",
        "            features = [\n",
        "                np.mean(codeword),\n",
        "                np.std(codeword),\n",
        "                np.sum(codeword),\n",
        "                np.count_nonzero(codeword)\n",
        "            ]\n",
        "        elif feature_type == 'frequency':\n",
        "            # Frequency-based features\n",
        "            unique, counts = np.unique(codeword, return_counts=True)\n",
        "            features = dict(zip(unique, counts))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported feature type: {feature_type}\")\n",
        "\n",
        "        X.append(features)\n",
        "\n",
        "        # Binary classification label (e.g., based on mean)\n",
        "        y.append(1 if np.mean(codeword) > 0.5 else 0)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_dataset_for_training(X, y, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Prepare dataset for neural network training\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Features\n",
        "        y (np.ndarray): Labels\n",
        "        test_size (float): Proportion of test data\n",
        "        random_state (int): Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        tuple: Train and test splits\n",
        "    \"\"\"\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def normalize_features(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Normalize features using min-max scaling\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Training features\n",
        "        X_test (np.ndarray): Test features\n",
        "\n",
        "    Returns:\n",
        "        tuple: Normalized training and test features\n",
        "    \"\"\"\n",
        "    # Compute min and max for each feature\n",
        "    min_vals = np.min(X_train, axis=0)\n",
        "    max_vals = np.max(X_train, axis=0)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    max_vals[max_vals == min_vals] = 1\n",
        "\n",
        "    # Normalize\n",
        "    X_train_normalized = (X_train - min_vals) / (max_vals - min_vals)\n",
        "    X_test_normalized = (X_test - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "    return X_train_normalized, X_test_normalized\n",
        "\n",
        "def create_rnn_model(input_size):\n",
        "    \"\"\"\n",
        "    Create RNN Model\n",
        "\n",
        "    Args:\n",
        "        input_size (int): Input feature dimension\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: RNN model\n",
        "    \"\"\"\n",
        "    class RNNDecoder(nn.Module):\n",
        "        def __init__(self, input_size):\n",
        "            super(RNNDecoder, self).__init__()\n",
        "\n",
        "            self.rnn = nn.Sequential(\n",
        "                nn.LSTM(\n",
        "                    input_size=input_size,\n",
        "                    hidden_size=64,\n",
        "                    num_layers=2,\n",
        "                    batch_first=True\n",
        "                ),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.3)\n",
        "            )\n",
        "\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Ensure input is 3D for RNN\n",
        "            if x.dim() == 2:\n",
        "                x = x.unsqueeze(2)\n",
        "\n",
        "            # RNN processing\n",
        "            rnn_out, _ = self.rnn(x)\n",
        "\n",
        "            # Take the last time step\n",
        "            out = rnn_out[:, -1, :]\n",
        "\n",
        "            # Final classification\n",
        "            return self.fc(out)\n",
        "\n",
        "    return RNNDecoder(input_size)\n",
        "def compute_channel_performance(model, channel, snr_range, polar_code_gen):\n",
        "    \"\"\"\n",
        "    Compute Bit Error Rate (BER) and Block Error Rate (BLER)\n",
        "\n",
        "    Args:\n",
        "        model: Trained decoder model\n",
        "        channel: Channel simulator\n",
        "        snr_range (np.ndarray): SNR range\n",
        "        polar_code_gen (PolarCodeGenerator): Polar code generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: BER and BLER arrays\n",
        "    \"\"\"\n",
        "    ber_values = []\n",
        "    bler_values = []\n",
        "\n",
        "    for snr in snr_range:\n",
        "        block_errors = 0\n",
        "        bit_errors = 0\n",
        "        total_blocks = 100\n",
        "\n",
        "        for _ in range(total_blocks):\n",
        "            # Generate info bits\n",
        "            info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "            # Encode\n",
        "            encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "            # Transmit through channel\n",
        "            received_signal = channel.transmit(encoded_signal, snr)\n",
        "\n",
        "            # Decode\n",
        "            decoded_bits = model.predict(received_signal.reshape(1, -1))\n",
        "            decoded_bits = (decoded_bits > 0.5).astype(int).flatten()\n",
        "\n",
        "            # Compute errors\n",
        "            block_error = not np.array_equal(info_bits, decoded_bits)\n",
        "            bit_error = np.sum(info_bits != decoded_bits)\n",
        "\n",
        "            block_errors += block_error\n",
        "            bit_errors += bit_error\n",
        "\n",
        "        # Compute BER and BLER\n",
        "        ber = bit_errors / (total_blocks * len(info_bits))\n",
        "        bler = block_errors / total_blocks\n",
        "\n",
        "        ber_values.append(ber)\n",
        "        bler_values.append(bler)\n",
        "\n",
        "    return np.array(ber_values), np.array(bler_values)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True labels\n",
        "        y_pred (array-like): Predicted labels\n",
        "        title (str): Plot title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{title.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_error_performance(snr_range, ber_data, bler_data):\n",
        "    \"\"\"\n",
        "    Plot BER and BLER performance\n",
        "\n",
        "    Args:\n",
        "        snr_range (np.ndarray): SNR range\n",
        "        ber_data (dict): Bit Error Rate data\n",
        "        bler_data (dict): Block Error Rate data\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # BER Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for model, ber in ber_data.items():\n",
        "        plt.semilogy(snr_range, ber, label=f'{model} BER')\n",
        "    plt.title('Bit Error Rate Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # BLER Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for model, bler in bler_data.items():\n",
        "        plt.semilogy(snr_range, bler, label=f'{model} BLER')\n",
        "    plt.title('Block Error Rate Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('error_performance.png')\n",
        "    plt.close()\n",
        "\n",
        "# The main function\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Comprehensive Polar Code Simulation and Machine Learning Decoder Evaluation\n",
        "    \"\"\"\n",
        "    # Simulation Parameters\n",
        "    SIMULATION_PARAMS = {\n",
        "        # Polar Code Parameters\n",
        "        'BLOCK_LENGTH': 128,      # Total block length (N)\n",
        "        'INFO_BITS': 64,          # Information bit length (K)\n",
        "\n",
        "        # Training Parameters\n",
        "        'LEARNING_RATE': 1e-3,    # Optimization learning rate\n",
        "        'EPOCHS': 10,            # Number of training epochs\n",
        "        'BATCH_SIZE': 64,         # Training batch size\n",
        "\n",
        "        # Dataset Parameters\n",
        "        'NUM_SAMPLES': 600,     # Total number of samples\n",
        "        'TEST_SPLIT': 0.2,        # Proportion of test data\n",
        "\n",
        "        # Channel Parameters\n",
        "        'SNR_RANGE': np.linspace(0, 10, 10),  # Signal-to-Noise Ratio range\n",
        "        'CHANNEL_TYPES': ['AWGN', 'Rayleigh'],  # Channel types\n",
        "\n",
        "        # Decoder Configurations\n",
        "        'LIST_SIZES': [1, 8, 16],  # List sizes for decoding\n",
        "\n",
        "        # Model Architectures\n",
        "        'RNN_CONFIG': {\n",
        "            'hidden_size': 64,\n",
        "            'num_layers': 2\n",
        "        },\n",
        "        'ML_CONFIG': {\n",
        "            'hidden_layers': [128, 256, 128]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Device Configuration\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\" Using Device: {device}\")\n",
        "\n",
        "        # 1. Polar Code Generator\n",
        "        polar_code_gen = PolarCodeGenerator(\n",
        "            N=SIMULATION_PARAMS['BLOCK_LENGTH'],\n",
        "            K=SIMULATION_PARAMS['INFO_BITS']\n",
        "        )\n",
        "        print(\" Polar Code Generator Initialized\")\n",
        "\n",
        "        # 2. Dataset Preparation\n",
        "        def prepare_dataset(polar_code_gen, num_samples):\n",
        "            \"\"\"\n",
        "            Prepare dataset for training\n",
        "            \"\"\"\n",
        "            X, y = [], []\n",
        "\n",
        "            for _ in range(num_samples):\n",
        "                # Generate info bits\n",
        "                info_bits = polar_code_gen.generate_info_bits()\n",
        "\n",
        "                # Encode\n",
        "                codeword = polar_code_gen.polar_encode(info_bits)\n",
        "\n",
        "                # Extract features\n",
        "                X.append(codeword)\n",
        "                y.append(1 if np.mean(codeword) > 0.5 else 0)\n",
        "\n",
        "            return np.array(X), np.array(y)\n",
        "\n",
        "        # Generate Dataset\n",
        "        X, y = prepare_dataset(\n",
        "            polar_code_gen,\n",
        "            num_samples=SIMULATION_PARAMS['NUM_SAMPLES']\n",
        "        )\n",
        "\n",
        "        # Validate dataset\n",
        "        if X is None or y is None:\n",
        "            raise ValueError(\"Dataset preparation failed\")\n",
        "\n",
        "        print(f\"Dataset Prepared: X shape {X.shape}, y shape {y.shape}\")\n",
        "\n",
        "        # 3. Split Dataset\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=SIMULATION_PARAMS['TEST_SPLIT'],\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        X_train = torch.FloatTensor(X_train).to(device)\n",
        "        X_test = torch.FloatTensor(X_test).to(device)\n",
        "        y_train = torch.FloatTensor(y_train).to(device).unsqueeze(1)\n",
        "        y_test = torch.FloatTensor(y_test).to(device).unsqueeze(1)\n",
        "\n",
        "        print(\" Dataset Split Completed\")\n",
        "\n",
        "        # 4. Traditional Polar Decoder Performance\n",
        "        traditional_decoder = TraditionalPolarDecoder(\n",
        "            N=SIMULATION_PARAMS['BLOCK_LENGTH'],\n",
        "            K=SIMULATION_PARAMS['INFO_BITS']\n",
        "        )\n",
        "\n",
        "        # Channel Simulators\n",
        "        channels = {\n",
        "            'AWGN': ChannelSimulator(channel_type='AWGN'),\n",
        "            'Rayleigh': ChannelSimulator(channel_type='Rayleigh')\n",
        "        }\n",
        "\n",
        "        # Compute Traditional Decoder Performance\n",
        "        traditional_performance = {}\n",
        "        for channel_name, channel in channels.items():\n",
        "            ber, bler = traditional_decoder.compute_performance(\n",
        "                channel,\n",
        "                SIMULATION_PARAMS['SNR_RANGE'],\n",
        "                polar_code_gen\n",
        "            )\n",
        "            traditional_performance[channel_name] = {\n",
        "                'BER': ber,\n",
        "                'BLER': bler\n",
        "            }\n",
        "\n",
        "        # 5. RNN Decoder Training\n",
        "        rnn_model = RNNDecoder(\n",
        "            input_size=4,  # Adjusted input size\n",
        "            hidden_size=SIMULATION_PARAMS['RNN_CONFIG']['hidden_size'],\n",
        "            num_layers=SIMULATION_PARAMS['RNN_CONFIG']['num_layers']\n",
        "        ).to(device)\n",
        "\n",
        "        rnn_trainer = RNNTrainer(rnn_model, learning_rate=SIMULATION_PARAMS['LEARNING_RATE'])\n",
        "\n",
        "        # Train RNN model\n",
        "        rnn_train_losses, rnn_val_losses = rnn_trainer.train(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            epochs=SIMULATION_PARAMS['EPOCHS'],\n",
        "            batch_size=SIMULATION_PARAMS['BATCH_SIZE']\n",
        "        )\n",
        "        print(\" RNN Decoder Training Completed\")\n",
        "\n",
        "        # Print out key parameters and initial results\n",
        "        print(\"\\n Simulation Parameters:\")\n",
        "        for key, value in SIMULATION_PARAMS.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "        print(\"\\n Initial Performance Metrics:\")\n",
        "        for channel, perf in traditional_performance.items():\n",
        "            print(f\"{channel} Channel:\")\n",
        "            print(f\"  Average BER: {np.mean(perf['BER']):.4e}\")\n",
        "            print(f\"  Average BLER: {np.mean(perf['BLER']):.4e}\")\n",
        "\n",
        "       # 6. Training Performance and Error Performance Visualization\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # Combined Training and Validation Losses\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.plot(rnn_train_losses, label='RNN Training Loss', color='blue')\n",
        "        plt.plot(rnn_val_losses, label='RNN Validation Loss', color='red')\n",
        "        plt.plot(ml_train_losses, label='ML Training Loss', color='green')\n",
        "        plt.plot(ml_val_losses, label='ML Validation Loss', color='orange')\n",
        "        plt.title('Combined Training and Validation Losses')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # RNN Decoder Performance\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.plot(rnn_train_losses, label='RNN Training Loss', color='blue')\n",
        "        plt.plot(rnn_val_losses, label='RNN Validation Loss', color='red')\n",
        "        plt.title('RNN Decoder Training Performance')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # ML Decoder Performance\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.plot(ml_train_losses, label='ML Training Loss', color='green')\n",
        "        plt.plot(ml_val_losses, label='ML Validation Loss', color='orange')\n",
        "        plt.title('ML Decoder Training Performance')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Prepare results dictionary for BER/BLER\n",
        "        performance_results = {\n",
        "            'Traditional': {\n",
        "                'ber_awgn': traditional_ber_awgn,\n",
        "                'bler_awgn': traditional_bler_awgn,\n",
        "                'ber_rayleigh': traditional_ber_rayleigh,\n",
        "                'bler_rayleigh': traditional_bler_rayleigh\n",
        "            },\n",
        "            'RNN': {\n",
        "                'ber_awgn': rnn_ber_awgn,\n",
        "                'bler_awgn': rnn_bler_awgn,\n",
        "                'ber_rayleigh': rnn_ber_rayleigh,\n",
        "                'bler_rayleigh': rnn_bler_rayleigh\n",
        "            },\n",
        "            'ML': {\n",
        "                'ber_awgn': ml_ber_awgn,\n",
        "                'bler_awgn': ml_bler_awgn,\n",
        "                'ber_rayleigh': ml_ber_rayleigh,\n",
        "                'bler_rayleigh': ml_bler_rayleigh\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # BER - AWGN Channel\n",
        "        plt.subplot(2, 3, 4)\n",
        "        for decoder in ['Traditional', 'RNN', 'ML']:\n",
        "            plt.semilogy(\n",
        "                SNR_RANGE,\n",
        "                performance_results[decoder]['ber_awgn'],\n",
        "                label=f'{decoder} BER',\n",
        "                marker='o'\n",
        "            )\n",
        "        plt.title('BER - AWGN Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Bit Error Rate')\n",
        "        plt.ylim(1e-5, 1e0)\n",
        "        plt.legend()\n",
        "        plt.grid(True, which='both', ls='-', alpha=0.5)\n",
        "\n",
        "        # BLER - AWGN Channel\n",
        "        plt.subplot(2, 3, 5)\n",
        "        for decoder in ['Traditional', 'RNN', 'ML']:\n",
        "            plt.semilogy(\n",
        "                SNR_RANGE,\n",
        "                performance_results[decoder]['bler_awgn'],\n",
        "                label=f'{decoder} BLER',\n",
        "                marker='s'\n",
        "            )\n",
        "        plt.title('BLER - AWGN Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Block Error Rate')\n",
        "        plt.ylim(1e-5, 1e0)\n",
        "        plt.legend()\n",
        "        plt.grid(True, which='both', ls='-', alpha=0.5)\n",
        "\n",
        "        # Rayleigh Channel BER\n",
        "        plt.subplot(2, 3, 6)\n",
        "        for decoder in ['Traditional', 'RNN', 'ML']:\n",
        "            plt.semilogy(\n",
        "                SNR_RANGE,\n",
        "                performance_results[decoder]['ber_rayleigh'],\n",
        "                label=f'{decoder} BER',\n",
        "                marker='^'\n",
        "            )\n",
        "        plt.title('BER - Rayleigh Channel')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Bit Error Rate')\n",
        "        plt.ylim(1e-5, 1e0)\n",
        "        plt.legend()\n",
        "        plt.grid(True, which='both', ls='-', alpha=0.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('comprehensive_performance.png')\n",
        "        plt.close()\n",
        "        print(\" Comprehensive Performance Plots Saved\")\n",
        "\n",
        "        # Confusion Matrix Visualization\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        # Traditional Decoder Confusion Matrix\n",
        "        plt.subplot(1, 3, 1)\n",
        "        traditional_predictions = traditional_decoder.predict(X_test.cpu().numpy())\n",
        "        traditional_pred_classes = (traditional_predictions > 0.5).astype(int)\n",
        "        traditional_cm = confusion_matrix(y_test.cpu().numpy(), traditional_pred_classes)\n",
        "        sns.heatmap(traditional_cm, annot=True, fmt='d', cmap='Purples')\n",
        "        plt.title('Traditional Decoder\\nConfusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        # RNN Decoder Confusion Matrix\n",
        "        plt.subplot(1, 3, 2)\n",
        "        rnn_predictions = rnn_trainer.predict(X_test.cpu().numpy())\n",
        "        rnn_pred_classes = (rnn_predictions > 0.5).astype(int)\n",
        "        rnn_cm = confusion_matrix(y_test.cpu().numpy(), rnn_pred_classes)\n",
        "        sns.heatmap(rnn_cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('RNN Decoder\\nConfusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        # ML Decoder Confusion Matrix\n",
        "        plt.subplot(1, 3, 3)\n",
        "        ml_predictions = ml_trainer.predict(X_test.cpu().numpy())\n",
        "        ml_pred_classes = (ml_predictions > 0.5).astype(int)\n",
        "        ml_cm = confusion_matrix(y_test.cpu().numpy(), ml_pred_classes)\n",
        "        sns.heatmap(ml_cm, annot=True, fmt='d', cmap='Greens')\n",
        "        plt.title('ML Decoder\\nConfusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('decoder_confusion_matrices.png')\n",
        "        plt.close()\n",
        "        print(\" Confusion Matrices Saved\")\n",
        "\n",
        "        # Classification Reports\n",
        "        print(\"Classification Reports:\")\n",
        "        print(\"Traditional Decoder Classification Report:\")\n",
        "        print(classification_report(\n",
        "            y_test.cpu().numpy(),\n",
        "            traditional_pred_classes\n",
        "        ))\n",
        "\n",
        "        print(\"RNN Decoder Classification Report:\")\n",
        "        print(classification_report(\n",
        "            y_test.cpu().numpy(),\n",
        "            rnn_pred_classes\n",
        "        ))\n",
        "\n",
        "        print(\"ML Decoder Classification Report:\")\n",
        "        print(classification_report(\n",
        "            y_test.cpu().numpy(),\n",
        "            ml_pred_classes\n",
        "        ))\n",
        "\n",
        "        print(\" Simulation Complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Comprehensive Simulation Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YPxNgr3bQNk",
        "outputId": "ee87493c-7f4f-42f3-8a02-ee393a74d052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipympl\n",
            "  Downloading ipympl-0.9.7-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: ipython<10 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (7.7.1)\n",
            "Requirement already satisfied: matplotlib<4,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from ipympl) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipympl) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipympl) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /usr/local/lib/python3.11/dist-packages (from ipympl) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython<10->ipympl)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython<10->ipympl) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4,>=3.5.0->ipympl) (2.8.2)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (6.4.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython<10->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython<10->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<10->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.5.7)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (5.7.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<9,>=7.6.0->ipympl) (4.3.7)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.13.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipympl) (1.3.1)\n",
            "Downloading ipympl-0.9.7-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m515.7/515.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, ipympl\n",
            "Successfully installed ipympl-0.9.7 jedi-0.19.2\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "Successfully installed matplotlib-3.10.1\n",
            " Using Device: cpu\n",
            " Polar Code Generator Initialized\n",
            "Dataset Prepared: X shape (600, 128), y shape (600,)\n",
            " Dataset Split Completed\n",
            " Comprehensive Simulation Error: name 'TraditionalPolarDecoder' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-3-e2cddc8aa471>\", line 1053, in main\n",
            "    traditional_decoder = TraditionalPolarDecoder(\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "NameError: name 'TraditionalPolarDecoder' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1-OixOfGdn0D",
        "outputId": "a6e38ecd-7c22-4200-8379-353e5e9d401c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fd59f27f-cbff-4865-afe3-ed9b8026ff42\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fd59f27f-cbff-4865-afe3-ed9b8026ff42\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MLPolarKSALT.py to MLPolarKSALT.py\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
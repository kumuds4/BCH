{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tH5CsPqlu8hd",
        "outputId": "7666681d-9d5b-42c8-cad4-acd1530578fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ejYDMVa1ve9f",
        "outputId": "5b0bdb59-84e8-4dd8-ce82-6e99c025bf4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4132b0eb-ab52-4808-b2bc-4aa3c4f3c9dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4132b0eb-ab52-4808-b2bc-4aa3c4f3c9dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RMpolarML0622.py to RMpolarML0622.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#latest modification\n",
        "#RM-polar codes and machine learning simulations\n",
        "#06/21/2025\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import torch\n",
        "# Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensure TensorFlow uses the GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth for GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import necessary PyTorch data utilities\n",
        "\n",
        "#################################################################################\n",
        "\n",
        "#Lates RM-polar\n",
        "# Configuration\n",
        "# ---- Configuration ----\n",
        "BLOCK_LENGTH = 128\n",
        "INFO_BITS = 64\n",
        "RM_ORDER = 3\n",
        "SNR_RANGE = np.arange(0, 7, 0.5)\n",
        "LIST_SIZES = [1, 8, 16, 32]\n",
        "NUM_FRAMES = 20000\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001  # Add this for clarity\n",
        "SNR_DB = 2.0  # Signal-to-noise ratio in dB\n",
        "\n",
        "# BPSK Modulation and AWGN Channel\n",
        "def bpsk_modulate(x):\n",
        "    return 1 - 2 * x\n",
        "\n",
        "def awgn_noise(x, snr_db, rate):\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1 / (2 * rate * snr_linear))\n",
        "    return x + sigma * np.random.randn(*x.shape)\n",
        "\n",
        "def llr_awgn(y, snr_db, rate):\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    sigma2 = 1 / (2 * rate * snr_linear)\n",
        "    return 2 * y / sigma2\n",
        "\n",
        "# RM-Polar Construction\n",
        "def get_rm_polar_frozen_indices(N, K, rm_order):\n",
        "    n = int(np.log2(N))\n",
        "    indices = np.arange(N)\n",
        "    bin_indices = np.array([list(np.binary_repr(i, n)) for i in indices], dtype=int)\n",
        "    hamming_weights = bin_indices.sum(axis=1)\n",
        "    rm_candidates = indices[hamming_weights >= rm_order]\n",
        "\n",
        "    # Bhattacharyya parameters (assuming perfect channel)\n",
        "    Z = np.zeros(N)\n",
        "    Z[0] = 2 ** n\n",
        "    for i in range(1, N):\n",
        "        Z[i] = Z[i // 2] / 2 if i % 2 == 0 else (Z[i // 2 - 1] + Z[i // 2]) / 2\n",
        "\n",
        "    polar_order = np.argsort(Z)\n",
        "    filtered = [i for i in polar_order if i in rm_candidates]\n",
        "    info_indices = np.array(filtered[:K])\n",
        "    frozen_indices = np.setdiff1d(np.arange(N), info_indices)\n",
        "    return frozen_indices\n",
        "\n",
        "# Polar Encoder\n",
        "def polar_encode(u, frozen_indices):\n",
        "    N = len(frozen_indices) + len(u)\n",
        "    x = np.zeros(N, dtype=int)\n",
        "    info_idx = np.setdiff1d(np.arange(N), frozen_indices)\n",
        "    x[info_idx] = u\n",
        "    n = int(np.log2(N))\n",
        "    for i in range(n):\n",
        "        step = 2 ** i\n",
        "        for j in range(0, N, 2 * step):\n",
        "            for k in range(step):\n",
        "                x[j + k] ^= x[j + k + step]\n",
        "    return x\n",
        "\n",
        "# SC Decoder\n",
        "def sc_decode(llr, frozen_indices):\n",
        "    N = len(llr)\n",
        "    u_hat = np.zeros(N, dtype=int)\n",
        "    info_idx = np.setdiff1d(np.arange(N), frozen_indices)\n",
        "    u_hat[frozen_indices] = 0\n",
        "    u_hat[info_idx] = (llr[info_idx] < 0).astype(int)\n",
        "    return u_hat[info_idx]\n",
        "\n",
        "# SCL Decoder\n",
        "def scl_decode(llr, frozen_indices, L):\n",
        "    N = len(llr)\n",
        "    paths = [(0.0, np.zeros(N, dtype=int))]\n",
        "    info_idx = np.setdiff1d(np.arange(N), frozen_indices)\n",
        "    for i in range(N):\n",
        "        new_paths = []\n",
        "        for pm, u in paths:\n",
        "            if i in frozen_indices:\n",
        "                u0 = u.copy()\n",
        "                u0[i] = 0\n",
        "                pm_new = pm + np.log1p(np.exp(-abs(llr[i])))\n",
        "                new_paths.append((pm_new, u0))\n",
        "            else:\n",
        "                for bit in [0, 1]:\n",
        "                    u_new = u.copy()\n",
        "                    u_new[i] = bit\n",
        "                    llr_i = llr[i]\n",
        "                    pm_new = pm + (np.log1p(np.exp(-abs(llr_i))) if bit != (llr_i < 0) else 0)\n",
        "                    new_paths.append((pm_new, u_new))\n",
        "        paths = heapq.nsmallest(L, new_paths)\n",
        "    best_path = min(paths, key=lambda x: x[0])[1]\n",
        "    return best_path[info_idx]\n",
        "\n",
        "# RM-Polar Code Class\n",
        "class RMPolarCode:\n",
        "    def __init__(self, N=128, K=64, rm_order=3, snr_db=2.0):  # Correct setup\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.rm_order = rm_order\n",
        "        self.rate = K / N\n",
        "        self.snr_db = snr_db  # Initialize snr_db\n",
        "        self.frozen_indices = get_rm_polar_frozen_indices(N, K, rm_order)\n",
        "\n",
        "    def polar_encode(self, u):\n",
        "        return polar_encode(u, self.frozen_indices)\n",
        "\n",
        "    def calculate_mi(self, snr_db):\n",
        "        self.snr_db = snr_db  # Optionally set snr_db\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        return 0.5 * np.log2(1 + snr_linear * self.rate)\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Define your RNN model with the correct input size\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, output_size=64):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "     output, _ = self.lstm(x)\n",
        "     output = self.fc(output[:, -1, :])\n",
        "     return torch.sigmoid(output)  # Apply sigmoid activation\n",
        "# ---- RNN Decoder ----\n",
        "##########################################################################\n",
        "#latest train decoder\n",
        "def train_rnn_decoder(code, train_loader, val_loader, test_loader, epochs=30, lr=0.001, device='cpu'):\n",
        "    model = RNNDecoder(input_size=1, hidden_size=128, output_size=code.K).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_Y in train_loader:\n",
        "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = torch.sigmoid(model(batch_X))  # Apply sigmoid to output\n",
        "            print(f\"Train Preds shape: {preds.shape}, Targets shape: {batch_Y.shape}\")  # Debug shapes\n",
        "            print(f\"Preds min/max: {preds.min()}/{preds.max()}\")  # Debug values\n",
        "            loss = criterion(preds, batch_Y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for val_X, val_Y in val_loader:\n",
        "                val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
        "                val_preds = torch.sigmoid(model(val_X))  # Apply sigmoid to output\n",
        "                print(f\"Val Preds shape: {val_preds.shape}, Targets shape: {val_Y.shape}\")  # Debug shapes\n",
        "                loss = criterion(val_preds, val_Y)\n",
        "                val_loss += loss.item()\n",
        "            val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_losses[-1]:.4f}\")\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "###########################################################################\n",
        "\n",
        "# ---- ML Decoder Utilities ----\n",
        "\n",
        "#Latest data loader\n",
        "\n",
        "# Reshape data in get_dataloaders to match the expected input size\n",
        "\n",
        "###################################################################\n",
        "\n",
        "\n",
        "\n",
        "def generate_data(num_samples, timesteps, features):\n",
        "    X = np.random.randn(num_samples, timesteps, features)\n",
        "    y = np.random.randint(0, 2, (num_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "# Data preparation\n",
        "timesteps = 10  # Example number of timesteps\n",
        "features = 3    # Number of features, e.g., BPSK, SNR, etc.\n",
        "X_train, y_train = generate_data(1000, timesteps, features)\n",
        "X_val, y_val = generate_data(200, timesteps, features)\n",
        "X_test, y_test = generate_data(200, timesteps, features)\n",
        "\n",
        "# RNN Model Configuration\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(timesteps, features), return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile Model\n",
        "learning_rate = 0.001\n",
        "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Prediction & Calculating BER and BLER\n",
        "def calculate_ber(y_true, y_pred):\n",
        "    errors = np.sum(y_true != y_pred)\n",
        "    total_bits = y_true.size\n",
        "    return errors / total_bits\n",
        "\n",
        "def calculate_bler(y_true, y_pred):\n",
        "    block_errors = np.sum(np.any(y_true != y_pred, axis=1))\n",
        "    num_blocks = y_true.shape[0]\n",
        "    return block_errors / num_blocks\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.round(y_pred_probs)\n",
        "\n",
        "ber = calculate_ber(y_test, y_pred)\n",
        "bler = calculate_bler(y_test, y_pred)\n",
        "\n",
        "print(f\"BER: {ber}, BLER: {bler}\")\n",
        "\n",
        "#######################################################\n",
        "# Reshape data in get_dataloaders to match the expected input size\n",
        "def get_dataloaders(code, batch_size):\n",
        "    num_samples_train = 10000\n",
        "    num_samples_val = 1000\n",
        "    num_samples_test = 1000\n",
        "\n",
        "    # Simulate data\n",
        "    X_train = np.random.randn(num_samples_train, code.N).reshape(-1, code.N, 1)\n",
        "    y_train = np.random.randint(0, 2, (num_samples_train, code.K))\n",
        "\n",
        "    X_val = np.random.randn(num_samples_val, code.N).reshape(-1, code.N, 1)\n",
        "    y_val = np.random.randint(0, 2, (num_samples_val, code.K))\n",
        "\n",
        "    X_test = np.random.randn(num_samples_test, code.N).reshape(-1, code.N, 1)\n",
        "    y_test = np.random.randint(0, 2, (num_samples_test, code.K))\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "####################################################\n",
        "\n",
        "#####################################################\n",
        "#latest evaluator\n",
        "def evaluate_decoder(code, decoder_func, snrs, L=1, model=None, device='cpu'):\n",
        "    frozen_indices = code.frozen_indices\n",
        "    rate = code.rate\n",
        "    ber_curve, bler_curve = [], []\n",
        "    for snr in snrs:\n",
        "        bit_errors, block_errors, total_bits = 0, 0, 0\n",
        "        for _ in range(NUM_FRAMES):\n",
        "            u = np.random.randint(0, 2, code.K)\n",
        "            x = code.polar_encode(u)\n",
        "            y = bpsk_modulate(x)\n",
        "            y_noisy = awgn_noise(y, snr, rate)\n",
        "            llrs = llr_awgn(y_noisy, snr, rate)\n",
        "            if decoder_func == \"sc\":\n",
        "                u_hat = sc_decode(llrs, frozen_indices)\n",
        "            elif decoder_func == \"scl\":\n",
        "                u_hat = scl_decode(llrs, frozen_indices, L)\n",
        "            elif decoder_func == \"rnn\" and model is not None:\n",
        "                y_noisy_tensor = torch.tensor(y_noisy, dtype=torch.float32).to(device)\n",
        "                u_hat = model(y_noisy_tensor.unsqueeze(0)).squeeze().round().cpu().numpy()\n",
        "            else:\n",
        "                raise ValueError(\"Invalid decoder function or model missing for RNN\")\n",
        "            bit_errors += np.sum(u != u_hat)\n",
        "            block_errors += int(not np.array_equal(u, u_hat))\n",
        "            total_bits += code.K\n",
        "        ber_curve.append(bit_errors / total_bits)\n",
        "        bler_curve.append(block_errors / NUM_FRAMES)\n",
        "        print(f\"SNR={snr:.1f} dB: BER={ber_curve[-1]:.2e}, BLER={bler_curve[-1]:.2e}\")\n",
        "    return ber_curve, bler_curve\n",
        "######################################################\n",
        "#Latest main()\n",
        "def main():\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize the RMPolarCode with snr_db\n",
        "    code = RMPolarCode(N=BLOCK_LENGTH, K=INFO_BITS, rm_order=RM_ORDER, snr_db=SNR_DB)\n",
        "\n",
        "    # Prepare data loaders\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(code, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Train and Evaluate RNN Decoder\n",
        "    model, train_losses, val_losses = train_rnn_decoder(\n",
        "        code, train_loader, val_loader, test_loader, epochs=EPOCHS, lr=LEARNING_RATE, device=device\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   # model, train_losses, val_losses = train_rnn_decoder(code, train_loader, val_loader, epochs=EPOCHS, device=device)\n",
        "\n",
        "    print(\"Running SC decoder...\")\n",
        "    ber_sc, bler_sc = evaluate_decoder(code, decoder_func=\"sc\", snrs=SNR_RANGE)\n",
        "\n",
        "    ber_scl_all, bler_scl_all = {}, {}\n",
        "    for L in LIST_SIZES:\n",
        "        print(f\"Running SCL decoder with list size {L}...\")\n",
        "        ber_scl, bler_scl = evaluate_decoder(code, decoder_func=\"scl\", snrs=SNR_RANGE, L=L)\n",
        "        ber_scl_all[L] = ber_scl\n",
        "        bler_scl_all[L] = bler_scl\n",
        "\n",
        "    print(\"Evaluating RNN Decoder...\")\n",
        "    ber_rnn, bler_rnn = evaluate_decoder(code, decoder_func=\"rnn\", snrs=SNR_RANGE, model=model, device=device)\n",
        "\n",
        "    # Plot Training/Validation Loss\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BER Comparison\n",
        "    plt.figure()\n",
        "    plt.semilogy(SNR_RANGE, ber_sc, label=\"SC\")\n",
        "    for L in LIST_SIZES:\n",
        "        plt.semilogy(SNR_RANGE, ber_scl_all[L], label=f\"SCL L={L}\")\n",
        "    plt.semilogy(SNR_RANGE, ber_rnn, label=\"RNN\")\n",
        "    plt.xlabel(\"SNR (dB)\")\n",
        "    plt.ylabel(\"BER\")\n",
        "    plt.ylim(1e-6, 1)\n",
        "    plt.title(\"Bit Error Rate Comparison\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BLER Comparison\n",
        "    plt.figure()\n",
        "    plt.semilogy(SNR_RANGE, bler_sc, label=\"SC\")\n",
        "    for L in LIST_SIZES:\n",
        "        plt.semilogy(SNR_RANGE, bler_scl_all[L], label=f\"SCL L={L}\")\n",
        "    plt.semilogy(SNR_RANGE, bler_rnn, label=\"RNN\")\n",
        "    plt.xlabel(\"SNR (dB)\")\n",
        "    plt.ylabel(\"BLER\")\n",
        "    plt.ylim(1e-6, 1)\n",
        "    plt.title(\"Block Error Rate Comparison\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Mutual Information\n",
        "    plt.figure()\n",
        "    mi_values = [code.calculate_mi(snr) for snr in SNR_RANGE]\n",
        "    plt.plot(SNR_RANGE, mi_values, label='Mutual Information')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Mutual Information (bits)')\n",
        "    plt.title('Mutual Information vs. SNR')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "######################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "Lx8UYrg_wgDU",
        "outputId": "822678e5-7495-4886-b09d-fe1fec7691e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical devices cannot be modified after being initialized\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Cast: CUDA error: Error recording CUDA event: CUDA_ERROR_ASSERT: device-side assert triggered [Op:Cast] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-842708224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m# RNN Model Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, seed, return_sequences, return_state, go_backwards, stateful, unroll, use_cudnn, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     ):\n\u001b[0;32m--> 477\u001b[0;31m         cell = LSTMCell(\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/lstm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, seed, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_mask_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_forget_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munit_forget_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/random/seed_generator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, seed, name, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             self.state = self.backend.Variable(\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mseed_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initializer, shape, dtype, trainable, autocast, aggregation, name)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_with_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36m_initialize_with_initializer\u001b[0;34m(self, initializer)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_with_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deferred_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         self._value = tf.Variable(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_with_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deferred_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/random/seed_generator.py\u001b[0m in \u001b[0;36mseed_initializer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mseed_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# and cast instead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Cast: CUDA error: Error recording CUDA event: CUDA_ERROR_ASSERT: device-side assert triggered [Op:Cast] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#latest modification\n",
        "#RM-polar codes and machine learning simulations\n",
        "#06/21/2025\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "import torch\n",
        "# Imports\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensure TensorFlow uses the GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth for GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import necessary PyTorch data utilities\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "\n",
        "#Lates RM-polar\n",
        "# Configuration\n",
        "# ---- Configuration ----\n",
        "BLOCK_LENGTH = 128\n",
        "INFO_BITS = 64\n",
        "RM_ORDER = 3\n",
        "SNR_RANGE = np.arange(0, 7, 0.5)\n",
        "LIST_SIZES = [1, 8, 16, 32]\n",
        "NUM_FRAMES = 20000\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001  # Add this for clarity\n",
        "SNR_DB = 2.0  # Signal-to-noise ratio in dB\n",
        "\n",
        "# BPSK Modulation and AWGN Channel\n",
        "def bpsk_modulate(x):\n",
        "    return 1 - 2 * x\n",
        "\n",
        "def awgn_noise(x, snr_db, rate):\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1 / (2 * rate * snr_linear))\n",
        "    return x + sigma * np.random.randn(*x.shape)\n",
        "\n",
        "def llr_awgn(y, snr_db, rate):\n",
        "    snr_linear = 10 ** (snr_db / 10)\n",
        "    sigma2 = 1 / (2 * rate * snr_linear)\n",
        "    return 2 * y / sigma2\n",
        "\n",
        "# RM-Polar Construction\n",
        "def get_rm_polar_frozen_indices(N, K, rm_order):\n",
        "    n = int(np.log2(N))\n",
        "    indices = np.arange(N)\n",
        "    bin_indices = np.array([list(np.binary_repr(i, n)) for i in indices], dtype=int)\n",
        "    hamming_weights = bin_indices.sum(axis=1)\n",
        "    rm_candidates = indices[hamming_weights >= rm_order]\n",
        "\n",
        "    # Bhattacharyya parameters (assuming perfect channel)\n",
        "    Z = np.zeros(N)\n",
        "    Z[0] = 2 ** n\n",
        "    for i in range(1, N):\n",
        "        Z[i] = Z[i // 2] / 2 if i % 2 == 0 else (Z[i // 2 - 1] + Z[i // 2]) / 2\n",
        "\n",
        "    polar_order = np.argsort(Z)\n",
        "    filtered = [i for i in polar_order if i in rm_candidates]\n",
        "    info_indices = np.array(filtered[:K])\n",
        "    frozen_indices = np.setdiff1d(np.arange(N), info_indices)\n",
        "    return frozen_indices\n",
        "\n",
        "# Polar Encoder\n",
        "def polar_encode(u, frozen_indices):\n",
        "    N = len(frozen_indices) + len(u)\n",
        "    x = np.zeros(N, dtype=int)\n",
        "    info_idx = np.setdiff1d(np.arange(N), frozen_indices)\n",
        "    x[info_idx] = u\n",
        "    n = int(np.log2(N))\n",
        "    for i in range(n):\n",
        "        step = 2 ** i\n",
        "        for j in range(0, N, 2 * step):\n",
        "            for k in range(step):\n",
        "                x[j + k] ^= x[j + k + step]\n",
        "    return x\n",
        "\n",
        "# SC Decoder\n",
        "def sc_decode(llr, frozen_indices):\n",
        "    N = len(llr)\n",
        "    u_hat = np.zeros(N, dtype=int)\n",
        "    info_idx = np.setdiff1d(np.arange(N), frozen_indices)\n",
        "    u_hat[frozen_indices] = 0\n",
        "    u_hat[info_idx] = (llr[info_idx] < 0).astype(int)\n",
        "    return u_hat[info_idx]\n",
        "\n",
        "# SCL Decoder\n",
        "def scl_decode(llr, frozen_indices, L):\n",
        "    N = len(llr)\n",
        "    paths = [(0.0, np.zeros(N, dtype=int))]\n",
        "    info_idx = np.setdiff1d(np.arange(N), frozen_indices)\n",
        "    for i in range(N):\n",
        "        new_paths = []\n",
        "        for pm, u in paths:\n",
        "            if i in frozen_indices:\n",
        "                u0 = u.copy()\n",
        "                u0[i] = 0\n",
        "                pm_new = pm + np.log1p(np.exp(-abs(llr[i])))\n",
        "                new_paths.append((pm_new, u0))\n",
        "            else:\n",
        "                for bit in [0, 1]:\n",
        "                    u_new = u.copy()\n",
        "                    u_new[i] = bit\n",
        "                    llr_i = llr[i]\n",
        "                    pm_new = pm + (np.log1p(np.exp(-abs(llr_i))) if bit != (llr_i < 0) else 0)\n",
        "                    new_paths.append((pm_new, u_new))\n",
        "        paths = heapq.nsmallest(L, new_paths)\n",
        "    best_path = min(paths, key=lambda x: x[0])[1]\n",
        "    return best_path[info_idx]\n",
        "\n",
        "# RM-Polar Code Class\n",
        "class RMPolarCode:\n",
        "    def __init__(self, N=128, K=64, rm_order=3, snr_db=2.0):  # Correct setup\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.rm_order = rm_order\n",
        "        self.rate = K / N\n",
        "        self.snr_db = snr_db  # Initialize snr_db\n",
        "        self.frozen_indices = get_rm_polar_frozen_indices(N, K, rm_order)\n",
        "\n",
        "    def polar_encode(self, u):\n",
        "        return polar_encode(u, self.frozen_indices)\n",
        "\n",
        "    def calculate_mi(self, snr_db):\n",
        "        self.snr_db = snr_db  # Optionally set snr_db\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        return 0.5 * np.log2(1 + snr_linear * self.rate)\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Define your RNN model with the correct input size\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=128, output_size=64):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "     output, _ = self.lstm(x)\n",
        "     output = self.fc(output[:, -1, :])\n",
        "     return torch.sigmoid(output)  # Apply sigmoid activation\n",
        "# ---- RNN Decoder ----\n",
        "##########################################################################\n",
        "#latest train decoder\n",
        "def train_rnn_decoder(code, train_loader, val_loader, test_loader, epochs=30, lr=0.001, device='cpu'):\n",
        "    model = RNNDecoder(input_size=1, hidden_size=128, output_size=code.K).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch_X, batch_Y in train_loader:\n",
        "            batch_X, batch_Y = batch_X.to(device), batch_Y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # No sigmoid here, BCELoss with logits is more stable.\n",
        "            # But since the model already has sigmoid, keep it for now.\n",
        "            # Ideally, remove sigmoid from forward and use nn.BCEWithLogitsLoss\n",
        "            preds = model(batch_X)\n",
        "            # print(f\"Train Preds shape: {preds.shape}, Targets shape: {batch_Y.shape}\")  # Debug shapes\n",
        "            # print(f\"Preds min/max: {preds.min()}/{preds.max()}\")  # Debug values\n",
        "            loss = criterion(preds, batch_Y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for val_X, val_Y in val_loader:\n",
        "                val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
        "                # No sigmoid here\n",
        "                val_preds = model(val_X)\n",
        "                # print(f\"Val Preds shape: {val_preds.shape}, Targets shape: {val_Y.shape}\")  # Debug shapes\n",
        "                loss = criterion(val_preds, val_Y)\n",
        "                val_loss += loss.item()\n",
        "            val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_losses[-1]:.4f}\")\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "###########################################################################\n",
        "\n",
        "# ---- ML Decoder Utilities ----\n",
        "\n",
        "#Latest data loader\n",
        "\n",
        "# Reshape data in get_dataloaders to match the expected input size\n",
        "\n",
        "###################################################################\n",
        "\n",
        "\n",
        "\n",
        "def generate_data(num_samples, timesteps, features):\n",
        "    X = np.random.randn(num_samples, timesteps, features)\n",
        "    y = np.random.randint(0, 2, (num_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "# Data preparation - These TensorFlow data generators are not used in the main\n",
        "# function's RNN training, which uses PyTorch data loaders.\n",
        "# Keeping them here for context but they don't affect the error.\n",
        "timesteps = 10  # Example number of timesteps\n",
        "features = 3    # Number of features, e.g., BPSK, SNR, etc.\n",
        "X_train_tf, y_train_tf = generate_data(1000, timesteps, features)\n",
        "X_val_tf, y_val_tf = generate_data(200, timesteps, features)\n",
        "X_test_tf, y_test_tf = generate_data(200, timesteps, features)\n",
        "\n",
        "# RNN Model Configuration - This is the problematic Keras model\n",
        "model_tf = Sequential()\n",
        "# Change: Explicitly set seed to None for the Keras LSTM layer\n",
        "model_tf.add(LSTM(128, input_shape=(timesteps, features), return_sequences=False, seed=None))\n",
        "model_tf.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile Model - This Keras model compilation is not used in the main function\n",
        "learning_rate_tf = 0.001\n",
        "model_tf.compile(optimizer=Adam(learning_rate=learning_rate_tf), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model - This Keras model training is not used in the main function\n",
        "model_tf.fit(X_train_tf, y_train_tf, epochs=20, batch_size=32, validation_data=(X_val_tf, y_val_tf))\n",
        "\n",
        "# Prediction & Calculating BER and BLER - These functions are for the Keras model\n",
        "def calculate_ber(y_true, y_pred):\n",
        "    errors = np.sum(y_true != y_pred)\n",
        "    total_bits = y_true.size\n",
        "    return errors / total_bits\n",
        "\n",
        "def calculate_bler(y_true, y_pred):\n",
        "    block_errors = np.sum(np.any(y_true != y_pred, axis=1))\n",
        "    num_blocks = y_true.shape[0]\n",
        "    return block_errors / num_blocks\n",
        "\n",
        "# Make Predictions - These predictions are for the Keras model\n",
        "y_pred_probs_tf = model_tf.predict(X_test_tf)\n",
        "y_pred_tf = np.round(y_pred_probs_tf)\n",
        "\n",
        "ber_tf = calculate_ber(y_test_tf, y_pred_tf)\n",
        "bler_tf = calculate_bler(y_test_tf, y_pred_tf)\n",
        "\n",
        "print(f\"Keras Model BER: {ber_tf}, BLER: {bler_tf}\")\n",
        "\n",
        "#######################################################\n",
        "# Reshape data in get_dataloaders to match the expected input size\n",
        "def get_dataloaders(code, batch_size):\n",
        "    num_samples_train = 10000\n",
        "    num_samples_val = 1000\n",
        "    num_samples_test = 1000\n",
        "\n",
        "    # Simulate data\n",
        "    # The PyTorch RNN expects input shape (batch_size, sequence_length, input_size)\n",
        "    # Here, the sequence_length is code.N and input_size is 1.\n",
        "    X_train = np.random.randn(num_samples_train, code.N, 1)\n",
        "    y_train = np.random.randint(0, 2, (num_samples_train, code.K))\n",
        "\n",
        "    X_val = np.random.randn(num_samples_val, code.N, 1)\n",
        "    y_val = np.random.randint(0, 2, (num_samples_val, code.K))\n",
        "\n",
        "    # For the RNN decoder evaluation, we will simulate data within the evaluate_decoder function\n",
        "    # This avoids generating potentially massive datasets if NUM_FRAMES is large.\n",
        "    # We will still return a test loader here for completeness, but the evaluation\n",
        "    # uses on-the-fly data generation.\n",
        "    X_test = np.random.randn(num_samples_test, code.N, 1)\n",
        "    y_test = np.random.randint(0, 2, (num_samples_test, code.K))\n",
        "\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)) # Not used in evaluate_decoder\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # Not used in evaluate_decoder\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "####################################################\n",
        "\n",
        "#####################################################\n",
        "#latest evaluator\n",
        "def evaluate_decoder(code, decoder_func, snrs, L=1, model=None, device='cpu'):\n",
        "    frozen_indices = code.frozen_indices\n",
        "    rate = code.rate\n",
        "    ber_curve, bler_curve = [], []\n",
        "    for snr in snrs:\n",
        "        bit_errors, block_errors, total_bits = 0, 0, 0\n",
        "        # Use NUM_FRAMES for evaluation samples\n",
        "        for _ in range(NUM_FRAMES):\n",
        "            # Generate data on the fly for evaluation\n",
        "            u = np.random.randint(0, 2, code.K)\n",
        "            x = code.polar_encode(u)\n",
        "            y = bpsk_modulate(x)\n",
        "            y_noisy = awgn_noise(y, snr, rate)\n",
        "            llrs = llr_awgn(y_noisy, snr, rate)\n",
        "\n",
        "            if decoder_func == \"sc\":\n",
        "                u_hat = sc_decode(llrs, frozen_indices)\n",
        "            elif decoder_func == \"scl\":\n",
        "                u_hat = scl_decode(llrs, frozen_indices, L)\n",
        "            elif decoder_func == \"rnn\" and model is not None:\n",
        "                # Reshape the input for the PyTorch RNN model\n",
        "                # Expected shape: (batch_size, sequence_length, input_size)\n",
        "                # Since we are evaluating one frame at a time, batch_size=1\n",
        "                y_noisy_tensor = torch.tensor(llrs, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "                # The model's forward method handles the LSTM and final layer.\n",
        "                # The output already has sigmoid applied in the RNNDecoder forward pass.\n",
        "                # We just need to round the output to get binary predictions.\n",
        "                u_hat = model(y_noisy_tensor).squeeze().round().cpu().numpy().astype(int)\n",
        "\n",
        "                # Ensure u_hat has the correct shape (K,)\n",
        "                if u_hat.ndim == 0:\n",
        "                    u_hat = np.array([u_hat])\n",
        "                elif u_hat.ndim > 1:\n",
        "                     # This case should ideally not happen if the model output size is code.K\n",
        "                     # If it does, you might need to debug the model's final layer output shape.\n",
        "                     print(f\"Warning: RNN decoder output has unexpected shape {u_hat.shape}. Expected shape ({code.K},). Squeezing.\")\n",
        "                     u_hat = u_hat.squeeze() # Try squeezing again\n",
        "\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid decoder function or model missing for RNN\")\n",
        "\n",
        "            # Ensure u_hat has the same shape as u before comparison\n",
        "            # The u is (code.K,) numpy array.\n",
        "            if u_hat.shape != u.shape:\n",
        "                 print(f\"Warning: Decoder output shape {u_hat.shape} does not match info bits shape {u.shape} for SNR {snr} dB.\")\n",
        "                 # Attempt to reshape if necessary, but ideally the decoder should output the correct shape\n",
        "                 # This might indicate an issue in the decoder function itself if not 'rnn'\n",
        "                 if decoder_func != \"rnn\":\n",
        "                      # This might be a bug in SC/SCL if they return wrong shape\n",
        "                      pass # Handle appropriately or debug SC/SCL\n",
        "\n",
        "\n",
        "            # Calculate errors only if shapes match to avoid ValueError\n",
        "            if u_hat.shape == u.shape:\n",
        "                bit_errors += np.sum(u != u_hat)\n",
        "                block_errors += int(not np.array_equal(u, u_hat))\n",
        "                total_bits += code.K\n",
        "            else:\n",
        "                print(f\"Skipping error calculation for this frame due to shape mismatch: u.shape={u.shape}, u_hat.shape={u_hat.shape}\")\n",
        "\n",
        "\n",
        "        # Avoid division by zero if total_bits is 0 (should not happen if samples > 0)\n",
        "        ber_curve.append(bit_errors / total_bits if total_bits > 0 else np.nan)\n",
        "        bler_curve.append(block_errors / NUM_FRAMES if NUM_FRAMES > 0 else np.nan)\n",
        "        print(f\"SNR={snr:.1f} dB: BER={ber_curve[-1]:.2e}, BLER={bler_curve[-1]:.2e}\")\n",
        "    return ber_curve, bler_curve\n",
        "######################################################\n",
        "#Latest main()\n",
        "def main():\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\") # Print the device being used\n",
        "\n",
        "    # Initialize the RMPolarCode with snr_db\n",
        "    code = RMPolarCode(N=BLOCK_LENGTH, K=INFO_BITS, rm_order=RM_ORDER, snr_db=SNR_DB)\n",
        "\n",
        "    # Prepare data loaders (for PyTorch RNN training)\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(code, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Train and Evaluate RNN Decoder\n",
        "    print(\"Training RNN Decoder...\")\n",
        "    model, train_losses, val_losses = train_rnn_decoder(\n",
        "        code, train_loader, val_loader, test_loader, epochs=EPOCHS, lr=LEARNING_RATE, device=device\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Running SC decoder...\")\n",
        "    ber_sc, bler_sc = evaluate_decoder(code, decoder_func=\"sc\", snrs=SNR_RANGE, code=code) # Pass code object\n",
        "\n",
        "    ber_scl_all, bler_scl_all = {}, {}\n",
        "    for L in LIST_SIZES:\n",
        "        print(f\"Running SCL decoder with list size {L}...\")\n",
        "        ber_scl, bler_scl = evaluate_decoder(code, decoder_func=\"scl\", snrs=SNR_RANGE, L=L, code=code) # Pass code object\n",
        "        ber_scl_all[L] = ber_scl\n",
        "        bler_scl_all[L] = bler_scl\n",
        "\n",
        "    print(\"Evaluating RNN Decoder...\")\n",
        "    # For the RNN evaluation, we pass the trained PyTorch model\n",
        "    ber_rnn, bler_rnn = evaluate_decoder(code, decoder_func=\"rnn\", snrs=SNR_RANGE, model=model, device=device, code=code) # Pass code object\n",
        "\n",
        "\n",
        "    # Plot Training/Validation Loss\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BER Comparison\n",
        "    plt.figure()\n",
        "    plt.semilogy(SNR_RANGE, ber_sc, label=\"SC\")\n",
        "    for L in LIST_SIZES:\n",
        "        plt.semilogy(SNR_RANGE, ber_scl_all[L], label=f\"SCL L={L}\")\n",
        "    plt.semilogy(SNR_RANGE, ber_rnn, label=\"RNN\")\n",
        "    plt.xlabel(\"SNR (dB)\")\n",
        "    plt.ylabel(\"BER\")\n",
        "    plt.ylim(1e-6, 1)\n",
        "    plt.title(\"Bit Error Rate Comparison\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BLER Comparison\n",
        "    plt.figure()\n",
        "    plt.semilogy(SNR_RANGE, bler_sc, label=\"SC\")\n",
        "    for L in LIST_SIZES:\n",
        "        plt.semilogy(SNR_RANGE, bler_scl_all[L], label=f\"SCL L={L}\")\n",
        "    plt.semilogy(SNR_RANGE, bler_rnn, label=\"RNN\")\n",
        "    plt.xlabel(\"SNR (dB)\")\n",
        "    plt.ylabel(\"BLER\")\n",
        "    plt.ylim(1e-6, 1)\n",
        "    plt.title(\"Block Error Rate Comparison\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Mutual Information\n",
        "    plt.figure()\n",
        "    mi_values = [code.calculate_mi(snr) for snr in SNR_RANGE]\n",
        "    plt.plot(SNR_RANGE, mi_values, label='Mutual Information')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Mutual Information (bits)')\n",
        "    plt.title('Mutual Information vs. SNR')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "X2BgCwFVMJOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
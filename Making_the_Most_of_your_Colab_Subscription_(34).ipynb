{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Making_the_Most_of_your_Colab_Subscription_(34).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PI9KoJNn92sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IoPMYD3y9o3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "sPWvfcK5-Ews"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "N = 128                      # Block length\n",
        "K = 64                       # Info bits\n",
        "CRC_LEN = 8                  # CRC bits\n",
        "L_list = [1, 4, 8, 16]       # SCL list sizes\n",
        "TOTAL_BITS = K + CRC_LEN\n",
        "SNR_DB_RANGE = np.arange(0.0, 4.5, 0.5)\n",
        "NUM_FRAMES = 80000\n",
        "\n",
        "# RNN Training Config\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_SNR_DB = 2.0\n",
        "VAL_SNR_DB = 2.0\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- CRC-8 Polynomial: x^8 + x^2 + x + 1 ---\n",
        "CRC_POLY = 0x07\n",
        "\n",
        "def crc_encode(info_bits):\n",
        "    data = np.concatenate([info_bits, np.zeros(CRC_LEN, dtype=int)])\n",
        "    for i in range(len(info_bits)):\n",
        "        if data[i] == 1:\n",
        "            for j in range(CRC_LEN + 1):\n",
        "                if ((CRC_POLY >> j) & 1):\n",
        "                    data[i + j] ^= 1\n",
        "    return np.concatenate([info_bits, data[-CRC_LEN:]])\n",
        "\n",
        "def crc_check(codeword):\n",
        "    data = codeword.copy()\n",
        "    for i in range(len(codeword) - CRC_LEN):\n",
        "        if data[i] == 1:\n",
        "            for j in range(CRC_LEN + 1):\n",
        "                if ((CRC_POLY >> j) & 1):\n",
        "                    data[i + j] ^= 1\n",
        "    return np.all(data[-CRC_LEN:] == 0)\n",
        "\n",
        "# --- Polar Reliability (5G 128-bit) ---\n",
        "polar_reliability_sequence = list(range(128))  # placeholder; replace with actual 5G sequence\n",
        "info_indices = sorted(polar_reliability_sequence[:TOTAL_BITS])\n",
        "frozen_indices = sorted(set(range(N)) - set(info_indices))\n",
        "\n",
        "# --- Polar Transform ---\n",
        "def polar_transform(u):\n",
        "    u = u.copy()\n",
        "    n = int(np.log2(len(u)))\n",
        "    for d in range(n):\n",
        "        step = 2 ** d\n",
        "        for i in range(0, len(u), 2 * step):\n",
        "            for j in range(step):\n",
        "                u[i + j] ^= u[i + j + step]\n",
        "    return u\n",
        "\n",
        "# --- Encoder ---\n",
        "def polar_encode(info_bits):\n",
        "    u = np.zeros(N, dtype=int)\n",
        "    info_crc = crc_encode(info_bits)\n",
        "    u[info_indices] = info_crc\n",
        "    return polar_transform(u)\n",
        "\n",
        "# --- Channel + LLR ---\n",
        "def bpsk(x): return 1 - 2 * x\n",
        "\n",
        "def awgn(y, snr_db):\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1 / (2 * snr))\n",
        "    noise = sigma * np.random.randn(*y.shape)\n",
        "    return y + noise, sigma\n",
        "\n",
        "def llr_calc(y, sigma):\n",
        "    return 2 * y / (sigma ** 2)\n",
        "\n",
        "def compute_mutual_information(llrs, bits):\n",
        "    llrs = np.clip(llrs, -50, 50)\n",
        "    mi = 1 - np.mean(np.log2(1 + np.exp(-llrs * (1 - 2 * bits))))\n",
        "    return mi\n",
        "\n",
        "# --- Dataset generation ---\n",
        "def generate_dataset(num_frames, snr_db):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for _ in range(num_frames):\n",
        "        info_bits = np.random.randint(0, 2, K)\n",
        "        encoded = polar_encode(info_bits)\n",
        "        tx_signal = bpsk(encoded)\n",
        "        rx_signal, sigma = awgn(tx_signal, snr_db)\n",
        "        llrs = llr_calc(rx_signal, sigma)\n",
        "        inputs.append(llrs)\n",
        "        targets.append(info_bits)\n",
        "    inputs = torch.tensor(np.array(inputs), dtype=torch.float32)\n",
        "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
        "    return TensorDataset(inputs, targets)\n",
        "\n",
        "# --- Fast SSC Decoder ---\n",
        "def f_func(a, b):\n",
        "    return np.sign(a) * np.sign(b) * np.minimum(np.abs(a), np.abs(b))\n",
        "\n",
        "def g_func(a, b, c):\n",
        "    return b + (1 - 2 * c) * a\n",
        "\n",
        "def fast_ssc_decode(llrs, frozen_indices):\n",
        "    N = len(llrs)\n",
        "    is_frozen = np.zeros(N, dtype=bool)\n",
        "    is_frozen[frozen_indices] = True\n",
        "\n",
        "    def recurse(llr, frozen_mask):\n",
        "        length = len(llr)\n",
        "        if length == 1:\n",
        "            return np.array([0]) if frozen_mask[0] else np.array([0 if llr[0] >= 0 else 1])\n",
        "\n",
        "        if np.all(frozen_mask):\n",
        "            return np.zeros(length, dtype=int)\n",
        "\n",
        "        if not np.any(frozen_mask):\n",
        "            return np.array([0 if l >= 0 else 1 for l in llr])\n",
        "\n",
        "        if np.all(frozen_mask[:-1]) and not frozen_mask[-1]:\n",
        "            total_llr = np.sum(llr)\n",
        "            bit = 0 if total_llr >= 0 else 1\n",
        "            return np.concatenate([np.zeros(length - 1, dtype=int), [bit]])\n",
        "\n",
        "        if frozen_mask[0] and not np.any(frozen_mask[1:]):\n",
        "            bits = np.array([0 if l >= 0 else 1 for l in llr])\n",
        "            parity = np.sum(bits) % 2\n",
        "            if parity != 0:\n",
        "                min_index = np.argmin(np.abs(llr))\n",
        "                bits[min_index] ^= 1\n",
        "            return bits\n",
        "\n",
        "        half = length // 2\n",
        "        llr_left = f_func(llr[:half], llr[half:])\n",
        "        u_left = recurse(llr_left, frozen_mask[:half])\n",
        "        llr_right = g_func(llr[:half], llr[half:], u_left)\n",
        "        u_right = recurse(llr_right, frozen_mask[half:])\n",
        "        return np.concatenate([u_left, u_right])\n",
        "\n",
        "    return recurse(llrs, is_frozen)\n",
        "\n",
        "# --- Fast SC Decoder ---\n",
        "def fast_sc_decode(llr, frozen_indices):\n",
        "    N = len(llr)\n",
        "    frozen_set = set(frozen_indices)\n",
        "    u_hat = np.zeros(N, dtype=int)\n",
        "\n",
        "    def recurse(llr, depth, offset):\n",
        "        if depth == 0:\n",
        "            index = offset\n",
        "            if index in frozen_set:\n",
        "                return np.array([0])\n",
        "            else:\n",
        "                return np.array([int(llr[0] < 0)])\n",
        "\n",
        "        half = len(llr) // 2\n",
        "        l0 = f_func(llr[:half], llr[half:])\n",
        "        u0 = recurse(l0, depth - 1, offset)\n",
        "        l1 = g_func(llr[:half], llr[half:], u0)\n",
        "        u1 = recurse(l1, depth - 1, offset + half)\n",
        "        return np.concatenate([u0, u1])\n",
        "\n",
        "    depth = int(np.log2(N))\n",
        "    return recurse(llr, depth, 0)\n",
        "\n",
        "# --- Fast SCL Decoder ---\n",
        "def fast_scl_decode(llr, frozen_indices, L):\n",
        "    paths = [(np.zeros(len(llr), dtype=int), 0.0)]\n",
        "    frozen_set = set(frozen_indices)\n",
        "\n",
        "    for i in range(len(llr)):\n",
        "        new_paths = []\n",
        "        for u_prev, pm in paths:\n",
        "            if i in frozen_set:\n",
        "                u = u_prev.copy()\n",
        "                u[i] = 0\n",
        "                pm_new = pm + (0 if llr[i] >= 0 else abs(llr[i]))\n",
        "                new_paths.append((u, pm_new))\n",
        "            else:\n",
        "                for bit in [0, 1]:\n",
        "                    u = u_prev.copy()\n",
        "                    u[i] = bit\n",
        "                    pm_new = pm + (0 if bit == (llr[i] < 0) else abs(llr[i]))\n",
        "                    new_paths.append((u, pm_new))\n",
        "        new_paths.sort(key=lambda x: x[1])\n",
        "        paths = new_paths[:L]\n",
        "\n",
        "    for u, _ in paths:\n",
        "        decoded = polar_transform(u)\n",
        "        if crc_check(decoded[info_indices]):\n",
        "            return u\n",
        "\n",
        "    return paths[0][0]\n",
        "\n",
        "# --- RNN Decoder ---\n",
        "class PolarRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size=N, hidden_size=256, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, K)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # shape: (batch, N, 1)\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out  # raw logits for BCEWithLogitsLoss\n",
        "\n",
        "# --- Training and Validation Dataset ---\n",
        "def generate_data(num_samples, snr_db):\n",
        "    X, Y = [], []\n",
        "    for _ in range(num_samples):\n",
        "        info = np.random.randint(0, 2, K)\n",
        "        x = polar_encode(info)\n",
        "        y = bpsk(x)\n",
        "        y_noisy, sigma = awgn(y, snr_db)\n",
        "        llr = llr_calc(y_noisy, sigma)\n",
        "        X.append(llr)\n",
        "        Y.append(info)\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_rnn(model, train_loader, val_loader, criterion, optimizer):\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                outputs = model(xb)\n",
        "                val_loss += criterion(outputs, yb).item()\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# --- Plotting Functions ---\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('RNN Training/Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_sc_decoder(snr_range, ber_sc, bler_sc):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.semilogy(snr_range, ber_sc, 'o-', label='SC BER')\n",
        "    plt.semilogy(snr_range, bler_sc, 's--', label='SC BLER')\n",
        "    plt.ylim(1e-5, 1e0)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title('SC Decoder: BER and BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_scl_decoder(snr_range, ber_scl_dict, bler_scl_dict):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for L in ber_scl_dict:\n",
        "        plt.semilogy(snr_range, ber_scl_dict[L], marker='o', label=f'SCL L={L} BER')\n",
        "    for L in bler_scl_dict:\n",
        "        plt.semilogy(snr_range, bler_scl_dict[L], marker='s', linestyle='--', label=f'SCL L={L} BLER')\n",
        "    plt.ylim(1e-5, 1e0)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title('SCL Decoder: BER and BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_rnn_decoder(snr_range, ber_rnn, bler_rnn):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.semilogy(snr_range, ber_rnn, 'o-', label='RNN BER')\n",
        "    plt.semilogy(snr_range, bler_rnn, 's--', label='RNN BLER')\n",
        "    plt.ylim(1e-5, 1e0)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Error Rate')\n",
        "    plt.title('RNN Decoder: BER and BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- Main function ---\n",
        "def main():\n",
        "    print(f\"Running on device: {DEVICE}\")\n",
        "\n",
        "    # Prepare RNN model, optimizer, loss\n",
        "    rnn_model = PolarRNNDecoder().to(DEVICE)\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Generate dataset\n",
        "    trainval_dataset = generate_dataset(NUM_FRAMES, TRAIN_SNR_DB)\n",
        "    train_size = int(0.8 * len(trainval_dataset))\n",
        "    val_size = len(trainval_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Train RNN\n",
        "    train_losses, val_losses = train_rnn(rnn_model, train_loader, val_loader, criterion, optimizer)\n",
        "    plot_loss(train_losses, val_losses)\n",
        "\n",
        "    # Evaluate decoders over SNR range\n",
        "    ber_sc, bler_sc = [], []\n",
        "    ber_rnn, bler_rnn = [], []\n",
        "    ber_scl_dict = {L: [] for L in L_list}\n",
        "    bler_scl_dict = {L: [] for L in L_list}\n",
        "    mi_vals = []\n",
        "\n",
        "    for snr_db in SNR_DB_RANGE:\n",
        "        print(f\"Simulating SNR={snr_db:.1f} dB\")\n",
        "        bit_errors_sc = 0\n",
        "        block_errors_sc = 0\n",
        "        bit_errors_rnn = 0\n",
        "        block_errors_rnn = 0\n",
        "        bit_errors_scl = {L: 0 for L in L_list}\n",
        "        block_errors_scl = {L: 0 for L in L_list}\n",
        "        mi_accum = []\n",
        "\n",
        "        for _ in range(NUM_FRAMES):\n",
        "            info_bits = np.random.randint(0, 2, K)\n",
        "            encoded = polar_encode(info_bits)\n",
        "            tx_signal = bpsk(encoded)\n",
        "            rx_signal, sigma = awgn(tx_signal, snr_db)\n",
        "            llrs = llr_calc(rx_signal, sigma)\n",
        "\n",
        "            mi_accum.append(compute_mutual_information(llrs[info_indices], info_bits))\n",
        "\n",
        "            # SC decode\n",
        "            sc_u_hat = fast_ssc_decode(llrs, frozen_indices)\n",
        "            sc_bits_hat = polar_transform(sc_u_hat)[info_indices][:TOTAL_BITS]\n",
        "            sc_info_hat = sc_bits_hat[:K]\n",
        "            err_bits_sc = np.sum(sc_info_hat != info_bits)\n",
        "            bit_errors_sc += err_bits_sc\n",
        "            if err_bits_sc > 0:\n",
        "                block_errors_sc += 1\n",
        "\n",
        "            # RNN decode\n",
        "            llr_tensor = torch.tensor(llrs, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(DEVICE)\n",
        "            rnn_model.eval()\n",
        "            with torch.no_grad():\n",
        "                rnn_output = rnn_model(llr_tensor).cpu().numpy().flatten()\n",
        "            rnn_info_hat = (rnn_output > 0).astype(int)\n",
        "            err_bits_rnn = np.sum(rnn_info_hat[:K] != info_bits)\n",
        "            bit_errors_rnn += err_bits_rnn\n",
        "            if err_bits_rnn > 0:\n",
        "                block_errors_rnn += 1\n",
        "\n",
        "            # SCL decode\n",
        "            for L in L_list:\n",
        "                scl_u_hat = fast_scl_decode(llrs, frozen_indices, L)\n",
        "                scl_bits_hat = polar_transform(scl_u_hat)[info_indices][:TOTAL_BITS]\n",
        "                scl_info_hat = scl_bits_hat[:K]\n",
        "                err_bits_scl = np.sum(scl_info_hat != info_bits)\n",
        "                bit_errors_scl[L] += err_bits_scl\n",
        "                if err_bits_scl > 0:\n",
        "                    block_errors_scl[L] += 1\n",
        "\n",
        "        ber_sc.append(bit_errors_sc / (NUM_FRAMES * K))\n",
        "        bler_sc.append(block_errors_sc / NUM_FRAMES)\n",
        "        ber_rnn.append(bit_errors_rnn / (NUM_FRAMES * K))\n",
        "        bler_rnn.append(block_errors_rnn / NUM_FRAMES)\n",
        "        for L in L_list:\n",
        "            ber_scl_dict[L].append(bit_errors_scl[L] / (NUM_FRAMES * K))\n",
        "            bler_scl_dict[L].append(block_errors_scl[L] / NUM_FRAMES)\n",
        "\n",
        "        mi_vals.append(np.mean(mi_accum))\n",
        "\n",
        "        print(f\"SNR={snr_db:.1f} | SC BER={ber_sc[-1]:.2e}, BLER={bler_sc[-1]:.2e} | \"\n",
        "              f\"RNN BER={ber_rnn[-1]:.2e}, BLER={bler_rnn[-1]:.2e} | \" +\n",
        "              \" | \".join([f\"SCL L={L} BER={ber_scl_dict[L][-1]:.2e}, BLER={bler_scl_dict[L][-1]:.2e}\" for L in L_list])\n",
        "        )\n",
        "\n",
        "    # Plot BER/BLER curves separately\n",
        "    plot_sc_decoder(SNR_DB_RANGE, ber_sc, bler_sc)\n",
        "    plot_rnn_decoder(SNR_DB_RANGE, ber_rnn, bler_rnn)\n",
        "    plot_scl_decoder(SNR_DB_RANGE, ber_scl_dict, bler_scl_dict)\n",
        "\n",
        "    # Plot Mutual Information\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(SNR_DB_RANGE, mi_vals, marker='o', label='Mutual Information')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Mutual Information (bits)')\n",
        "    plt.title('Mutual Information vs SNR')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "ZFMGymCQTNVi",
        "outputId": "6843f29f-f329-459f-bf18-82ad932af9d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Running on device: cuda\n",
            "Epoch 1: Train Loss = 0.6934, Val Loss = 0.6933\n",
            "Epoch 2: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 3: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 4: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 5: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 6: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 7: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 8: Train Loss = 0.6932, Val Loss = 0.6932\n",
            "Epoch 9: Train Loss = 0.6915, Val Loss = 0.6892\n",
            "Epoch 10: Train Loss = 0.6876, Val Loss = 0.6861\n",
            "Epoch 11: Train Loss = 0.6847, Val Loss = 0.6839\n",
            "Epoch 12: Train Loss = 0.6825, Val Loss = 0.6818\n",
            "Epoch 13: Train Loss = 0.6798, Val Loss = 0.6814\n",
            "Epoch 14: Train Loss = 0.6759, Val Loss = 0.6800\n",
            "Epoch 15: Train Loss = 0.6711, Val Loss = 0.6686\n",
            "Epoch 16: Train Loss = 0.6658, Val Loss = 0.6675\n",
            "Epoch 17: Train Loss = 0.6606, Val Loss = 0.6600\n",
            "Epoch 18: Train Loss = 0.6567, Val Loss = 0.6581\n",
            "Epoch 19: Train Loss = 0.6760, Val Loss = 0.6809\n",
            "Epoch 20: Train Loss = 0.6677, Val Loss = 0.6633\n",
            "Epoch 21: Train Loss = 0.6555, Val Loss = 0.6737\n",
            "Epoch 22: Train Loss = 0.6493, Val Loss = 0.6614\n",
            "Epoch 23: Train Loss = 0.6422, Val Loss = 0.6606\n",
            "Epoch 24: Train Loss = 0.6482, Val Loss = 0.6848\n",
            "Epoch 25: Train Loss = 0.6668, Val Loss = 0.6907\n",
            "Epoch 26: Train Loss = 0.6674, Val Loss = 0.6850\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2199045812.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-6-2199045812.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# Train RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-2199045812.py\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-2199045812.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# shape: (batch, N, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m  \u001b[0;31m# raw logits for BCEWithLogitsLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 \u001b[0mnum_directions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m                 hx = torch.zeros(\n\u001b[0m\u001b[1;32m   1380\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                     \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unified Polar Code Simulation Script\n",
        "# Includes: SC, CRC-aided SCL, RNN Decoder (GPU-enabled), BER/BLER + MI Plots\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "N = 128                      # Block length\n",
        "K = 64                       # Info bits\n",
        "CRC_LEN = 8                  # CRC bits\n",
        "L_list = [1, 4, 8, 16]       # SCL list sizes\n",
        "TOTAL_BITS = K + CRC_LEN\n",
        "SNR_DB_RANGE = np.arange(-0.5, 4.5, 0.5)\n",
        "NUM_FRAMES = 140000\n",
        "\n",
        "# RNN Training Config\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 128\n",
        "TRAIN_SNR_DB = 2.0\n",
        "VAL_SNR_DB = 2.0\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- CRC-8 Polynomial: x^8 + x^2 + x + 1 ---\n",
        "CRC_POLY = 0x07\n",
        "\n",
        "def crc_encode(info_bits):\n",
        "    data = np.concatenate([info_bits, np.zeros(CRC_LEN, dtype=int)])\n",
        "    for i in range(len(info_bits)):\n",
        "        if data[i] == 1:\n",
        "            for j in range(CRC_LEN + 1):\n",
        "                if ((CRC_POLY >> j) & 1):\n",
        "                    data[i + j] ^= 1\n",
        "    return np.concatenate([info_bits, data[-CRC_LEN:]])\n",
        "\n",
        "def crc_check(codeword):\n",
        "    data = codeword.copy()\n",
        "    for i in range(len(codeword) - CRC_LEN):\n",
        "        if data[i] == 1:\n",
        "            for j in range(CRC_LEN + 1):\n",
        "                if ((CRC_POLY >> j) & 1):\n",
        "                    data[i + j] ^= 1\n",
        "    return np.all(data[-CRC_LEN:] == 0)\n",
        "\n",
        "# --- Polar Reliability (5G 128-bit) ---\n",
        "polar_reliability_sequence = list(range(128))  # placeholder; replace with actual 5G sequence\n",
        "info_indices = sorted(polar_reliability_sequence[:TOTAL_BITS])\n",
        "frozen_indices = sorted(set(range(N)) - set(info_indices))\n",
        "\n",
        "# --- Polar Transform ---\n",
        "def polar_transform(u):\n",
        "    u = u.copy()\n",
        "    n = int(np.log2(len(u)))\n",
        "    for d in range(n):\n",
        "        step = 2 ** d\n",
        "        for i in range(0, len(u), 2 * step):\n",
        "            for j in range(step):\n",
        "                u[i + j] ^= u[i + j + step]\n",
        "    return u\n",
        "\n",
        "# --- Encoder ---\n",
        "def polar_encode(info_bits):\n",
        "    u = np.zeros(N, dtype=int)\n",
        "    info_crc = crc_encode(info_bits)\n",
        "    u[info_indices] = info_crc\n",
        "    return polar_transform(u)\n",
        "\n",
        "# --- Channel + LLR ---\n",
        "def bpsk(x): return 1 - 2 * x\n",
        "\n",
        "def awgn(y, snr_db):\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    sigma = np.sqrt(1 / (2 * snr))\n",
        "    noise = sigma * np.random.randn(*y.shape)\n",
        "    return y + noise, sigma\n",
        "\n",
        "def llr_calc(y, sigma):\n",
        "    return 2 * y / (sigma ** 2)\n",
        "\n",
        "def compute_mutual_information(llrs, bits):\n",
        "    # Align bits length with llrs length to avoid broadcasting errors\n",
        "    bits = bits[:len(llrs)]\n",
        "    llrs = np.clip(llrs, -50, 50)\n",
        "    mi = 1 - np.mean(np.log2(1 + np.exp(-llrs * (1 - 2 * bits))))\n",
        "    return mi\n",
        "\n",
        "\n",
        "def generate_dataset(num_frames, snr_db):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for _ in range(num_frames):\n",
        "        info_bits = np.random.randint(0, 2, K)\n",
        "        encoded = polar_encode(info_bits)\n",
        "        tx_signal = bpsk(encoded)\n",
        "        rx_signal, sigma = awgn(tx_signal, snr_db)\n",
        "        llrs = llr_calc(rx_signal, sigma)\n",
        "\n",
        "        inputs.append(llrs)\n",
        "        targets.append(info_bits)\n",
        "    inputs = torch.tensor(np.array(inputs), dtype=torch.float32)\n",
        "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
        "    return TensorDataset(inputs, targets)\n",
        "\n",
        "def fast_ssc_decode(llrs, frozen_indices):\n",
        "    N = len(llrs)\n",
        "    u_hat = np.zeros(N, dtype=int)\n",
        "    is_frozen = np.zeros(N, dtype=bool)\n",
        "    is_frozen[frozen_indices] = True\n",
        "\n",
        "    def recurse(llr, start, length, frozen_mask):\n",
        "        if length == 1:\n",
        "            return np.array([0]) if frozen_mask[0] else np.array([0 if llr[0] >= 0 else 1])\n",
        "\n",
        "        # Rate-0 node (all frozen)\n",
        "        if np.all(frozen_mask):\n",
        "            return np.zeros(length, dtype=int)\n",
        "\n",
        "        # Rate-1 node (all info)\n",
        "        if not np.any(frozen_mask):\n",
        "            return np.array([0 if l >= 0 else 1 for l in llr])\n",
        "\n",
        "        # Repetition node (only last bit is info)\n",
        "        if np.all(frozen_mask[:-1]) and not frozen_mask[-1]:\n",
        "            total_llr = np.sum(llr)\n",
        "            bit = 0 if total_llr >= 0 else 1\n",
        "            return np.concatenate([np.zeros(length - 1, dtype=int), [bit]])\n",
        "\n",
        "        # SPC node (only first bit is frozen)\n",
        "        if frozen_mask[0] and not np.any(frozen_mask[1:]):\n",
        "            bits = np.array([0 if l >= 0 else 1 for l in llr])\n",
        "            parity = np.sum(bits) % 2\n",
        "            if parity != 0:\n",
        "                # Flip the LSB with least reliable LLR\n",
        "                min_index = np.argmin(np.abs(llr))\n",
        "                bits[min_index] ^= 1\n",
        "            return bits\n",
        "\n",
        "        # Recursive case: split into halves\n",
        "        half = length // 2\n",
        "        llr_left = f_func(llr[:half], llr[half:])\n",
        "        u_left = recurse(llr_left, start, half, frozen_mask[:half])\n",
        "\n",
        "        llr_right = g_func(llr[:half], llr[half:], u_left)\n",
        "        u_right = recurse(llr_right, start + half, half, frozen_mask[half:])\n",
        "\n",
        "        return np.concatenate([u_left, u_right])\n",
        "\n",
        "    # f and g node operations\n",
        "    def f_func(a, b):\n",
        "        return np.sign(a) * np.sign(b) * np.minimum(np.abs(a), np.abs(b))\n",
        "\n",
        "    def g_func(a, b, c):\n",
        "        return b + (1 - 2 * c) * a\n",
        "\n",
        "    u_hat = recurse(llrs, 0, N, is_frozen)\n",
        "    return u_hat\n",
        "\n",
        "################################################################################\n",
        "# Fast SC Decoder functions\n",
        "def f(a, b):\n",
        "    return np.sign(a) * np.sign(b) * np.minimum(np.abs(a), np.abs(b))\n",
        "\n",
        "def g(a, b, c):\n",
        "    return b + (1 - 2 * c) * a\n",
        "\n",
        "def fast_sc_decode(llr, frozen_indices):\n",
        "    N = len(llr)\n",
        "    u_hat = np.zeros(N, dtype=int)\n",
        "\n",
        "    def recurse(llr, depth, offset):\n",
        "        if depth == 0:\n",
        "            index = offset\n",
        "            if index in frozen_indices:\n",
        "                return np.array([0])\n",
        "            else:\n",
        "                return np.array([int(llr[0] < 0)])\n",
        "\n",
        "        half = len(llr) // 2\n",
        "        l0 = f(llr[:half], llr[half:])\n",
        "        u0 = recurse(l0, depth - 1, offset)\n",
        "        l1 = g(llr[:half], llr[half:], u0)\n",
        "        u1 = recurse(l1, depth - 1, offset + half)\n",
        "        return np.concatenate([u0, u1])\n",
        "\n",
        "    depth = int(np.log2(N))\n",
        "    return recurse(llr, depth, 0)\n",
        "\n",
        "################################################################################\n",
        "# Fast CRC-Aided SCL Decoder (simple path metric based)\n",
        "def fast_scl_decode(llr, frozen_indices, L):\n",
        "    paths = [(np.zeros(len(llr), dtype=int), 0.0)]\n",
        "\n",
        "    for i in range(len(llr)):\n",
        "        new_paths = []\n",
        "        for u_prev, pm in paths:\n",
        "            if i in frozen_indices:\n",
        "                u = u_prev.copy()\n",
        "                u[i] = 0\n",
        "                pm_new = pm + (0 if llr[i] >= 0 else abs(llr[i]))\n",
        "                new_paths.append((u, pm_new))\n",
        "            else:\n",
        "                for bit in [0, 1]:\n",
        "                    u = u_prev.copy()\n",
        "                    u[i] = bit\n",
        "                    pm_new = pm + (0 if bit == (llr[i] < 0) else abs(llr[i]))\n",
        "                    new_paths.append((u, pm_new))\n",
        "        new_paths.sort(key=lambda x: x[1])\n",
        "        paths = new_paths[:L]\n",
        "\n",
        "    for u, _ in paths:\n",
        "        decoded = polar_transform(u)\n",
        "        if crc_check(decoded[info_indices]):\n",
        "            return u\n",
        "\n",
        "    return paths[0][0]\n",
        "\n",
        "################################################################################\n",
        "# Tuned RNN\n",
        "# --- RNN Decoder (Tuned) ---\n",
        "class PolarRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size=N, hidden_size=512, num_layers=3):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, K)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # shape: (batch, N, 1)\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "# --- Training Function (unchanged, uses updated model and dataset) ---\n",
        "#def train_rnn(model, train_loader, val_loader, criterion, optimizer):\n",
        "def train_rnn(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "                outputs = model(xb)\n",
        "                val_loss += criterion(outputs, yb).item()\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
        "    return train_losses, val_losses\n",
        "#################################################################################\n",
        "# RNN Decoder (Simple GRU)\n",
        "def generate_dataset(num_frames, snr_db_or_range):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        info_bits = np.random.randint(0, 2, K)\n",
        "\n",
        "        encoded = polar_encode(info_bits)\n",
        "        tx_signal = bpsk(encoded)\n",
        "\n",
        "        # Support either single SNR or a range\n",
        "        if isinstance(snr_db_or_range, (list, np.ndarray)):\n",
        "            snr_db = np.random.choice(snr_db_or_range)\n",
        "        else:\n",
        "            snr_db = snr_db_or_range\n",
        "\n",
        "        rx_signal, sigma = awgn(tx_signal, snr_db)\n",
        "        llrs = llr_calc(rx_signal, sigma)\n",
        "\n",
        "        inputs.append(llrs)\n",
        "        targets.append(info_bits)\n",
        "\n",
        "    inputs = torch.tensor(np.array(inputs), dtype=torch.float32)\n",
        "    targets = torch.tensor(np.array(targets), dtype=torch.float32)\n",
        "    return TensorDataset(inputs, targets)\n",
        "\n",
        "\n",
        "\n",
        "# CSV file saving\n",
        "def save_ber_bler_csv(snr_range, ber_dict, bler_dict, filename=\"ber_bler_results.csv\"):\n",
        "    data = {\"SNR(dB)\": snr_range}\n",
        "    for label, ber in ber_dict.items():\n",
        "        data[f\"{label}_BER\"] = ber\n",
        "    for label, bler in bler_dict.items():\n",
        "        data[f\"{label}_BLER\"] = bler\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"[✓] Saved BER/BLER results to: {filename}\")\n",
        "####################################################################\n",
        "# Latest Plotting Functions\n",
        "\n",
        "\n",
        "\n",
        "# Ensure plot uses correct font and styles\n",
        "plt.rcParams.update({\n",
        "    'font.size': 12,\n",
        "    'axes.labelsize': 14,\n",
        "    'axes.titlesize': 16,\n",
        "    'legend.fontsize': 12,\n",
        "    'xtick.labelsize': 12,\n",
        "    'ytick.labelsize': 12,\n",
        "    'figure.figsize': (10, 6),\n",
        "    'axes.grid': True\n",
        "})\n",
        "##################################################################################################################\n",
        "\n",
        "def plot_ber(snr_range, ber_results, title):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for label, ber in ber_results.items():\n",
        "        plt.semilogy(snr_range, ber, marker='o', label=label)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate (BER)')\n",
        "    plt.title(title)\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_bler(snr_range, bler_results, title):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    for label, bler in bler_results.items():\n",
        "        plt.semilogy(snr_range, bler, marker='s', label=label)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate (BLER)')\n",
        "    plt.title(title)\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_validation_loss(train_loss, val_loss):\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(epochs, train_loss, 'b-o', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'r-o', label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_mutual_information(snr_range, mi_values):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(snr_range, mi_values, 'g-o', label='Mutual Information')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Mutual Information (bits)')\n",
        "    plt.title('Mutual Information vs. SNR')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    print(f\"Running on device: {DEVICE}\")\n",
        "\n",
        "    # --- Configuration ---\n",
        "    NUM_FRAMES = 140000\n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 128\n",
        "    LEARNING_RATE = 1e-3\n",
        "    TRAIN_SNR_RANGE = np.arange(0.5, 4.5, 0.5)\n",
        "    SNR_DB_RANGE = np.arange(-0.5, 4.5, 0.5)\n",
        "    L_list = [1, 4, 8, 16]\n",
        "\n",
        "    # Initialize RNN model, optimizer, and loss function\n",
        "    rnn_model = PolarRNNDecoder().to(DEVICE)\n",
        "    optimizer = optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Generate datasets and loaders\n",
        "    trainval_dataset = generate_dataset(NUM_FRAMES, TRAIN_SNR_RANGE)\n",
        "    train_size = int(0.8 * len(trainval_dataset))\n",
        "    val_size = len(trainval_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Train RNN and plot training/validation loss\n",
        "    train_losses, val_losses = train_rnn(rnn_model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)\n",
        "   # train_losses, val_losses = train_rnn(rnn_model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)\n",
        "    plot_training_validation_loss(train_losses, val_losses)\n",
        "\n",
        "    # Initialize results containers\n",
        "    ber_sc, bler_sc = [], []\n",
        "    ber_rnn, bler_rnn = [], []\n",
        "    ber_scl_dict = {L: [] for L in L_list}\n",
        "    bler_scl_dict = {L: [] for L in L_list}\n",
        "    mi_vals = []\n",
        "\n",
        "    # Evaluate decoders over SNR range\n",
        "    for snr_db in SNR_DB_RANGE:\n",
        "        print(f\"Simulating SNR={snr_db:.1f} dB\")\n",
        "        bit_errors_sc = 0\n",
        "        block_errors_sc = 0\n",
        "        bit_errors_rnn = 0\n",
        "        block_errors_rnn = 0\n",
        "        bit_errors_scl = {L: 0 for L in L_list}\n",
        "        block_errors_scl = {L: 0 for L in L_list}\n",
        "        mi_accum = []\n",
        "\n",
        "        for _ in range(NUM_FRAMES):\n",
        "            info_bits = np.random.randint(0, 2, K)\n",
        "            encoded = polar_encode(info_bits)\n",
        "            tx_signal = bpsk(encoded)\n",
        "            rx_signal, sigma = awgn(tx_signal, snr_db)\n",
        "            llrs = llr_calc(rx_signal, sigma)\n",
        "\n",
        "            mi_accum.append(compute_mutual_information(llrs[info_indices], crc_encode(info_bits)))\n",
        "\n",
        "            # SC decoding\n",
        "            sc_u_hat = fast_ssc_decode(llrs, frozen_indices)\n",
        "            sc_bits_hat = polar_transform(sc_u_hat)[info_indices][:TOTAL_BITS]\n",
        "            sc_info_hat = sc_bits_hat[:K]\n",
        "            err_bits_sc = np.sum(sc_info_hat != info_bits)\n",
        "            bit_errors_sc += err_bits_sc\n",
        "            if err_bits_sc > 0:\n",
        "                block_errors_sc += 1\n",
        "\n",
        "            # RNN decoding\n",
        "            llr_tensor = torch.tensor(llrs, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
        "            rnn_model.eval()\n",
        "            with torch.no_grad():\n",
        "                rnn_output = rnn_model(llr_tensor).cpu().numpy().flatten()\n",
        "            rnn_info_hat = (rnn_output > 0.5).astype(int)\n",
        "            err_bits_rnn = np.sum(rnn_info_hat[:K] != info_bits)\n",
        "            bit_errors_rnn += err_bits_rnn\n",
        "            if err_bits_rnn > 0:\n",
        "                block_errors_rnn += 1\n",
        "\n",
        "            # SCL decoding for each list size\n",
        "            for L in L_list:\n",
        "                scl_u_hat = fast_scl_decode(llrs, frozen_indices, L)\n",
        "                scl_bits_hat = polar_transform(scl_u_hat)[info_indices][:TOTAL_BITS]\n",
        "                scl_info_hat = scl_bits_hat[:K]\n",
        "                err_bits_scl = np.sum(scl_info_hat != info_bits)\n",
        "                bit_errors_scl[L] += err_bits_scl\n",
        "                if err_bits_scl > 0:\n",
        "                    block_errors_scl[L] += 1\n",
        "\n",
        "        # Append results for this SNR\n",
        "        ber_sc.append(bit_errors_sc / (NUM_FRAMES * K))\n",
        "        bler_sc.append(block_errors_sc / NUM_FRAMES)\n",
        "        ber_rnn.append(bit_errors_rnn / (NUM_FRAMES * K))\n",
        "        bler_rnn.append(block_errors_rnn / NUM_FRAMES)\n",
        "        for L in L_list:\n",
        "            ber_scl_dict[L].append(bit_errors_scl[L] / (NUM_FRAMES * K))\n",
        "            bler_scl_dict[L].append(block_errors_scl[L] / NUM_FRAMES)\n",
        "        mi_vals.append(np.mean(mi_accum))\n",
        "\n",
        "        print(f\"SNR={snr_db:.1f} | SC BER={ber_sc[-1]:.2e}, BLER={bler_sc[-1]:.2e} | \"\n",
        "              f\"RNN BER={ber_rnn[-1]:.2e}, BLER={bler_rnn[-1]:.2e} | \" +\n",
        "              \" | \".join([f\"SCL L={L} BER={ber_scl_dict[L][-1]:.2e}, BLER={bler_scl_dict[L][-1]:.2e}\" for L in L_list]))\n",
        "\n",
        "    # Organize results for plotting\n",
        "    ber_results_sc = {\"SC\": ber_sc}\n",
        "    bler_results_sc = {\"SC\": bler_sc}\n",
        "    ber_results_scl = {f\"L={L}\": ber_scl_dict[L] for L in L_list if L != 1}  # SCL excludes SC (L=1)\n",
        "    bler_results_scl = {f\"L={L}\": bler_scl_dict[L] for L in L_list if L != 1}\n",
        "    ber_results_rnn = {\"RNN\": ber_rnn}\n",
        "    bler_results_rnn = {\"RNN\": bler_rnn}\n",
        "\n",
        "    # Plot BER & BLER separately for SC, SCL, and RNN\n",
        "    plot_ber(SNR_DB_RANGE, ber_results_sc, title=\"BER vs SNR SC\")\n",
        "    plot_bler(SNR_DB_RANGE, bler_results_sc, title=\"BLER vs SNR SC\")\n",
        "\n",
        "    plot_ber(SNR_DB_RANGE, ber_results_scl, title=\"BER vs SNR SCL\")\n",
        "    plot_bler(SNR_DB_RANGE, bler_results_scl, title=\"BLER vs SNR SCL\")\n",
        "\n",
        "    plot_ber(SNR_DB_RANGE, ber_results_rnn, title=\"BER vs SNR RNN\")\n",
        "    plot_bler(SNR_DB_RANGE, bler_results_rnn, title=\"BLER vs SNR RNN\")\n",
        "\n",
        "    # Plot Mutual Information\n",
        "    plot_mutual_information(SNR_DB_RANGE, mi_vals)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "###############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c4a3EAjcYKwL",
        "outputId": "e6fbeaa8-ebe8-4474-fdf9-9b18360a9323"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Running on device: cuda\n",
            "Epoch 1: Train Loss = 0.6932, Val Loss = 0.6931\n",
            "Epoch 2: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 3: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 4: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 5: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 6: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 7: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 8: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 9: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 10: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 11: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 12: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 13: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 14: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 15: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 16: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 17: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 18: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 19: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 20: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 21: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 22: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 23: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 24: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 25: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 26: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 27: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 28: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 29: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 30: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 31: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 32: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 33: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 34: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 35: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 36: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 37: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 38: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 39: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 40: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 41: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 42: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 43: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 44: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 45: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 46: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 47: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 48: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 49: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 50: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 51: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 52: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 53: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 54: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 55: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 56: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 57: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 58: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 59: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 60: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 61: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 62: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 63: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 64: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 65: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 66: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 67: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 68: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 69: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 70: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 71: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 72: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 73: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 74: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 75: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 76: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 77: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 78: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 79: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 80: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 81: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 82: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 83: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 84: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 85: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 86: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 87: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 88: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 89: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 90: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 91: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 92: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 93: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 94: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 95: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 96: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 97: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 98: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 99: Train Loss = 0.6931, Val Loss = 0.6931\n",
            "Epoch 100: Train Loss = 0.6931, Val Loss = 0.6931\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating SNR=-0.5 dB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-703191617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;31m###############################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-19-703191617.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;31m# SCL decoding for each list size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mL\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mL_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m                 \u001b[0mscl_u_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_scl_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozen_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m                 \u001b[0mscl_bits_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolar_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscl_u_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfo_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTOTAL_BITS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mscl_info_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscl_bits_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-19-703191617.py\u001b[0m in \u001b[0;36mfast_scl_decode\u001b[0;34m(llr, frozen_indices, L)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mpm_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mllr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mnew_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpm_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
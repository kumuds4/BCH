{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/Copy_of_Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNLtDNXm4_gH",
        "outputId": "a7872b1f-f6fe-436e-94ae-f005f9f5ca74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "9KqMvqUS6fSs",
        "outputId": "c4556e0b-6620-485a-c677-3c3d062f85ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-82531fa1-992e-4328-a5e4-8b9da54568f5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-82531fa1-992e-4328-a5e4-8b9da54568f5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PythonKumudS.ipynb to PythonKumudS.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "WOclrgq47iuV",
        "outputId": "05e02924-24ab-4099-8d8b-6beac89b8e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6d3a413c-1161-4acc-9285-d9c465293b97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6d3a413c-1161-4acc-9285-d9c465293b97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PythonKumudSA.py to PythonKumudSA.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "3mEXPAyL8yPA",
        "outputId": "d9b1edcb-5bfa-4605-9f3a-26c88b8abc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-465b753b-56e9-49cd-aaf7-1b47d7fa84d5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-465b753b-56e9-49cd-aaf7-1b47d7fa84d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PolaRNN.py to PolaRNN.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Gr5puf-x94FT",
        "outputId": "9839b802-e42b-4bd0-b2b5-8e4006cdc225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-01c8dca4-11e6-4b44-bb6f-976045f8175f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-01c8dca4-11e6-4b44-bb6f-976045f8175f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving polarmlrnn.ipynb to polarmlrnn.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "xiZ1JHYj-_kO",
        "outputId": "25895b3b-bc2a-4698-d17b-94bb5c455074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-958c59db-aac1-41dd-8ecf-065973c6982f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-958c59db-aac1-41dd-8ecf-065973c6982f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving polarmlr.py to polarmlr.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  # If using train_test_split\n",
        "\n",
        "class CRC:\n",
        "    def __init__(self, polynomial=0b10011011, order=7):\n",
        "        self.polynomial = polynomial\n",
        "        self.order = order\n",
        "\n",
        "    def generate_crc(self, data):\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.order, dtype=int)])\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    data_with_zeros[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        return data_with_zeros[-self.order:]\n",
        "\n",
        "    def verify_crc(self, data, received_crc):\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    full_data[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        return np.all(full_data[-self.order:] == 0)\n",
        "\n",
        "\n",
        "    # ... (existing CRC implementation)\n",
        "\n",
        "\n",
        "\n",
        "# Then, add the new PolarCodeGenerator class\n",
        "class PolarCodeGenerator:\n",
        "\n",
        "\n",
        "    def __init__(self, N=128, K=64):\n",
        "        self.N = N  # Total block length\n",
        "        self.K = K  # Number of information bits\n",
        "        self.R = K/N  # Code rate\n",
        "        self.crc = CRC()  # CRC object\n",
        "\n",
        "    def generate_polar_code_matrix(self):\n",
        "        \"\"\"\n",
        "        Generate channel polarization using Bhattacharyya parameter method\n",
        "        \"\"\"\n",
        "        def bhattacharyya_parameter(W, n):\n",
        "            if n == 0:\n",
        "                return W\n",
        "            W_used = bhattacharyya_parameter(W, n-1)\n",
        "            W_transform = 2 * W_used**2 - W_used**4\n",
        "            return W_transform\n",
        "\n",
        "        # Compute Bhattacharyya parameters for each channel\n",
        "        channel_capacities = []\n",
        "        for i in range(self.N):\n",
        "            W = 0.5  # Binary symmetric channel\n",
        "            capacity = bhattacharyya_parameter(W, int(np.log2(self.N)))\n",
        "            channel_capacities.append(capacity)\n",
        "\n",
        "        # Sort and select best channels for information bits\n",
        "        sorted_indices = np.argsort(channel_capacities)\n",
        "        info_indices = sorted_indices[self.N - self.K:]\n",
        "\n",
        "        return info_indices\n",
        "\n",
        "    def polar_transform(self, u):\n",
        "        \"\"\"\n",
        "        Recursive Polar Transform (Arkan's Polarization Transform)\n",
        "        \"\"\"\n",
        "        n = int(np.log2(len(u)))\n",
        "        for i in range(n):\n",
        "            u1 = np.zeros(len(u), dtype=int)\n",
        "            for j in range(len(u) // 2):\n",
        "                # Butterfly operation\n",
        "                u1[2*j] = np.mod(u[j] + u[j + len(u)//2], 2)\n",
        "                u1[2*j + 1] = u[j + len(u)//2]\n",
        "            u = u1\n",
        "        return u\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Polar Encoding Process\n",
        "        1. Add CRC\n",
        "        2. Polar Encoding\n",
        "        \"\"\"\n",
        "        # Add CRC to information bits\n",
        "        crc_bits = self.crc.generate_crc(info_bits)\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Polar Encoding\n",
        "        encoded_bits = self._polar_encode(full_info)\n",
        "\n",
        "        return encoded_bits\n",
        "\n",
        "    def _polar_encode(self, bits):\n",
        "        \"\"\"\n",
        "        Detailed Polar Encoding Implementation\n",
        "        \"\"\"\n",
        "        # Initialize codeword\n",
        "        x = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Determine information bit positions\n",
        "        info_indices = self.generate_polar_code_matrix()\n",
        "\n",
        "        # Assign information bits to selected indices\n",
        "        x[info_indices] = bits\n",
        "\n",
        "        # Polar transformation\n",
        "        n = int(np.log2(self.N))\n",
        "        for i in range(n):\n",
        "            for j in range(0, self.N, 2**(i+1)):\n",
        "                for k in range(2**i):\n",
        "                    # Butterfly operation\n",
        "                    u = x[j+k]\n",
        "                    v = x[j+k+2**i]\n",
        "                    x[j+k] = (u + v) % 2\n",
        "                    x[j+k+2**i] = v\n",
        "\n",
        "        return x\n",
        "\n",
        "    def systematic_polar_encode(self, info_bits):\n",
        "        \"\"\"\n",
        "        Systematic Polar Encoding\n",
        "        Preserves original information bits in specific positions\n",
        "        \"\"\"\n",
        "        # Add CRC\n",
        "        crc_bits = self.crc.generate_crc(info_bits)\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "\n",
        "        # Initialize codeword\n",
        "        x = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Determine information bit positions\n",
        "        info_indices = self.generate_polar_code_matrix()\n",
        "\n",
        "        # Assign information bits to selected indices\n",
        "        x[info_indices] = full_info\n",
        "\n",
        "        # Polar transformation\n",
        "        n = int(np.log2(self.N))\n",
        "        for i in range(n):\n",
        "            for j in range(0, self.N, 2**(i+1)):\n",
        "                for k in range(2**i):\n",
        "                    # Butterfly operation\n",
        "                    u = x[j+k]\n",
        "                    v = x[j+k+2**i]\n",
        "                    x[j+k] = (u + v) % 2\n",
        "                    x[j+k+2**i] = v\n",
        "\n",
        "        return x  # ... (the entire implementation I just provided)\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N=128, K=64):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.R = K/N\n",
        "        self.crc = CRC()\n",
        "\n",
        "    def generate_polar_code_matrix(self):\n",
        "        def bhattacharyya_parameter(W, n):\n",
        "            if n == 0:\n",
        "                return W\n",
        "            W_used = bhattacharyya_parameter(W, n-1)\n",
        "            W_transform = 2 * W_used**2 - W_used**4\n",
        "            return W_transform\n",
        "\n",
        "        channel_capacities = []\n",
        "        for i in range(self.N):\n",
        "            W = 0.5\n",
        "            capacity = bhattacharyya_parameter(W, int(np.log2(self.N)))\n",
        "            channel_capacities.append(capacity)\n",
        "\n",
        "        sorted_indices = np.argsort(channel_capacities)\n",
        "        info_indices = sorted_indices[self.N - self.K:]\n",
        "\n",
        "        return info_indices\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        crc_bits = self.crc.generate_crc(info_bits)\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "        encoded_bits = self._polar_encode(full_info)\n",
        "        return encoded_bits\n",
        "\n",
        "    def _polar_encode(self, bits):\n",
        "         n = int(np.log2(self.N))\n",
        "         x = np.zeros(self.N, dtype=int)\n",
        "         x[:len(bits)] = bits\n",
        "\n",
        "         for i in range(n):\n",
        "            for j in range(0, self.N, 2**(i+1)):\n",
        "                for k in range(2**i):\n",
        "                    u = x[j+k]\n",
        "                    v = x[j+k+2**i]\n",
        "                    x[j+k] = (u + v) % 2\n",
        "                    x[j+k+2**i] = v\n",
        "\n",
        "\n",
        "         return x  # Return the encoded bits\n",
        "class SCLDecoder:\n",
        "    def __init__(self, list_size=8):\n",
        "        self.list_size = list_size\n",
        "\n",
        "    def decode(self, received_signal, info_indices, block_length):\n",
        "        N = block_length  # Code length\n",
        "        K = len(info_indices)  # Number of information bits\n",
        "        L = self.list_size  # List size\n",
        "\n",
        "        # Initialize lists\n",
        "        active_paths = [([0] * K, 0)]  # (path, path metric)\n",
        "\n",
        "        for i in range(N):\n",
        "            new_paths = []\n",
        "            for path, metric in active_paths:\n",
        "                # Calculate likelihoods for 0 and 1\n",
        "                llr = self._calculate_llr(received_signal[i])  # Replace with your LLR calculation\n",
        "\n",
        "                # Extend paths for both 0 and 1\n",
        "                path0 = path + [0]\n",
        "                path1 = path + [1]\n",
        "\n",
        "                # Update path metrics\n",
        "                metric0 = metric + (0 if llr > 0 else -llr)\n",
        "                metric1 = metric + (0 if llr <= 0 else llr)\n",
        "\n",
        "                new_paths.extend([(path0, metric0), (path1, metric1)])\n",
        "\n",
        "            # Sort and prune paths\n",
        "            new_paths = sorted(new_paths, key=lambda x: x[1])  # Sort by path metric\n",
        "            active_paths = new_paths[:L]  # Keep only L best paths\n",
        "\n",
        "        # Select best path\n",
        "        best_path, _ = active_paths[0]\n",
        "\n",
        "        decoded_bits = np.array(best_path)  # Convert to numpy array\n",
        "        return decoded_bits[:K]  # Extract information bits\n",
        "\n",
        "    def _calculate_llr(self, received_symbol):\n",
        "        # Calculate Log-Likelihood Ratio (LLR)\n",
        "        # This is a placeholder and needs to be replaced with your actual LLR calculation logic\n",
        "        # Example: For AWGN channel\n",
        "        # llr = 2 * received_symbol / (noise_variance ** 2)\n",
        "        # ...\n",
        "        return 0 # Placeholder, replace with actual calculation\n",
        "\n",
        "class PolarCodeSimulation:\n",
        "    def __init__(self, block_length, info_bits, snr_range, hidden_layers, learning_rate, epochs, batch_size, list_sizes=[1, 8, 16]):\n",
        "   # def __init__(self, block_length=128, info_bits=64, snr_range=np.linspace(0, 10, 11), hidden_layers=[128, 256, 128], learning_rate=1e-3, epochs=100, batch_size=64):\n",
        "        self.block_length = block_length\n",
        "        self.info_bits = info_bits\n",
        "        self.snr_range = snr_range\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.list_sizes = list_sizes\n",
        "        self.ber_traditional = {list_size: [] for list_size in list_sizes}  # Store BER for each list size\n",
        "        self.bler_traditional = {list_size: [] for list_size in list_sizes}  # Store BLER for each list size\n",
        "        self.polar_code_gen = PolarCodeGenerator(N=block_length, K=info_bits)\n",
        "        self.channel_simulator = ChannelSimulator()\n",
        "        self.scl_decoder = SCLDecoder()  # Assuming you have an SCLDecoder class\n",
        "        self.ber_traditional = []\n",
        "        self.ber_ml = []\n",
        "        self.bler_traditional = []\n",
        "        self.bler_ml = []\n",
        "\n",
        "    def evaluate_performance(self, ml_model):\n",
        "        num_trials = 1000  # Number of simulations for each SNR\n",
        "        ber_traditional_list = {list_size: [] for list_size in self.list_sizes}\n",
        "        ber_ml_list = []\n",
        "        bler_traditional_list = {list_size: [] for list_size in self.list_sizes}\n",
        "        bler_ml_list = []\n",
        "        info_indices = self.polar_code_gen.generate_polar_code_matrix()  # Get info bit indices\n",
        "\n",
        "        for snr in self.snr_range:\n",
        "            for list_size in self.list_sizes:  # Iterate over the list sizes\n",
        "                ber_traditional = 0\n",
        "                ber_ml = 0\n",
        "                bler_traditional = 0\n",
        "                bler_ml = 0\n",
        "                scl_decoder = SCLDecoder(list_size=list_size)  # Create SCLDecoder with current list size\n",
        "\n",
        "                for _ in range(num_trials):\n",
        "                    info_bits = np.random.randint(2, size=self.info_bits)\n",
        "                    encoded_bits = self.polar_code_gen.encode(info_bits)\n",
        "                    received_signal = self.channel_simulator.transmit(encoded_bits, snr)\n",
        "\n",
        "                    # Traditional Decoding (SCL)\n",
        "                    decoded_bits_traditional = scl_decoder.decode(received_signal, info_indices, self.block_length)\n",
        "\n",
        "                    # ML Decoding\n",
        "                    received_signal_tensor = torch.tensor(received_signal, dtype=torch.float32).to(ml_model.device)\n",
        "                    decoded_bits_ml = ml_model(received_signal_tensor).cpu().detach().numpy()\n",
        "                    decoded_bits_ml = (decoded_bits_ml > 0.5).astype(int)\n",
        "\n",
        "                    # Calculate Errors\n",
        "                    ber_traditional += np.sum(np.abs(info_bits - decoded_bits_traditional)) / self.info_bits\n",
        "                    ber_ml += np.sum(np.abs(info_bits - decoded_bits_ml)) / self.info_bits\n",
        "                    bler_traditional += int(np.any(info_bits != decoded_bits_traditional))\n",
        "                    bler_ml += int(np.any(info_bits != decoded_bits_ml))\n",
        "\n",
        "                # Average Errors over Trials and store in the dictionaries\n",
        "                ber_traditional_list[list_size].append(ber_traditional / num_trials)\n",
        "                ber_ml_list.append(ber_ml / num_trials)\n",
        "                bler_traditional_list[list_size].append(bler_traditional / num_trials)\n",
        "                bler_ml_list.append(bler_ml / num_trials)\n",
        "\n",
        "        # Update simulation results\n",
        "        self.ber_traditional = ber_traditional_list\n",
        "        self.ber_ml = ber_ml_list\n",
        "        self.bler_traditional = bler_traditional_list\n",
        "        self.bler_ml = bler_ml_list\n",
        "\n",
        "        return ber_traditional_list, ber_ml_list, bler_traditional_list, bler_ml_list\n",
        "\n",
        "    def run_simulation(self):\n",
        "        # Training\n",
        "        model = MLPolarDecoder(input_size=self.block_length, hidden_layers=self.hidden_layers, output_size=self.info_bits)\n",
        "        trainer = MLTrainer(model, learning_rate=self.learning_rate)\n",
        "        X_train, y_train = trainer.generate_training_data(num_samples=1000, block_length=self.block_length, snr_range=self.snr_range)\n",
        "        train_losses, val_losses = trainer.train(X_train, y_train, epochs=self.epochs, batch_size=self.batch_size)\n",
        "\n",
        "        # Evaluation and Plotting\n",
        "        self.evaluate_performance(trainer.model)  # Evaluate performance to update BER/BLER values\n",
        "        self.plot_training_metrics(train_losses, val_losses, self.snr_range, self.ber_traditional, self.ber_ml, self.bler_traditional, self.bler_ml)  # Plot results\n",
        "\n",
        "    #def plot_training_metrics(self, train_losses, val_losses, snr_range, ber_traditional, ber_ml, bler_traditional, bler_ml):\n",
        "    #    plt.figure(figsize=(15, 10))\n",
        "    def plot_training_metrics(self, train_losses, val_losses, snr_range, ber_traditional, ber_ml, bler_traditional, bler_ml, y_true, y_pred):  # Added y_true, y_pred\n",
        "   # def plot_ber_bler(self, snr_range, ber_traditional, ber_ml, bler_traditional, bler_ml):\n",
        "      #  plt.figure(figsize=(12, 6))\n",
        "\n",
        "     def plot_training_validation_loss(self, train_losses, val_losses):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "        plt.title('Decoder Performance Confusion Matrix')\n",
        "        plt.xlabel(\"Predicted Labels\")\n",
        "        plt.ylabel(\"True Labels\")\n",
        "        plt.show()\n",
        "\n",
        "def plot_ber_bler(self, snr_range, ber_traditional_awgn, ber_ml_awgn, bler_traditional_awgn, bler_ml_awgn,\n",
        "                      ber_traditional_rayleigh, ber_ml_rayleigh, bler_traditional_rayleigh, bler_ml_rayleigh):\n",
        "\n",
        "    # Function to plot BER/BLER for a single channel type\n",
        "    def plot_channel(snr_range, ber_traditional, ber_ml, bler_traditional, bler_ml, channel_name):\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot BER\n",
        "        plt.subplot(1, 2, 1)\n",
        "        for list_size in self.list_sizes:\n",
        "            plt.semilogy(snr_range, ber_traditional.get(list_size, []), label=f'Traditional (List Size: {list_size})')\n",
        "        plt.semilogy(snr_range, ber_ml, label='ML (BER)')\n",
        "        plt.title(f'Bit Error Rate Comparison ({channel_name})')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Bit Error Rate')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot BLER\n",
        "        plt.subplot(1, 2, 2)\n",
        "        for list_size in self.list_sizes:\n",
        "            plt.semilogy(snr_range, bler_traditional.get(list_size, []), label=f'Traditional (List Size: {list_size})')\n",
        "        plt.semilogy(snr_range, bler_ml, label='ML (BLER)')\n",
        "        plt.title(f'Block Error Rate Comparison ({channel_name})')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Block Error Rate')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Plot for AWGN\n",
        "    plot_channel(snr_range, ber_traditional_awgn, ber_ml_awgn, bler_traditional_awgn, bler_ml_awgn, 'AWGN')\n",
        "\n",
        "    # Plot for Rayleigh\n",
        "    plot_channel(snr_range, ber_traditional_rayleigh, ber_ml_rayleigh, bler_traditional_rayleigh, bler_ml_rayleigh, 'Rayleigh')\n",
        "\n",
        "# Example usage:\n",
        "#simulation = PolarCodeSimulation()\n",
        "#simulation.run_simulation()\n",
        "\n",
        "class ListDecoder:\n",
        "    def __init__(self, list_size=8):\n",
        "        self.list_size = list_size\n",
        "\n",
        "    def decode(self, received_signal):\n",
        "        # Placeholder for list decoding algorithm\n",
        "        # Implement Tal-Vardy list decoding\n",
        "        pass\n",
        "\n",
        "class ChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def transmit(self, signal, snr):\n",
        "        if self.channel_type == 'AWGN':\n",
        "            return self.awgn_channel(signal, snr)\n",
        "        elif self.channel_type == 'Rayleigh':\n",
        "            return self.rayleigh_fading_channel(signal, snr)\n",
        "\n",
        "    def awgn_channel(self, signal, snr):\n",
        "        noise_std = 10 ** (-snr/20)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        return signal + noise\n",
        "\n",
        "    def rayleigh_fading_channel(self, signal, snr):\n",
        "        fading_coeff = np.random.rayleigh(scale=1, size=signal.shape)\n",
        "        noisy_signal = fading_coeff * signal\n",
        "        noise_std = 10 ** (-snr/20)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        return noisy_signal + noise\n",
        "\n",
        "class MLPolarDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        super(MLPolarDecoder, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        prev_size = input_size\n",
        "        for hidden_size in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.3))\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, output_size))  # Output layer with correct size\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class PolarCodeTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        self.model = model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "\n",
        "   # def train(self, X_train, y_train, epochs=50, batch_size=64):\n",
        "\n",
        "def train(self, X_train, y_train, epochs=100, batch_size=32):\n",
        "    # 1. Split data using train_test_split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "    train_losses = []\n",
        "    val_losses = []  # Initialize list for validation losses\n",
        "\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x = batch_x.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(batch_x)\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        self.scheduler.step(avg_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
        "\n",
        "        # Calculate validation loss\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = self.model(X_val.to(self.device))\n",
        "            val_loss = self.criterion(val_outputs, y_val.to(self.device))\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    return train_losses, val_losses # The return is outside the loop\n",
        "\n",
        "class MLPolarDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        super(MLPolarDecoder, self).__init__()\n",
        "        layers = []\n",
        "      #  self.to(self.device)  # Move the model to the device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Add this line\n",
        "\n",
        "        # Dynamic layer creation\n",
        "        prev_size = input_size\n",
        "        for hidden_size in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            # Change the num_features to 128:\n",
        "           # layers.append(nn.BatchNorm1d(128))  # num_features=128\n",
        "            layers.append(nn.InstanceNorm1d(128))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.3))\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.to(self.device)  # Move the model to the device\n",
        "\n",
        "           # Change the num_features to 1:\n",
        "       # layers.append(nn.BatchNorm1d(1))  # num_features=1\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = x.to(self.device)  # Move input to the device\n",
        "         x = torch.unsqueeze(x, 1)  # Add a dimension to make it 2D\n",
        "        # ... your model calculations ...\n",
        "         output = self.model(x) # Assuming self.model is your sequential model\n",
        "         return output\n",
        "       # return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "class MLTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        self.model = model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = self.model.device # Add this line\n",
        "        self.model.to(self.device)\n",
        "      #  input_tensor = input_tensor.to(self.device)\n",
        "        # Loss and Optimizer\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "\n",
        "    def generate_training_data(self, num_samples, block_length, snr_range):\n",
        "        # Generate synthetic polar code training data\n",
        "         # Generate synthetic polar code training data\n",
        "           X_train = []\n",
        "           y_train = []\n",
        "\n",
        "           polar_code_gen = PolarCodeGenerator(N=block_length, K=block_length // 2)  # Assuming K = block_length // 2\n",
        "\n",
        "           for snr in snr_range:\n",
        "               for _ in range(num_samples):\n",
        "                   # Generate random information bits\n",
        "                   info_bits = np.random.randint(2, size=block_length // 2)\n",
        "\n",
        "                   # Polar code encoding\n",
        "                   encoded_bits = polar_code_gen.encode(info_bits)  # Encode the information bits\n",
        "\n",
        "                   # Channel simulation\n",
        "                   noisy_signal = self._apply_channel_noise(encoded_bits, snr)\n",
        "\n",
        "                   X_train.append(noisy_signal)\n",
        "                   y_train.append(info_bits)  # Append the original information bits\n",
        "\n",
        "           return (torch.FloatTensor(X_train),\n",
        "                   torch.FloatTensor(y_train))\n",
        "\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=10, batch_size=32):\n",
        "        # 1. Split data using train_test_split\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42  # Adjust test_size and random_state as needed\n",
        "        )\n",
        "        train_losses = []\n",
        "        val_losses = []  # Initialize list for validation losses\n",
        "\n",
        "\n",
        "        # Data preparation\n",
        "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Epoch training\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch_x, batch_y in dataloader:\n",
        "                # Move to device\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "\n",
        "                # Zero gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(batch_x)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Optimize\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            # Average epoch loss\n",
        "            avg_loss = epoch_loss / len(dataloader)\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(avg_loss)\n",
        "\n",
        "            # Logging\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
        "\n",
        "            # Calculate validation loss\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val.to(self.device))\n",
        "                val_loss = self.criterion(val_outputs, y_val.to(self.device))\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "        print(f\"Returning: {train_losses}, {val_losses}\")  # Print before return statement\n",
        "        return train_losses, val_losses  # Return both list\n",
        "\n",
        "    def _polar_encode(self, bits):\n",
        "        # Simplified polar encoding\n",
        "        # Implement actual polar encoding logic\n",
        "        return bits\n",
        "\n",
        "    def _apply_channel_noise(self, signal, snr):\n",
        "        # AWGN channel simulation\n",
        "        noise_std = 10 ** (-snr/20)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        return signal + noise\n",
        "\n",
        "def main():\n",
        "    # Hyperparameters\n",
        "    BLOCK_LENGTH = 128\n",
        "    INFO_BITS = 64\n",
        "    LEARNING_RATE = 1e-3\n",
        "    EPOCHS = 10\n",
        "    BATCH_SIZE = 64\n",
        "    SNR_RANGE = np.linspace(0, 10, 5)  # Define SNR_RANGE here\n",
        "    LIST_SIZES = [1, 8, 16]  # Define your desired list sizes\n",
        "    hidden_layers = [128, 256, 128]\n",
        "\n",
        "    # Initialize Model\n",
        "    model = MLPolarDecoder(\n",
        "        input_size=BLOCK_LENGTH,\n",
        "        hidden_layers=hidden_layers,\n",
        "        output_size=INFO_BITS\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = MLTrainer(model, learning_rate=LEARNING_RATE)\n",
        "\n",
        "    # Generate Training Data\n",
        "    X_train, y_train = trainer.generate_training_data(\n",
        "        num_samples=1000,\n",
        "        block_length=BLOCK_LENGTH,\n",
        "        snr_range=SNR_RANGE  # Pass SNR_RANGE here\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "        train_losses, val_losses = trainer.train(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "    except ValueError:\n",
        "        print(\"Warning: trainer.train() returned more than two values. Using first two.\")\n",
        "        return_values = trainer.train(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "        train_losses, val_losses = return_values[:2]  # Take the first two elements\n",
        "\n",
        "     # Generate y_true and y_pred using your trained model and data\n",
        "     # ... (Add code here to generate y_true and y_pred)\n",
        "    # Example:\n",
        "    y_true = np.random.randint(0, 2, size=100)  # Example with random binary labels\n",
        "    y_pred = np.random.randint(0, 2, size=100)  # Example with random binary predictions\n",
        "    y_true = y_train\n",
        "    y_pred = X_train\n",
        "\n",
        "       # Visualize Training\n",
        "        # Visualize Training and Results\n",
        "    #simulation = PolarCodeSimulation(...)\n",
        "    simulation = PolarCodeSimulation(\n",
        "        block_length=BLOCK_LENGTH,  # Provide values for all the arguments\n",
        "        info_bits=INFO_BITS,\n",
        "        snr_range=SNR_RANGE,\n",
        "        hidden_layers=hidden_layers,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        list_sizes=LIST_SIZES\n",
        "    )\n",
        "\n",
        "    # Get results (ber & bler)\n",
        "    results = simulation.evaluate_performance(trainer.model)\n",
        "\n",
        "    # --- Key changes here for passing BER and BLER to plotting ---\n",
        "    simulation.plot_training_metrics(train_losses, val_losses, SNR_RANGE,\n",
        "                                       results['AWGN']['ber_traditional'], results['AWGN']['ber_ml'],\n",
        "                                       results['AWGN']['bler_traditional'], results['AWGN']['bler_ml'],\n",
        "                                       y_true, y_pred)\n",
        "\n",
        "    simulation.plot_ber_bler(SNR_RANGE,\n",
        "                              results['AWGN']['ber_traditional'], results['AWGN']['ber_ml'],\n",
        "                              results['AWGN']['bler_traditional'], results['AWGN']['bler_ml'],\n",
        "                              results['Rayleigh']['ber_traditional'], results['Rayleigh']['ber_ml'],\n",
        "                              results['Rayleigh']['bler_traditional'], results['Rayleigh']['bler_ml'])\n",
        "\n",
        "    simulation.plot_confusion_matrix(y_true, y_pred)\n",
        "    #simulation = PolarCodeSimulation(block_length=BLOCK_LENGTH, info_bits=INFO_BITS,\n",
        "                                       # snr_range=SNR_RANGE, hidden_layers=hidden_layers,\n",
        "                                        #learning_rate=LEARNING_RATE, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                                        #list_sizes=LIST_SIZES)\n",
        "\n",
        "   # simulation.plot_training_metrics(train_losses, val_losses, SNR_RANGE, [], [], [], [], y_true, y_pred)\n",
        "    #simulation.plot_ber_bler(SNR_RANGE, [], [], [], [])\n",
        "    #simulation.plot_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Visualize Training\n",
        "    #Visualize Training\n",
        "   # simulation = PolarCodeSimulation(block_length=BLOCK_LENGTH, info_bits=INFO_BITS,\n",
        "                                #     snr_range=SNR_RANGE, hidden_layers=hidden_layers,\n",
        "                                 #    learning_rate=LEARNING_RATE, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                                  #   list_sizes=LIST_SIZES)\n",
        "    #simulation.plot_training_metrics(train_losses, val_losses, SNR_RANGE, [], [], [], [])\n",
        "    #simulation.plot_training_metrics(train_losses, val_losses, SNR_RANGE, [], [], [], [], y_true, y_pred)  # Add y_true, y_pred\n",
        "    #simulation.plot_ber_bler(SNR_RANGE, [], [], [], [])  # Pass empty lists as placeholders for BER/BLER\n",
        "    #simulation.plot_confusion_matrix(y_true, y_pred)  # Call with appropriate y_true, y_pred\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NaWpSCjP_XxA",
        "outputId": "3a8a883d-9446-42b8-8bca-b2fea8e17d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "running_mean should contain 1 elements not 128",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-ace7ecdaa137>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;31m#simulation.plot_confusion_matrix(y_true, y_pred)  # Call with appropriate y_true, y_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-ace7ecdaa137>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning: trainer.train() returned more than two values. Using first two.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-ace7ecdaa137>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, epochs, batch_size)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-ace7ecdaa137>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    556\u001b[0m          \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add a dimension to make it 2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# ... your model calculations ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m          \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Assuming self.model is your sequential model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m          \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m        \u001b[0;31m# return self.model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 1 elements not 128"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "TVGJRp4o_4_Z",
        "outputId": "708afcd1-02d3-42a9-c538-24088db27f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a72ac684-5ecf-4b4f-97a7-a0c1f2220367\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a72ac684-5ecf-4b4f-97a7-a0c1f2220367\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PolaRNN.py to PolaRNN (1).py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mport numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split  # If using train_test_split\n",
        "\n",
        "class CRC:\n",
        "    def __init__(self, polynomial=0b10011011, order=7):\n",
        "        self.polynomial = polynomial\n",
        "        self.order = order\n",
        "\n",
        "    def generate_crc(self, data):\n",
        "        data_with_zeros = np.concatenate([data, np.zeros(self.order, dtype=int)])\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            if data_with_zeros[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    data_with_zeros[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        return data_with_zeros[-self.order:]\n",
        "\n",
        "    def verify_crc(self, data, received_crc):\n",
        "        full_data = np.concatenate([data, received_crc])\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            if full_data[i] == 1:\n",
        "                for j in range(self.order + 1):\n",
        "                    full_data[i+j] ^= ((self.polynomial >> j) & 1)\n",
        "\n",
        "        return np.all(full_data[-self.order:] == 0)\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N=128, K=64):\n",
        "        self.N = N      # Block length\n",
        "        self.K = K      # Information bits\n",
        "        self.R = K/N    # Code Rate\n",
        "        self.crc = CRC()\n",
        "        self.design_SNR = 0\n",
        "\n",
        "    def polar_transform(self, u):\n",
        "        n = int(np.log2(len(u)))\n",
        "        for i in range(n):\n",
        "            u1 = np.zeros(len(u))\n",
        "            for j in range(len(u) // 2):\n",
        "                u1[2*j] = np.mod(u[j] + u[j + len(u)//2], 2)\n",
        "                u1[2*j + 1] = u[j + len(u)//2]\n",
        "            u = u1\n",
        "        return u\n",
        "\n",
        "    def generate_polar_code_matrix(self):\n",
        "        def bhattacharyya_parameter(W, n):\n",
        "            if n == 0:\n",
        "                return W\n",
        "            W_used = bhattacharyya_parameter(W, n-1)\n",
        "            W_transform = 2 * W_used**2 - W_used**4\n",
        "            return W_transform\n",
        "\n",
        "        channel_capacities = []\n",
        "        for i in range(self.N):\n",
        "            W = 0.5  # Binary symmetric channel\n",
        "            capacity = bhattacharyya_parameter(W, int(np.log2(self.N)))\n",
        "            channel_capacities.append(capacity)\n",
        "\n",
        "        sorted_indices = np.argsort(channel_capacities)\n",
        "        info_indices = sorted_indices[self.N - self.K:]\n",
        "\n",
        "        return info_indices\n",
        "\n",
        "    def systematic_polar_encode(self, info_bits):\n",
        "        \"\"\"Systematic Polar Encoding\"\"\"\n",
        "        crc_bits = self.crc.generate_crc(info_bits)  # Calculate CRC\n",
        "        full_info = np.concatenate([info_bits, crc_bits])  # Combine info and CRC\n",
        "\n",
        "        # Initialize the codeword with zeros\n",
        "        encoded_bits = np.zeros(self.N, dtype=int)\n",
        "\n",
        "        # Get the indices for information bits\n",
        "        info_indices = self.generate_polar_code_matrix()\n",
        "\n",
        "        # Place the full information (info + CRC) into the codeword\n",
        "        #  encoded_bits[info_indices] = full_info\n",
        "       # Adjust info_indices to accommodate full_info length\n",
        "        info_indices = info_indices[:len(full_info)]\n",
        "\n",
        "        # Apply polar transform\n",
        "        encoded_bits = self.polar_transform(encoded_bits)\n",
        "\n",
        "        return encoded_bits  # Return the encoded bits\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        crc_bits = self.crc.generate_crc(info_bits)\n",
        "        full_info = np.concatenate([info_bits, crc_bits])\n",
        "        encoded_bits = self._polar_encode(full_info)\n",
        "        return encoded_bits\n",
        "\n",
        "    def _polar_encode(self, bits):\n",
        "        n = int(np.log2(self.N))\n",
        "        x = np.zeros(self.N, dtype=int)\n",
        "        x[:len(bits)] = bits\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(0, self.N, 2**(i+1)):\n",
        "                for k in range(2**i):\n",
        "                    u = x[j+k]\n",
        "                    v = x[j+k+2**i]\n",
        "                    x[j+k] = (u + v) % 2\n",
        "                    x[j+k+2**i] = v\n",
        "\n",
        "        return x\n",
        "\n",
        "class ChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        self.channel_type = channel_type\n",
        "\n",
        "    def transmit(self, signal, snr):\n",
        "        if self.channel_type == 'AWGN':\n",
        "            return self.awgn_channel(signal, snr)\n",
        "        elif self.channel_type == 'Rayleigh':\n",
        "            return self.rayleigh_fading_channel(signal, snr)\n",
        "\n",
        "    def awgn_channel(self, signal, snr):\n",
        "        noise_std = 10 ** (-snr/20)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        return signal + noise\n",
        "\n",
        "    def rayleigh_fading_channel(self, signal, snr):\n",
        "        fading_coeff = np.random.rayleigh(scale=1, size=signal.shape)\n",
        "        noisy_signal = fading_coeff * signal\n",
        "        noise_std = 10 ** (-snr/20)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        return noisy_signal + noise\n",
        "\n",
        "class MLPolarDecoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        super(MLPolarDecoder, self).__init__()\n",
        "        layers = []\n",
        "\n",
        "        prev_size = input_size\n",
        "        for hidden_size in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.3))\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class MLTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3, block_length=128):\n",
        "        self.model = model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.block_length = block_length\n",
        "        self.polar_code_gen = PolarCodeGenerator(N=block_length, K=block_length // 2)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "    def generate_training_data(self, num_samples, block_length, snr_range, save_path='my_dataset.csv'):  # Set a default save_path\n",
        "        X_train = []\n",
        "        y_train = []\n",
        "        for snr in snr_range:\n",
        "            for _ in range(num_samples):\n",
        "                info_bits = np.random.randint(2, size=block_length // 2)\n",
        "                encoded_bits = self._polar_encode(info_bits)\n",
        "                noisy_signal = self._apply_channel_noise(encoded_bits, snr)\n",
        "                X_train.append(noisy_signal)\n",
        "                y_train.append(info_bits)\n",
        "\n",
        "        X_train = torch.FloatTensor(X_train)\n",
        "        y_train = torch.FloatTensor(y_train)\n",
        "\n",
        "       # if save_path:\n",
        "            # Choose your preferred saving method:\n",
        "\n",
        "            # 1. Using np.savez (as .npz file)\n",
        "          #  np.savez(save_path, X_train=X_train.numpy(), y_train=y_train.numpy())\n",
        "          #  print(f\"Dataset saved to {save_path} as .npz\")\n",
        "\n",
        "            # 2. Using np.savetxt (as .csv file)\n",
        "             #np.savetxt(save_path, np.hstack([X_train.numpy(), y_train.numpy()]), delimiter=',')\n",
        "             #print(f\"Dataset saved to {save_path} as .csv\")\n",
        "\n",
        "        if save_path:\n",
        "            # Save as CSV using np.savetxt\n",
        "            np.savetxt(save_path, np.hstack([X_train.numpy(), y_train.numpy()]), delimiter=',')\n",
        "            print(f\"Dataset saved to {save_path} as .csv\")\n",
        "\n",
        "        return X_train, y_train\n",
        "\n",
        "        return torch.FloatTensor(X_train), torch.FloatTensor(y_train)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=200, batch_size=32):\n",
        "        train_losses = []\n",
        "\n",
        "        # Split data using train_test_split\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        val_losses = []  # Initialize val_losses\n",
        "        dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0\n",
        "            for batch_x, batch_y in dataloader:\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            avg_loss = epoch_loss / len(dataloader)\n",
        "            train_losses.append(avg_loss)\n",
        "\n",
        "            # Calculate validation loss\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val.to(self.device))\n",
        "                val_loss = self.criterion(val_outputs, y_val.to(self.device))\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            self.scheduler.step(avg_loss)\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses  # Return both train_losses and val_losses\n",
        "\n",
        "    def _polar_encode(self, bits):\n",
        "        n = int(np.log2(self.polar_code_gen.N))\n",
        "        x = np.zeros(self.polar_code_gen.N, dtype=int)\n",
        "        x[:self.polar_code_gen.K] = bits[:self.polar_code_gen.K]\n",
        "        for i in range(n):\n",
        "            for j in range(0, self.polar_code_gen.N, 2**(i+1)):\n",
        "                for k in range(2**i):\n",
        "                    u = x[j+k]\n",
        "                    v = x[j+k+2**i]\n",
        "                    x[j+k] = (u + v) % 2\n",
        "                    x[j+k+2**i] = v\n",
        "        return x\n",
        "\n",
        "    def _apply_channel_noise(self, signal, snr):\n",
        "        noise_std = 10 ** (-snr/20)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        return signal + noise\n",
        "\n",
        "class PolarCodeSimulation:\n",
        "    def __init__(self, block_length, info_bits, learning_rate, epochs, batch_size, hidden_layers, channel_type='AWGN'):\n",
        "        self.snr_range = np.linspace(0, 10, 10)\n",
        "        self.ber_traditional = []\n",
        "        self.ber_ml = []\n",
        "        self.bler_traditional = []\n",
        "        self.bler_ml = []\n",
        "        self.block_length = block_length\n",
        "        self.info_bits = info_bits\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.polar_code_gen = PolarCodeGenerator(N=block_length, K=info_bits)\n",
        "        self.channel_simulator = ChannelSimulator(channel_type=channel_type)  # Initialize with channel type\n",
        "        self.channel_type = channel_type\n",
        "        self.ber_traditional_awgn = []  # Store BER for traditional decoder (AWGN)\n",
        "        self.ber_ml_awgn = []  # Store BER for ML decoder (AWGN)\n",
        "        self.bler_traditional_awgn = []  # Store BLER for traditional decoder (AWGN)\n",
        "        self.bler_ml_awgn = []  # Store BLER for ML decoder (AWGN)\n",
        "\n",
        "        self.ber_traditional_rayleigh = []  # Store BER for traditional decoder (Rayleigh)\n",
        "        self.ber_ml_rayleigh = []  # Store BER for ML decoder (Rayleigh)\n",
        "        self.bler_traditional_rayleigh = []  # Store BLER for traditional decoder (Rayleigh)\n",
        "        self.bler_ml_rayleigh = []  # Store BLER for ML decoder (Ray\n",
        "\n",
        "def run_simulation(self):\n",
        "    # 1. Training\n",
        "       model = MLPolarDecoder(input_size=self.block_length, hidden_layers=self.hidden_layers, output_size=self.info_bits)\n",
        "       trainer = MLTrainer(model, learning_rate=self.learning_rate) # Make sure MLTrainer is defined\n",
        "       X_train, y_train = trainer.generate_training_data(num_samples=1000, block_length=self.block_length, snr_range=self.snr_range)\n",
        "       train_losses, val_losses = trainer.train(X_train, y_train, epochs=self.epochs, batch_size=self.batch_size)\n",
        "    # 2. Evaluation for AWGN and Rayleigh channels\n",
        "       self.evaluate_performance(self.trainer.model, 'AWGN')  # Use self.trainer.model\n",
        "       self.evaluate_performance(self.trainer.model, 'Rayleigh') # Use self.trainer.model\n",
        "\n",
        "\n",
        "        # 3. Plotting\n",
        "       self.plot_training_metrics(train_losses, val_losses)\n",
        "\n",
        "\n",
        "          # Bind plot_training_metrics to the instance\n",
        "       plot_func = self.plot_training_metrics.__get__(self)\n",
        "\n",
        "        # Call plot_training_metrics within the run_simulation method\n",
        "       plot_func(train_losses, val_losses, snr_range)  # Pass snr_range\n",
        "\n",
        "         # Call the bound function with required arguments\n",
        "       plot_func(\n",
        "            train_losses,\n",
        "            val_losses,\n",
        "            self.snr_range,  # Pass self.snr_range here\n",
        "            self.ber_traditional,\n",
        "            self.ber_ml,\n",
        "            self.bler_traditional,\n",
        "            self.bler_ml\n",
        "        )  # Pass all required instance variables\n",
        "\n",
        "def evaluate_performance(self, ml_model, channel_type):\n",
        "\n",
        "          num_trials = 1000\n",
        "          ber_traditional_list = []\n",
        "          ber_ml_list = []\n",
        "          bler_traditional_list = []\n",
        "          bler_ml_list = []\n",
        "\n",
        "           # Initialize BER/BLER lists for the current channel type\n",
        "          if channel_type == 'AWGN':\n",
        "            ber_traditional_list = self.ber_traditional_awgn\n",
        "            ber_ml_list = self.ber_ml_awgn\n",
        "            bler_traditional_list = self.bler_traditional_awgn\n",
        "            bler_ml_list = self.bler_ml_awgn\n",
        "          else:  # Rayleigh\n",
        "            ber_traditional_list = self.ber_traditional_rayleigh\n",
        "            ber_ml_list = self.ber_ml_rayleigh\n",
        "            bler_traditional_list = self.bler_traditional_rayleigh\n",
        "            bler_ml_list = self.bler_ml_rayleigh\n",
        "\n",
        "          for snr in self.snr_range:\n",
        "            ber_traditional = 0\n",
        "            ber_ml = 0\n",
        "            bler_traditional = 0\n",
        "            bler_ml = 0\n",
        "\n",
        "            for _ in range(num_trials):\n",
        "                # Generate random information bits\n",
        "                info_bits = np.random.randint(2, size=self.info_bits)\n",
        "\n",
        "                # Traditional Encoding and Decoding\n",
        "                encoded_bits = self.polar_code_gen.encode(info_bits)\n",
        "                received_signal = self.channel_simulator.transmit(encoded_bits, snr)\n",
        "                # Replace with your traditional decoding logic\n",
        "                decoded_bits_traditional = self.traditional_decoder(received_signal)\n",
        "\n",
        "                # ML Decoding\n",
        "                received_signal_tensor = torch.tensor(received_signal, dtype=torch.float32).to(trainer.device)\n",
        "                decoded_bits_ml = ml_model(received_signal_tensor).cpu().detach().numpy()\n",
        "                decoded_bits_ml = (decoded_bits_ml > 0.5).astype(int)  # Threshold for binary decisions\n",
        "\n",
        "                # Calculate Errors\n",
        "                ber_traditional += np.sum(np.abs(info_bits - decoded_bits_traditional)) / self.info_bits\n",
        "                ber_ml += np.sum(np.abs(info_bits - decoded_bits_ml)) / self.info_bits\n",
        "                bler_traditional += int(np.any(info_bits != decoded_bits_traditional))\n",
        "                bler_ml += int(np.any(info_bits != decoded_bits_ml))\n",
        "\n",
        "            # Average Errors over Trials\n",
        "           # ber_traditional_list.append(ber_traditional / num_trials)\n",
        "            #ber_ml_list.append(ber_ml / num_trials)\n",
        "            #bler_traditional_list.append(bler_traditional / num_trials)\n",
        "            #bler_ml_list.append(bler_ml / num_trials)\n",
        "\n",
        "        #return ber_traditional_list, ber_ml_list, bler_traditional_list, bler_ml_list\n",
        "\n",
        "            # Average Errors over Trials and store in the correct lists\n",
        "                 # Average Errors over Trials and store in the correct lists\n",
        "\n",
        "\n",
        "def traditional_decoder(self, received_signal):\n",
        "        # Placeholder for your traditional decoding logic\n",
        "        # Replace with your actual decoder (e.g., successive cancellation)\n",
        "        decoded_bits = np.zeros_like(received_signal)  # Replace with actual decoding\n",
        "        return decoded_bits\n",
        "\n",
        "        # Call plot_training_metrics within the run_simulation method\n",
        "      #  self.plot_training_metrics(train_losses, val_losses, ber_traditional, ber_ml, bler_traditional, bler_ml)\n",
        "          # Call plot_training_metrics within the run_simulation method\n",
        "        self.plot_training_metrics(train_losses, val_losses)  # No need to pass variables here\n",
        "\n",
        "   # Run for AWGN channel\n",
        "    #    run_simulation('AWGN')\n",
        "\n",
        "   # Run for Rayleigh channel\n",
        "      #  run_simulation('Rayleigh')\n",
        "     # Call plot_training_metrics within the run_simulation method\n",
        "        self.plot_training_metrics(\n",
        "            train_losses,\n",
        "            val_losses,\n",
        "            self.snr_range,  # Pass self.snr_range here\n",
        "            self.ber_traditional,\n",
        "            self.ber_ml,\n",
        "            self.bler_traditional,\n",
        "            self.bler_ml\n",
        "        )  # Pass all required instance variables\n",
        "    # ... (training and evaluation loop with channel_simulator)\n",
        "    # ... (plotting results)\n",
        "def run_simulation(self):\n",
        "        # ... (training)\n",
        "        # 1. Training\n",
        "        model = MLPolarDecoder(input_size=self.block_length, hidden_layers=self.hidden_layers, output_size=self.info_bits)\n",
        "        self.trainer = MLTrainer(model, learning_rate=self.learning_rate)  # Assign trainer to self.trainer\n",
        "        X_train, y_train = self.trainer.generate_training_data(num_samples=1000, block_length=self.block_length, snr_range=self.snr_range)\n",
        "        train_losses, val_losses = self.trainer.train(X_train, y_train, epochs=self.epochs, batch_size=self.batch_size)\n",
        "\n",
        "        #2. Evaluation for AWGN and Rayleigh channels\n",
        "        self.evaluate_performance(trainer.model, 'AWGN')\n",
        "        self.evaluate_performance(trainer.model, 'Rayleigh')\n",
        "        # 3. Plotting\n",
        "        # ... (Existing plotting code)\n",
        "def plot_training_metrics(self, train_losses, val_losses, snr_range, ber_traditional, ber_ml, bler_traditional, bler_ml):\n",
        "        # Figure 1: Training and Validation Loss\n",
        "        plt.figure(figsize=(15, 5))  # Corrected: Removed extra indent\n",
        "\n",
        "        plt.subplot(1, 2, 1)  # Training and Validation Loss subplot\n",
        "        plt.plot(train_losses, label='Training Loss')\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Optional: Add Confusion Matrix to Figure 1\n",
        "        plt.subplot(1, 2, 2)  # Confusion Matrix subplot\n",
        "        confusion_matrix = np.random.rand(2, 2)  # Replace with your actual confusion matrix\n",
        "        sns.heatmap(confusion_matrix, annot=True, cmap='Blues')\n",
        "        plt.title('Decoder Performance Confusion Matrix')\n",
        "        plt.xlabel(\"Predicted Labels\")\n",
        "        plt.ylabel(\"True Labels\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "           # Figure 2: Bit Error Rate (BER)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.semilogy(snr_range, ber_traditional_awgn, label='Traditional (AWGN)', marker='o')\n",
        "        plt.semilogy(snr_range, ber_ml_awgn, label='ML (AWGN)', marker='s')\n",
        "        plt.semilogy(snr_range, ber_traditional_rayleigh, label='Traditional (Rayleigh)', marker='^')\n",
        "        plt.semilogy(snr_range, ber_ml_rayleigh, label='ML (Rayleigh)', marker='x')\n",
        "        plt.title('Bit Error Rate (BER) vs. SNR')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('BER')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "             # Figure 3: Block Error Rate (BLER)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.semilogy(snr_range, bler_traditional_awgn, label='Traditional (AWGN)', marker='o')\n",
        "        plt.semilogy(snr_range, bler_ml_awgn, label='ML (AWGN)', marker='s')\n",
        "        plt.semilogy(snr_range, bler_traditional_rayleigh, label='Traditional (Rayleigh)', marker='^')\n",
        "        plt.semilogy(snr_range, bler_ml_rayleigh, label='ML (Rayleigh)', marker='x')\n",
        "        plt.title('Block Error Rate (BLER) vs. SNR')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('BLER')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "      #  plt.subplot(2, 2, 1)  # Training Loss subplot\n",
        "      #  plt.plot(train_losses, label='Training Loss')\n",
        "       # plt.title('Training Loss over Epochs')\n",
        "       # plt.xlabel('Epochs')\n",
        "        #plt.ylabel('Loss')\n",
        "        #plt.legend()\n",
        "\n",
        "        #plt.subplot(2, 2, 2)  # Validation Loss subplot\n",
        "        #plt.plot(val_losses, label='Validation Loss')\n",
        "        #plt.title('Validation Loss over Epochs')\n",
        "        #plt.xlabel('Epochs')\n",
        "        #plt.ylabel('Loss')\n",
        "        #plt.legend()\n",
        "        # Figure 2: Bit Error Rate and Block Error Rate\n",
        "\n",
        "\n",
        "        # Figure 2: Bit Error Rate, Block Error Rate, Confusion Matrix\n",
        "       # plt.figure(figsize=(15, 10))  # Increased figure size for 3x2 grid\n",
        "\n",
        "        #plt.subplot(2, 2, 1)  # Subplot for BER\n",
        "        #snr_range = np.linspace(0, 10, 10)\n",
        "        #ber_traditional = [10**(-snr/2) for snr in snr_range]\n",
        "        #ber_ml = [10**(-snr/3) for snr in snr_range]\n",
        "\n",
        "        #plt.semilogy(snr_range, ber_traditional, label='Traditional Decoder')\n",
        "        #plt.semilogy(snr_range, ber_ml, label='ML Decoder')\n",
        "        #plt.title('Bit Error Rate Comparison')\n",
        "        #plt.xlabel('SNR (dB)')\n",
        "        #plt.ylabel('Bit Error Rate')\n",
        "        #plt.legend()\n",
        "\n",
        "\n",
        "      #  plt.subplot(2, 2, 2)  # Subplot for BLER\n",
        "       # snr_range = np.linspace(0, 10, 10)  # Define SNR range\n",
        "        #bler_traditional = [10**(-snr/2) for snr in snr_range]  # Example BLER for traditional decoder\n",
        "        #bler_ml = [10**(-snr/3) for snr in snr_range]  # Example BLER for ML decoder\n",
        "\n",
        "        #plt.semilogy(snr_range, bler_traditional, label='Traditional Decoder')  # Plot BLER for traditional decoder\n",
        "        #plt.semilogy(snr_range, bler_ml, label='ML Decoder')  # Plot BLER for ML decoder\n",
        "        #plt.title('Block Error Rate Comparison')\n",
        "        #plt.xlabel('SNR (dB)')\n",
        "        #plt.ylabel('Block Error Rate')\n",
        "        #plt.legend()\n",
        "\n",
        "        #plt.subplot(2, 2, 3)  # Subplot for confusion matrix\n",
        "        #confusion_matrix = np.random.rand(2, 2)\n",
        "        #sns.heatmap(confusion_matrix, annot=True, cmap='Blues')\n",
        "        #plt.title('Decoder Performance Confusion Matrix')\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Example usage in main function\n",
        "\n",
        "    # Initialize Polar Code Generator\n",
        "    polar_code = PolarCodeGenerator(N=128, K=64)\n",
        "\n",
        "    # Generate random information bits\n",
        "    info_bits = np.random.randint(2, size=64)\n",
        "\n",
        "    # Encode using standard polar encoding\n",
        "    encoded_bits_standard = polar_code.encode(info_bits)\n",
        "\n",
        "    # Encode using systematic polar encoding\n",
        "    encoded_bits_systematic = polar_code.systematic_polar_encode(info_bits)\n",
        "\n",
        "    print(\"Standard Encoded Bits:\", encoded_bits_standard)\n",
        "    print(\"Systematic Encoded Bits:\", encoded_bits_systematic)\n",
        "\n",
        "\n",
        "\n",
        "    # Hyperparameters\n",
        "    BLOCK_LENGTH = 128\n",
        "    INFO_BITS = 64\n",
        "    LEARNING_RATE = 1e-3\n",
        "    EPOCHS = 200\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    # Model Architecture\n",
        "    hidden_layers = [128, 256, 128]\n",
        "    channel_type = 'AWGN'  # or 'Rayleigh'\n",
        "\n",
        "    simulation = PolarCodeSimulation(BLOCK_LENGTH, INFO_BITS, LEARNING_RATE, EPOCHS, BATCH_SIZE, hidden_layers)\n",
        "   # simulation.run_simulation()  # Call run_simulation on the instance\n",
        "\n",
        "      # Simulation for AWGN channel\n",
        "    simulation_awgn = PolarCodeSimulation(BLOCK_LENGTH, INFO_BITS, LEARNING_RATE, EPOCHS, BATCH_SIZE, hidden_layers, channel_type='AWGN')\n",
        "   # simulation_awgn.run_simulation()\n",
        "\n",
        "    # Simulation for Rayleigh channel\n",
        "    simulation_rayleigh = PolarCodeSimulation(BLOCK_LENGTH, INFO_BITS, LEARNING_RATE, EPOCHS, BATCH_SIZE, hidden_layers, channel_type='Rayleigh')\n",
        "    simulation_rayleigh.run_simulation()\n",
        "\n",
        "    # Initialize Model\n",
        "    model = MLPolarDecoder(\n",
        "        input_size=BLOCK_LENGTH,\n",
        "        hidden_layers=hidden_layers,\n",
        "        output_size=INFO_BITS\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = MLTrainer(model, learning_rate=LEARNING_RATE, block_length=BLOCK_LENGTH)  # Corrected: Passed block_length\n",
        "\n",
        "    # Generate Training Data\n",
        "    SNR_RANGE = np.linspace(0, 10, 5)\n",
        "    X_train, y_train = trainer.generate_training_data(\n",
        "        num_samples=30000,\n",
        "        block_length=BLOCK_LENGTH,\n",
        "        snr_range=SNR_RANGE\n",
        "    )\n",
        "\n",
        "     # Load the dataset (if needed)\n",
        "    try:\n",
        "        # ... (existing code for loading the dataset)\n",
        "        dataset = np.loadtxt('my_dataset.csv', delimiter=',')  # Assuming your dataset is saved as 'my_dataset.csv'\n",
        "        X_train_loaded = dataset[:, :-64]  # Assuming the last 64 columns are labels (y_train)\n",
        "        y_train_loaded = dataset[:, -64:]\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        X_train = torch.from_numpy(X_train_loaded).float()\n",
        "        y_train = torch.from_numpy(y_train_loaded).float()\n",
        "\n",
        "        print(\"Dataset loaded from file.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Generate the dataset if the file is not found\n",
        "        print(\"Dataset file not found. Generating new dataset...\")\n",
        "        X_train, y_train = trainer.generate_training_data(\n",
        "            num_samples=30000,\n",
        "            block_length=BLOCK_LENGTH,\n",
        "            snr_range=SNR_RANGE,\n",
        "            save_path='my_dataset.csv'  # Pass save_path here\n",
        "        )\n",
        "\n",
        "    # ... (rest of your main function)\n",
        "\n",
        "    # Training\n",
        "    train_losses, val_losses = trainer.train(  # Get val_losses from train method\n",
        "        X_train,\n",
        "        y_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Visualize Training Metrics\n",
        "    trainer.plot_training_metrics(train_losses, val_losses)  # Pass val_losses here\n",
        "    # Evaluation Loop\n",
        "    num_blocks = 1000  # Number of blocks to test\n",
        "    block_errors = 0  # Initialize block error counter\n",
        "    bit_errors = 0  # Initialize bit error counter\n",
        "    total_bits = 0  # Initialize total bits counte\n",
        "\n",
        "    for snr in SNR_RANGE:\n",
        "        for _ in range(num_blocks):\n",
        "            # Generate random information bits\n",
        "            info_bits = np.random.randint(2, size=INFO_BITS)\n",
        "\n",
        "            # Encode using the polar code generator\n",
        "            encoded_bits = polar_code.encode(info_bits)\n",
        "\n",
        "            # Transmit over the channel simulator\n",
        "            received_signal = channel_simulator.transmit(encoded_bits, snr)\n",
        "\n",
        "            # Decode using your ML model\n",
        "            decoded_bits = model(torch.tensor(received_signal).float().to(trainer.device))\n",
        "            decoded_bits = (decoded_bits > 0.5).int().cpu().numpy()\n",
        "\n",
        "            # Calculate block and bit errors\n",
        "            block_errors += not np.array_equal(info_bits, decoded_bits)\n",
        "\n",
        "            bit_errors_current_block = np.sum(np.abs(info_bits - decoded_bits))\n",
        "            bit_errors += bit_errors_current_block\n",
        "            total_bits += len(info_bits)\n",
        "\n",
        "\n",
        "        # Calculate and print block error rate for each SNR\n",
        "        bler = block_errors / num_blocks  # Calculate BLER\n",
        "        ber = bit_errors / total_bits # Calculate BER\n",
        "\n",
        "        print(f\"SNR: {snr:.2f} dB, BLER: {bler:.4f}, BER: {ber:.4f}\")\n",
        "        # In main function:\n",
        "        # Create and run the simulation\n",
        "    simulation = PolarCodeSimulation(block_length, info_bits, learning_rate, epochs, batch_size, hidden_layers, channel_type)\n",
        "    simulation.run_simulation()\n",
        "#simulation = PolarCodeSimulation(...)\n",
        "#simulation.run_simulation('AWGN')\n",
        "#simulation.run_simulation('Rayleigh')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8wrHV0D1Ab0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/polarML56725.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANGh2hBVmVzp",
        "outputId": "2d2454a7-b0d2-4dc5-9e4d-0bee2031b3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deea5752-16d5-4356-e8bc-e34a339e8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 29 15:49:07 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d64b9507-2efe-41aa-af09-3591d4c68d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "63EaCr71qi25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up potentially conflicting packages aggressively\n",
        "!pip uninstall -y numpy pandas scipy scikit-learn sklearn-pandas torch torchvision torchaudio fastai matplotlib tensorflow tensorflow-decision-forests tensorflow-text tf-keras numba cuml-cu12 cudf-cu12 dask-cuda distributed-ucxx-cu12 dask-cudf-cu12 raft-dask-cu12 ml-dtypes tensorboard\n",
        "\n",
        "# Install numpy first\n",
        "#!pip install numpy --no-cache-dir\n",
        "# Install compatible versions of numpy and scipy\n",
        "# As of recent updates, scipy 1.12.0 is compatible with numpy 1.26.4.\n",
        "# You might need to adjust these versions based on the current compatibility matrix\n",
        "# if this specific combination still causes issues.\n",
        "#!pip install numpy==1.26.4 --no-cache-dir\n",
        "#!pip install scipy==1.12.0 --no-cache-dir\n",
        "# Then install the rest of the necessary libraries\n",
        "# This is a more extensive list based on common dependencies in data science notebooks\n",
        "# Explicitly install compatible versions of core libraries first\n",
        "!pip install numpy==1.26.4 --no-cache-dir\n",
        "!pip install scipy==1.12.0 --no-cache-dir\n",
        "#!pip install scikit-learn --no-cache-dir # Let pip find a compatible scikit-learn version\n",
        "#!pip install scikit-learn==1.4.1 --no-cache-dir # Explicitly install a recent, compatible scikit-learn version\n",
        "!pip install \"scikit-learn>=1.4.0,<1.5.0\" --no-cache-dir\n",
        "# Then install the rest of the necessary libraries\n",
        "# Now install the rest of the necessary libraries\n",
        "# Do NOT use --upgrade on numpy or scipy to preserve the installed versions\n",
        "!pip install \\\n",
        "    pandas \\\n",
        "    sklearn-pandas \\\n",
        "    torch torchvision torchaudio \\\n",
        "    fastai \\\n",
        "    matplotlib \\\n",
        "    tensorflow tensorflow-decision-forests tensorflow-text tf-keras \\\n",
        "    numba \\\n",
        "    --no-cache-dir\n",
        "\n",
        "# Now import all necessary libraries\n",
        "import logging\n",
        "import numpy as np\n",
        "import scipy\n",
        "import math\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "# Keep the scikit-learn imports as they are needed later\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.special as sps\n",
        "import logging, traceback, sys\n",
        "\n",
        "\n",
        "# Check versions after installation and import\n",
        "# This will now show the versions that pip installed based on resolving dependencies\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"SciPy version: {scipy.__version__}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "\n",
        "\n",
        "# Fix: Corrected format string for datefmt\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        ")\n",
        "\n",
        "# Device Configuration\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸš€ Using Device: {DEVICE}\")\n",
        "\n",
        "# Configuration\n",
        "BLOCK_LENGTH = 128\n",
        "INFO_BITS = 64\n",
        "NUM_SAMPLES_TRAIN = 10000\n",
        "NUM_TRIALS_PERF = 1000\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3\n",
        "SNR_AWGN = np.linspace(0, 5, 11)\n",
        "LIST_SIZES = [1, 8, 16]\n",
        "\n",
        "\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N, K, crc_type='CRC-7'):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc_type = crc_type\n",
        "        self.crc_polynomials = {\n",
        "            'CRC-7': {'polynomial': [1, 1, 1, 0, 0, 1, 1], 'length': 7}\n",
        "        }\n",
        "\n",
        "    def generate_info_bits(self):\n",
        "        return np.random.randint(2, size=self.K)\n",
        "\n",
        "    def compute_crc(self, bits):\n",
        "        poly_info = self.crc_polynomials.get(self.crc_type)\n",
        "        if not poly_info:\n",
        "            raise ValueError(f\"Unsupported CRC type: {self.crc_type}\")\n",
        "\n",
        "        polynomial = poly_info['polynomial']\n",
        "        crc_length = poly_info['length']\n",
        "        message = bits.tolist() + [0] * crc_length\n",
        "        for i in range(len(message) - crc_length):\n",
        "            if message[i] == 1:\n",
        "                for j in range(crc_length + 1):\n",
        "                    message[i + j] ^= polynomial[j] if j < len(polynomial) else 0\n",
        "\n",
        "        return np.array(message[-crc_length:], dtype=int)\n",
        "\n",
        "    def polar_encode(self, info_bits):\n",
        "        crc_bits = self.compute_crc(info_bits)\n",
        "        extended_info_bits = np.concatenate([info_bits, crc_bits])\n",
        "        codeword = np.zeros(self.N, dtype=int)\n",
        "        codeword[: len(extended_info_bits)] = extended_info_bits\n",
        "        return codeword\n",
        "\n",
        "    def verify_codeword(self, codeword):\n",
        "        poly_info = self.crc_polynomials[self.crc_type]\n",
        "        crc_length = poly_info['length']\n",
        "        info_bits = codeword[:-crc_length]\n",
        "        received_crc = codeword[-crc_length:]\n",
        "        computed_crc = self.compute_crc(info_bits)\n",
        "        return np.array_equal(received_crc, computed_crc)\n",
        "\n",
        "\n",
        "class EnhancedChannelSimulator:\n",
        "    def __init__(self, channel_type='AWGN'):\n",
        "        self.channel_type = channel_type\n",
        "        logging.info(f\"Initializing {channel_type} Channel Simulator\")\n",
        "\n",
        "    def simulate(self, encoded_signal, snr_db):\n",
        "        try:\n",
        "            encoded_signal = np.array(encoded_signal, dtype=float)\n",
        "            bpsk_signal = 1 - 2 * encoded_signal\n",
        "            snr_linear = 10 ** (snr_db / 10)\n",
        "            signal_power = np.mean(bpsk_signal**2)\n",
        "            noise_power = signal_power / snr_linear\n",
        "            noise_std = np.sqrt(noise_power / 2.0)\n",
        "\n",
        "            if self.channel_type == 'AWGN':\n",
        "                noise = np.random.normal(0, noise_std, bpsk_signal.shape)\n",
        "                received_signal = bpsk_signal + noise\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            return received_signal\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Channel simulation error: {e}\")\n",
        "            return bpsk_signal\n",
        "\n",
        "    def compute_theoretical_performance(self, block_length, snr_linear):\n",
        "        try:\n",
        "            if self.channel_type == 'AWGN':\n",
        "                bep = 0.5 * sps.erfc(np.sqrt(snr_linear))\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported channel type: {self.channel_type}\")\n",
        "\n",
        "            bler = 1 - (1 - bep) ** block_length\n",
        "            return bep, bler\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Theoretical performance computation error: {e}\")\n",
        "            return np.zeros_like(snr_linear), np.ones_like(snr_linear)\n",
        "\n",
        "\n",
        "def prepare_polar_dataset(polar_code_gen, num_samples, snr_db=5, channel_type='AWGN'):\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_type)\n",
        "    X, y = [], []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        info_bits = polar_code_gen.generate_info_bits()\n",
        "        encoded_signal = polar_code_gen.polar_encode(info_bits)\n",
        "        received_signal = channel_simulator.simulate(encoded_signal, snr_db)\n",
        "        X.append(received_signal)\n",
        "        y.append(info_bits)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "class EnhancedRNNDecoder(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(EnhancedRNNDecoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, output_size),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        if x.dim() > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class DecoderTrainer:\n",
        "    def __init__(self, model, learning_rate=1e-3):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = model.to(self.device)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.model.parameters(), lr=learning_rate, weight_decay=1e-5\n",
        "        )\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train(self, X, y, epochs=100, batch_size=32, validation_split=0.2):\n",
        "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
        "        y_tensor = torch.FloatTensor(y).to(self.device)\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        train_size = int((1 - validation_split) * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "            dataset, [train_size, val_size]\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            train_loss = self._train_epoch(train_loader)\n",
        "            self.train_losses.append(train_loss)\n",
        "\n",
        "            self.model.eval()\n",
        "            val_loss = self._validate(val_loader)\n",
        "            self.val_losses.append(val_loss)\n",
        "\n",
        "            self.scheduler.step(val_loss)\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss:\"\n",
        "                f\" {val_loss:.4f}\"\n",
        "            )\n",
        "\n",
        "        return self.train_losses, self.val_losses\n",
        "\n",
        "    def _train_epoch(self, dataloader):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            batch_X = batch_X.to(self.device)\n",
        "            batch_y = batch_y.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(batch_X)\n",
        "            loss = self.criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dataloader:\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_y = batch_y.to(self.device)\n",
        "                outputs = self.model(batch_X)\n",
        "                loss = self.criterion(outputs, batch_y)\n",
        "                total_loss += loss.item()\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def predict(self, X):\n",
        "        if not isinstance(X, torch.Tensor):\n",
        "            X = torch.FloatTensor(X)\n",
        "        if X.dim() > 2:\n",
        "            X = X.view(X.size(0), -1)\n",
        "        X = X.to(self.device)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X)\n",
        "        return (outputs > 0.5).cpu().numpy().astype(int)\n",
        "\n",
        "\n",
        "class SCLDecoder:\n",
        "    def __init__(self, N, K, list_size, crc_poly=None):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.list_size = list_size\n",
        "        self.crc_poly = crc_poly\n",
        "        self.paths = None\n",
        "        self.path_metrics = None\n",
        "        self.frozen_set = self._get_frozen_set()\n",
        "        self.info_set = sorted(\n",
        "            list(set(range(N)) - set(self.frozen_set))\n",
        "        )\n",
        "\n",
        "    def _get_frozen_set(self):\n",
        "        return set(range(self.K, self.N))\n",
        "\n",
        "    def _f(self, llr1, llr2):\n",
        "        return np.sign(llr1) * np.sign(llr2) * np.minimum(np.abs(llr1), np.abs(llr2))\n",
        "\n",
        "    def _g(self, llr1, llr2, bit):\n",
        "        return llr2 + (-1) ** bit * llr1\n",
        "\n",
        "    def _bit_combine(self, bit1, bit2):\n",
        "        return bit1 ^ bit2\n",
        "\n",
        "    def _calculate_path_metric(self, llr, decision):\n",
        "        if decision == 0:\n",
        "            return math.log(1 + math.exp(-llr))\n",
        "        else:\n",
        "            return math.log(1 + math.exp(llr))\n",
        "\n",
        "    def decode(self, received_signal, snr_db):\n",
        "        self.paths = [np.zeros(self.N, dtype=int) for _ in range(self.list_size)]\n",
        "        self.path_metrics = [0.0 for _ in range(self.list_size)]\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        llrs = 2 * np.array(received_signal) * snr_linear\n",
        "        current_llrs = [llrs.copy() for _ in range(self.list_size)]\n",
        "\n",
        "        for bit_index in range(self.N):\n",
        "            new_paths = []\n",
        "            new_path_metrics = []\n",
        "            new_llrs = []\n",
        "\n",
        "            for path_idx in range(len(self.paths)):\n",
        "                current_path = self.paths[path_idx]\n",
        "                current_metric = self.path_metrics[path_idx]\n",
        "                path_llrs = current_llrs[path_idx]\n",
        "\n",
        "                if bit_index in self.frozen_set:\n",
        "                    decision = 0\n",
        "                    new_path = current_path.copy()\n",
        "                    new_path[bit_index] = decision\n",
        "                    new_metric = current_metric + self._calculate_path_metric(\n",
        "                        path_llrs[bit_index], decision\n",
        "                    )\n",
        "                    new_llrs_for_path = path_llrs.copy()\n",
        "\n",
        "                    new_paths.append(new_path)\n",
        "                    new_path_metrics.append(new_metric)\n",
        "                    new_llrs.append(new_llrs_for_path)\n",
        "\n",
        "                else:\n",
        "                    for decision in [0, 1]:\n",
        "                        new_path = current_path.copy()\n",
        "                        new_path[bit_index] = decision\n",
        "                        new_metric = current_metric + self._calculate_path_metric(\n",
        "                            path_llrs[bit_index], decision\n",
        "                        )\n",
        "                        new_llrs_for_path = path_llrs.copy()\n",
        "\n",
        "                        new_paths.append(new_path)\n",
        "                        new_path_metrics.append(new_metric)\n",
        "                        new_llrs.append(new_llrs_for_path)\n",
        "\n",
        "            sorted_indices = np.argsort(new_path_metrics)\n",
        "            self.paths = [new_paths[i] for i in sorted_indices[: self.list_size]]\n",
        "            self.path_metrics = [\n",
        "                new_path_metrics[i] for i in sorted_indices[: self.list_size]\n",
        "            ]\n",
        "            current_llrs = [new_llrs[i] for i in sorted_indices[: self.list_size]]\n",
        "\n",
        "        best_path_index = -1\n",
        "        best_metric = float('inf')\n",
        "\n",
        "        for i in range(self.list_size):\n",
        "            candidate_codeword = self.paths[i]\n",
        "            candidate_info_bits = candidate_codeword[self.info_set]\n",
        "\n",
        "            if self.crc_poly:\n",
        "                computed_crc = self._compute_crc_for_info_bits(candidate_info_bits)\n",
        "                received_crc = candidate_codeword[\n",
        "                    self.N - len(computed_crc) : self.N\n",
        "                ]\n",
        "\n",
        "                if np.array_equal(computed_crc, received_crc):\n",
        "                    if self.path_metrics[i] < best_metric:\n",
        "                        best_metric = self.path_metrics[i]\n",
        "                        best_path_index = i\n",
        "            else:\n",
        "                if self.path_metrics[i] < best_metric:\n",
        "                    best_metric = self.path_metrics[i]\n",
        "                    best_path_index = i\n",
        "\n",
        "        if best_path_index == -1:\n",
        "            best_path_index = np.argmin(self.path_metrics)\n",
        "\n",
        "        decoded_info_bits = self.paths[best_path_index][self.info_set]\n",
        "        return decoded_info_bits\n",
        "\n",
        "    def _compute_crc_for_info_bits(self, info_bits):\n",
        "        pass\n",
        "\n",
        "\n",
        "def run_scl_decoder(polar_code_gen, SNRS, list_size, channel_type, num_trials):\n",
        "    results = []\n",
        "    for snr_db in SNRS:\n",
        "        X, y = prepare_polar_dataset(\n",
        "            polar_code_gen,\n",
        "            num_samples=num_trials,\n",
        "            snr_db=snr_db,\n",
        "            channel_type=channel_type,\n",
        "        )\n",
        "\n",
        "        decoder = SCLDecoder(\n",
        "            N=polar_code_gen.N, K=polar_code_gen.K, list_size=list_size\n",
        "        )\n",
        "\n",
        "        decoded_bits = np.array([decoder.decode(x, snr_db) for x in X])\n",
        "\n",
        "        ber = np.sum(np.abs(decoded_bits - y)) / (num_trials * polar_code_gen.K)\n",
        "        bler = np.mean(np.any(decoded_bits != y, axis=1))\n",
        "\n",
        "        results.append({'SNR': snr_db, 'BER': ber, 'BLER': bler})\n",
        "        print(\n",
        "            f\"SCL Decoder - SNR: {snr_db:.1f} dB, List Size: {list_size}, BER:\"\n",
        "            f\" {ber:.4f}, BLER: {bler:.4f}\"\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def compare_decoders(polar_code_gen, trained_rnn_trainer):\n",
        "    results = {}\n",
        "    for channel in ['AWGN']:\n",
        "        snr_grid = SNR_AWGN\n",
        "        rnn_trainer = trained_rnn_trainer[channel]\n",
        "\n",
        "        ml_results = performance_comparison(\n",
        "            rnn_trainer, polar_code_gen, snr_grid, channel, LIST_SIZES, NUM_TRIALS_PERF\n",
        "        )\n",
        "\n",
        "        scl_results = {}\n",
        "        for L in LIST_SIZES:\n",
        "            scl_list = run_scl_decoder(\n",
        "                polar_code_gen, snr_grid, L, channel, NUM_TRIALS_PERF\n",
        "            )\n",
        "            scl_results[L] = {\n",
        "                'BER': [x['BER'] for x in scl_list],\n",
        "                'BLER': [x['BLER'] for x in scl_list],\n",
        "            }\n",
        "\n",
        "        results[channel] = {'ML': ml_results, 'SCL': scl_results}\n",
        "    return results\n",
        "\n",
        "\n",
        "def performance_comparison(\n",
        "    rnn_trainer, polar_code_gen, snr_range, channel_name, list_sizes, num_trials\n",
        "):\n",
        "    performance_results = {\n",
        "        list_size: {'BER': [], 'BLER': []} for list_size in list_sizes\n",
        "    }\n",
        "    channel_simulator = EnhancedChannelSimulator(channel_type=channel_name)\n",
        "\n",
        "    for list_size in list_sizes:\n",
        "        for snr_db in snr_range:\n",
        "            X, y = prepare_polar_dataset(\n",
        "                polar_code_gen,\n",
        "                num_samples=num_trials,\n",
        "                snr_db=snr_db,\n",
        "                channel_type=channel_name,\n",
        "            )\n",
        "\n",
        "            predictions = rnn_trainer.predict(X)\n",
        "\n",
        "            actual_labels = y\n",
        "            ber = np.sum(np.abs(predictions - actual_labels)) / (\n",
        "                num_trials * polar_code_gen.K\n",
        "            )\n",
        "            block_errors = np.sum(np.any(predictions != actual_labels, axis=1))\n",
        "            bler = block_errors / num_trials\n",
        "\n",
        "            performance_results[list_size]['BER'].append(ber)\n",
        "            performance_results[list_size]['BLER'].append(bler)\n",
        "            print(f\"List Size: {list_size}, SNR: {snr_db}, BER: {ber}, BLER: {bler}\")\n",
        "\n",
        "    return performance_results\n",
        "\n",
        "\n",
        "def plot_comprehensive_analysis(\n",
        "    train_losses, val_losses, performance_results, snr_range, channel_name\n",
        "):\n",
        "    plt.figure(figsize=(12, 15))\n",
        "\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title(f'{channel_name} Channel - Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(3, 1, 2)\n",
        "    for list_size, results in performance_results.items():\n",
        "        plt.plot(snr_range, results['BER'], label=f'RNN Decoder (List size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_name} Channel - BER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.subplot(3, 1, 3)\n",
        "    for list_size, results in performance_results.items():\n",
        "        plt.plot(snr_range, results['BLER'], label=f'RNN Decoder (List size {list_size})')\n",
        "\n",
        "    plt.title(f'{channel_name} Channel - BLER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(1e-4, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_results(results, channel_type, decoder_key, decoder_label):\n",
        "    snr_range = SNR_AWGN\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for list_size, perf_data in results[channel_type][decoder_key].items():\n",
        "        plt.plot(\n",
        "            snr_range, perf_data['BER'], label=f'{decoder_label} (List Size {list_size})'\n",
        "        )\n",
        "    plt.title(f'{channel_type} Channel - {decoder_label} BER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(10**-4, 10**0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for list_size, perf_data in results[channel_type][decoder_key].items():\n",
        "        plt.plot(\n",
        "            snr_range, perf_data['BLER'], label=f'{decoder_label} (List Size {list_size})'\n",
        "        )\n",
        "    plt.title(f'{channel_type} Channel - {decoder_label} BLER Performance')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.yscale('log')\n",
        "    plt.ylim(10**-4, 10**0)\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# Add this function within your existing code\n",
        "def plot_confusion_matrix(y_true, y_pred, channel_name, list_size=None):\n",
        "    \"\"\"\n",
        "    Plots the confusion matrix for the given true and predicted labels.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true.flatten(), y_pred.flatten())\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    disp.plot(ax=ax)\n",
        "    title = f'Confusion Matrix - {channel_name} Channel'\n",
        "    if list_size is not None:\n",
        "        title += f' (List Size {list_size})'\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Bit Value')\n",
        "    plt.ylabel('True Bit Value')\n",
        "    plt.grid(False) # Remove grid lines for better visualization\n",
        "    plt.show()\n",
        "##############################################################################################\n",
        "\n",
        "#latest main\n",
        "def main():\n",
        "    # Configuration Parameters\n",
        "    BLOCK_LENGTH = 128\n",
        "    INFO_BITS = 64\n",
        "    NUM_SAMPLES_TRAIN = 10000\n",
        "    NUM_TRIALS_PERF = 1000\n",
        "    EPOCHS = 50\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 1e-3\n",
        "    SNR_AWGN = np.linspace(0, 5, 11)\n",
        "    LIST_SIZES = [1, 8, 16]\n",
        "\n",
        "    polar_code_gen = PolarCodeGenerator(N=BLOCK_LENGTH, K=INFO_BITS)\n",
        "\n",
        "    # Training the ML (RNN) Decoder\n",
        "    trained_rnn_trainer = {}\n",
        "    for channel in ['AWGN']:\n",
        "        print(f\"\\nTraining RNN Decoder for {channel} channel...\")\n",
        "        X_train, y_train = prepare_polar_dataset(\n",
        "            polar_code_gen,\n",
        "            num_samples=NUM_SAMPLES_TRAIN,\n",
        "            snr_db=2,\n",
        "            channel_type=channel,\n",
        "        )  # Train at a moderate SNR\n",
        "        rnn_model = EnhancedRNNDecoder(\n",
        "            input_size=BLOCK_LENGTH, output_size=INFO_BITS\n",
        "        ).to(DEVICE)\n",
        "        rnn_trainer = DecoderTrainer(\n",
        "            model=rnn_model, learning_rate=LEARNING_RATE\n",
        "        )\n",
        "        train_losses, val_losses = rnn_trainer.train(\n",
        "            X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
        "        )\n",
        "        trained_rnn_trainer[channel] = rnn_trainer\n",
        "\n",
        "        # Plot training progress for the RNN decoder\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(train_losses, label='Train Loss')\n",
        "        plt.plot(val_losses, label='Validation Loss')\n",
        "        plt.title(f'RNN Decoder Training for {channel} Channel')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    # Compare Decoders (ML and SCL)\n",
        "    print(\"\\nComparing Decoders...\")\n",
        "    comparison_results = compare_decoders(polar_code_gen, trained_rnn_trainer)\n",
        "\n",
        "    # Plot comparison results\n",
        "    for channel in ['AWGN']:\n",
        "        print(f\"\\nPlotting results for {channel} channel...\")\n",
        "        plot_results(comparison_results, channel, 'ML', 'ML (RNN)')\n",
        "        plot_results(comparison_results, channel, 'SCL', 'SCL')\n",
        "\n",
        "    # --- Add Confusion Matrix Plot ---\n",
        "        # Get data for confusion matrix for the ML decoder at the highest SNR\n",
        "        highest_snr_db = SNR_AWGN[-1]\n",
        "        X_test, y_test = prepare_polar_dataset(\n",
        "            polar_code_gen,\n",
        "            num_samples=NUM_TRIALS_PERF,\n",
        "            snr_db=highest_snr_db,\n",
        "            channel_type=channel,\n",
        "        )\n",
        "        rnn_trainer = trained_rnn_trainer[channel]\n",
        "        predictions = rnn_trainer.predict(X_test)\n",
        "        actual_labels = y_test\n",
        "\n",
        "        # Plot confusion matrix for the ML decoder\n",
        "        print(f\"\\nPlotting Confusion Matrix for ML Decoder at {highest_snr_db:.1f} dB SNR...\")\n",
        "        plot_confusion_matrix(actual_labels, predictions, channel)\n",
        "        # -----------------------------------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "###########################################################################################\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cnwkOaLt24bZ",
        "outputId": "7c2280ad-4ac6-48b8-ff81-d589b53eefa1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.3.0\n",
            "Uninstalling pandas-2.3.0:\n",
            "  Successfully uninstalled pandas-2.3.0\n",
            "Found existing installation: scipy 1.12.0\n",
            "Uninstalling scipy-1.12.0:\n",
            "  Successfully uninstalled scipy-1.12.0\n",
            "Found existing installation: scikit-learn 1.4.2\n",
            "Uninstalling scikit-learn-1.4.2:\n",
            "  Successfully uninstalled scikit-learn-1.4.2\n",
            "Found existing installation: sklearn-pandas 2.2.0\n",
            "Uninstalling sklearn-pandas-2.2.0:\n",
            "  Successfully uninstalled sklearn-pandas-2.2.0\n",
            "Found existing installation: torch 2.7.1\n",
            "Uninstalling torch-2.7.1:\n",
            "  Successfully uninstalled torch-2.7.1\n",
            "Found existing installation: torchvision 0.22.1\n",
            "Uninstalling torchvision-0.22.1:\n",
            "  Successfully uninstalled torchvision-0.22.1\n",
            "Found existing installation: torchaudio 2.7.1\n",
            "Uninstalling torchaudio-2.7.1:\n",
            "  Successfully uninstalled torchaudio-2.7.1\n",
            "Found existing installation: fastai 2.8.2\n",
            "Uninstalling fastai-2.8.2:\n",
            "  Successfully uninstalled fastai-2.8.2\n",
            "Found existing installation: matplotlib 3.10.3\n",
            "Uninstalling matplotlib-3.10.3:\n",
            "  Successfully uninstalled matplotlib-3.10.3\n",
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Found existing installation: tensorflow_decision_forests 1.12.0\n",
            "Uninstalling tensorflow_decision_forests-1.12.0:\n",
            "  Successfully uninstalled tensorflow_decision_forests-1.12.0\n",
            "Found existing installation: tensorflow-text 2.19.0\n",
            "Uninstalling tensorflow-text-2.19.0:\n",
            "  Successfully uninstalled tensorflow-text-2.19.0\n",
            "Found existing installation: tf_keras 2.19.0\n",
            "Uninstalling tf_keras-2.19.0:\n",
            "  Successfully uninstalled tf_keras-2.19.0\n",
            "Found existing installation: numba 0.61.2\n",
            "Uninstalling numba-0.61.2:\n",
            "  Successfully uninstalled numba-0.61.2\n",
            "\u001b[33mWARNING: Skipping cuml-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dask-cuda as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping distributed-ucxx-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dask-cudf-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping raft-dask-cu12 as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ml_dtypes 0.5.1\n",
            "Uninstalling ml_dtypes-0.5.1:\n",
            "  Successfully uninstalled ml_dtypes-0.5.1\n",
            "Found existing installation: tensorboard 2.19.0\n",
            "Uninstalling tensorboard-2.19.0:\n",
            "  Successfully uninstalled tensorboard-2.19.0\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m285.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "missingno 0.5.2 requires matplotlib, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "pytensor 2.31.3 requires scipy<2,>=1, which is not installed.\n",
            "datascience 0.17.6 requires matplotlib>=3.0.0, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "music21 9.3.0 requires matplotlib, which is not installed.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "arviz 0.21.0 requires matplotlib>=3.5, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "geemap 0.35.3 requires matplotlib, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "wordcloud 1.9.4 requires matplotlib, which is not installed.\n",
            "jaxlib 0.5.1 requires ml-dtypes>=0.2.0, which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "pymc 5.23.0 requires pandas>=0.24.0, which is not installed.\n",
            "pymc 5.23.0 requires scipy>=1.4.1, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "yellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "osqp 1.0.4 requires scipy>=0.13.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, which is not installed.\n",
            "xarray-einstats 0.9.0 requires scipy>=1.11, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires matplotlib, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scipy, which is not installed.\n",
            "sentence-transformers 4.1.0 requires torch>=1.11.0, which is not installed.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, which is not installed.\n",
            "keras-hub 0.18.1 requires tensorflow-text; platform_system != \"Darwin\", which is not installed.\n",
            "gradio 5.31.0 requires pandas<3.0,>=1.0, which is not installed.\n",
            "holoviews 1.20.2 requires pandas>=1.3, which is not installed.\n",
            "pandas-gbq 0.29.0 requires pandas>=1.1.4, which is not installed.\n",
            "tensorflow-hub 0.16.1 requires tf-keras>=2.14.1, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "peft 0.15.2 requires torch>=1.13.0, which is not installed.\n",
            "seaborn 0.13.2 requires matplotlib!=3.6.1,>=3.4, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "mlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "clarabel 0.11.0 requires scipy, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "datasets 2.14.4 requires pandas, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "bigframes 2.5.0 requires matplotlib>=3.7.1, which is not installed.\n",
            "bigframes 2.5.0 requires pandas>=1.5.3, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "accelerate 1.7.0 requires torch>=2.0.0, which is not installed.\n",
            "albumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\n",
            "yfinance 0.2.61 requires pandas>=1.3.0, which is not installed.\n",
            "keras 3.8.0 requires ml-dtypes, which is not installed.\n",
            "xarray 2025.3.1 requires pandas>=2.1, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "prophet 1.1.7 requires matplotlib>=2.0.0, which is not installed.\n",
            "prophet 1.1.7 requires pandas>=1.0.4, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "panel 1.7.1 requires pandas>=1.2, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting scipy==1.12.0\n",
            "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.29.0,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy==1.12.0) (1.26.4)\n",
            "Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m307.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "missingno 0.5.2 requires matplotlib, which is not installed.\n",
            "datascience 0.17.6 requires matplotlib>=3.0.0, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, which is not installed.\n",
            "arviz 0.21.0 requires matplotlib>=3.5, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "jaxlib 0.5.1 requires ml-dtypes>=0.2.0, which is not installed.\n",
            "pymc 5.23.0 requires pandas>=0.24.0, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "yellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tf-keras>=2.18.0, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires matplotlib, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 4.1.0 requires torch>=1.11.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "mlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "stumpy 1.13.0 requires numba>=0.57.1, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.12.0\n",
            "Collecting scikit-learn<1.5.0,>=1.4.0\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.5.0,>=1.4.0) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.5.0,>=1.4.0) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.5.0,>=1.4.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.5.0,>=1.4.0) (3.6.0)\n",
            "Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m247.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "librosa 0.11.0 requires numba>=0.51.0, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "yellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\n",
            "shap 0.47.2 requires numba>=0.54, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "pynndescent 0.5.13 requires numba>=0.51.2, which is not installed.\n",
            "sentence-transformers 4.1.0 requires torch>=1.11.0, which is not installed.\n",
            "mlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "umap-learn 0.5.7 requires numba>=0.51.2, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.4.2\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn-pandas\n",
            "  Downloading sklearn_pandas-2.2.0-py2.py3-none-any.whl.metadata (445 bytes)\n",
            "Collecting torch\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting fastai\n",
            "  Downloading fastai-2.8.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-decision-forests\n",
            "  Downloading tensorflow_decision_forests-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting tf-keras\n",
            "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting numba\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-pandas) (1.4.2)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.11/dist-packages (from sklearn-pandas) (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastai) (24.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.9,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from fastai) (1.8.2)\n",
            "Requirement already satisfied: fasttransform>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from fastai) (0.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fastai) (2.32.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from fastai) (6.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.11/dist-packages (from fastai) (3.8.7)\n",
            "Requirement already satisfied: plum-dispatch in /usr/local/lib/python3.11/dist-packages (from fastai) (2.5.7)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from fastai) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests) (0.12.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->fastai) (2025.4.26)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.0->sklearn-pandas) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.23.0->sklearn-pandas) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (4.67.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (2.11.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: beartype>=0.16.2 in /usr/local/lib/python3.11/dist-packages (from plum-dispatch->fastai) (0.21.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai) (7.1.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m263.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_pandas-2.2.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m323.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m324.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m261.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastai-2.8.2-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m321.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m329.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m315.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_decision_forests-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m204.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m336.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m360.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m335.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m344.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m342.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numba, ml-dtypes, tensorboard, pandas, matplotlib, torch, sklearn-pandas, torchvision, torchaudio, tensorflow, tf-keras, tensorflow-text, tensorflow-decision-forests, fastai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastai-2.8.2 matplotlib-3.10.3 ml-dtypes-0.5.1 numba-0.61.2 pandas-2.3.0 sklearn-pandas-2.2.0 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow-decision-forests-1.12.0 tensorflow-text-2.19.0 tf-keras-2.19.0 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a8a54d7a3450>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_api_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marray_api_compat_numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'array_namespace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'as_xparray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
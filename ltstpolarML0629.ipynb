{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumuds4/BCH/blob/master/ltstpolarML0629.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KG2tWIXLAsbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to faster GPUs and more memory. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to select from several accelerator options, subject to availability.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator to the desired option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available. More powerful GPUs are always offered with high-memory VMs.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape toggle button. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colab](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TPUs in Colab](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colab makes possible, check out these tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ===========================\n",
        "# Configuration Parameters\n",
        "# ===========================\n",
        "N = 128                # Block length\n",
        "K = 64                 # Number of information bits\n",
        "CRC_LEN = 8            # CRC length (bits)\n",
        "LIST_SIZE = [1, 4, 8, 16]  # SCL list sizes\n",
        "NUM_FRAMES_PER_SNR = 1000\n",
        "SNR_RANGE_DB = np.arange(0.5, 4.5, 0.5)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-3\n",
        "# Add these if used by your RNN decoder (if not, remove or adjust)\n",
        "RNN_HIDDEN_SIZE = 128\n",
        "RNN_NUM_LAYERS = 2\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ===========================\n",
        "# CRC Class (CRC-8)\n",
        "# ===========================\n",
        "class CRC:\n",
        "    def __init__(self, poly=0x07, length=8):\n",
        "        self.poly = poly\n",
        "        self.length = length\n",
        "        self.poly_mask = (1 << length)\n",
        "\n",
        "    def compute(self, bits):\n",
        "        reg = 0\n",
        "        for bit in bits:\n",
        "            reg = ((reg << 1) | bit) & ((1 << (self.length + 1)) - 1)\n",
        "            if reg & self.poly_mask:\n",
        "                reg ^= self.poly << (self.length - 1)\n",
        "        crc = reg & ((1 << self.length) - 1)\n",
        "        return crc\n",
        "\n",
        "    def check(self, bits):\n",
        "        # bits includes info + crc\n",
        "        crc_calc = self.compute(bits[:-self.length])\n",
        "        crc_received = 0\n",
        "        for i in range(self.length):\n",
        "            crc_received |= bits[-self.length + i] << (self.length - 1 - i)\n",
        "        return crc_calc == crc_received\n",
        "\n",
        "    def append_crc(self, bits):\n",
        "        crc = self.compute(bits)\n",
        "        crc_bits = [(crc >> (self.length - 1 - i)) & 1 for i in range(self.length)]\n",
        "        return np.concatenate([bits, crc_bits])\n",
        "\n",
        "# ===========================\n",
        "# Polar Code Generator Class\n",
        "# ===========================\n",
        "class PolarCodeGenerator:\n",
        "    def __init__(self, N, K, crc=None, design_snr_db=0.5):\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.crc = crc\n",
        "        self.design_snr_db = design_snr_db\n",
        "\n",
        "        self.K_total = self.K + (self.crc.length if self.crc else 0)\n",
        "\n",
        "        # Arikan reliability sequence for N=128\n",
        "        ARIKAN_RELIABILITY_128 = np.array([\n",
        "             0,  1,  2,  4,  8, 16, 32, 64,\n",
        "             3,  5,  6,  9, 10, 12, 17, 18,\n",
        "            20, 24, 33, 34, 36, 40, 48, 65,\n",
        "            66,  7, 11, 13, 14, 19, 21, 22,\n",
        "            25, 26, 28, 35, 37, 38, 41, 42,\n",
        "            44, 49, 50, 52, 67, 68, 69, 70,\n",
        "            15, 23, 27, 29, 30, 39, 43, 45,\n",
        "            46, 47, 51, 53, 54, 56, 71, 72,\n",
        "            73, 74, 75, 76, 77, 78, 79, 80,\n",
        "            31, 55, 57, 58, 59, 60, 61, 62,\n",
        "            63, 81, 82, 83, 84, 85, 86, 87,\n",
        "            88, 89, 90, 91, 92, 93, 94, 95,\n",
        "            96, 97, 98, 99,100,101,102,103,\n",
        "           104,105,106,107,108,109,110,111,\n",
        "           112,113,114,115,116,117,118,119,\n",
        "           120,121,122,123,124,125,126,127\n",
        "        ])\n",
        "\n",
        "        self.frozen_bits = np.ones(self.N, dtype=bool)\n",
        "        self.information_bits_positions = np.sort(ARIKAN_RELIABILITY_128[:self.K_total])\n",
        "        self.frozen_bits[self.information_bits_positions] = False\n",
        "        self.R = self.K / self.N\n",
        "\n",
        "\n",
        "    def encode(self, info_bits):\n",
        "        if self.crc is not None:\n",
        "            info_bits = self.crc.append_crc(info_bits)\n",
        "        u = np.zeros(self.N, dtype=int)\n",
        "        u[self.information_bits_positions] = info_bits[:self.K_total]\n",
        "        x = self._polar_transform(u)\n",
        "        return x\n",
        "\n",
        "    def _polar_transform(self, u):\n",
        "        N = len(u)\n",
        "        n = int(np.log2(N))\n",
        "        x = u.copy()\n",
        "        for i in range(n):\n",
        "            step = 2 ** i\n",
        "            for j in range(0, N, 2 * step):\n",
        "                for k in range(step):\n",
        "                    x[j + k] ^= x[j + k + step]\n",
        "        return x\n",
        "\n",
        "    def decode(self, llr, decoder_type='SC', list_size=1):\n",
        "        if decoder_type == 'SC':\n",
        "            decoded = self.sc_decode(llr)\n",
        "        elif decoder_type == 'SCL':\n",
        "            scl_decoder = SCLDecoder(self.N, self.K_total, self.frozen_bits, list_size)\n",
        "            decoded = scl_decoder.decode(llr)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported decoder type: {decoder_type}\")\n",
        "\n",
        "        if self.crc:\n",
        "            decoded_info = decoded[self.information_bits_positions[:self.K + self.crc.length]]\n",
        "        else:\n",
        "            decoded_info = decoded[self.information_bits_positions[:self.K]]\n",
        "\n",
        "        return decoded, decoded_info\n",
        "\n",
        "    def sc_decode(self, llr):\n",
        "        N = self.N\n",
        "        u_hat = np.zeros(N, dtype=int)\n",
        "\n",
        "        def recursive_sc_decode(llr_sub, start_idx):\n",
        "            n = len(llr_sub)\n",
        "            if n == 1:\n",
        "                bit_idx = start_idx\n",
        "                if self.frozen_bits[bit_idx]:\n",
        "                    return np.array([0], dtype=int)\n",
        "                else:\n",
        "                    return np.array([0 if llr_sub[0] >= 0 else 1], dtype=int)\n",
        "\n",
        "            llr_left = np.sign(llr_sub[:n//2]) * np.minimum(np.abs(llr_sub[:n//2]), np.abs(llr_sub[n//2:]))\n",
        "\n",
        "            u_left = recursive_sc_decode(llr_left, start_idx)\n",
        "            llr_right = llr_sub[n//2:] + ((-1) ** u_left) * llr_sub[:n//2]\n",
        "\n",
        "            u_right = recursive_sc_decode(llr_right, start_idx + n//2)\n",
        "\n",
        "            return np.concatenate([u_left ^ u_right, u_right])\n",
        "\n",
        "        u_hat = recursive_sc_decode(llr, 0)\n",
        "        return u_hat\n",
        "\n",
        "# ===========================\n",
        "# Tal-Vardy SCL Decoder (list decoder)\n",
        "# ===========================\n",
        "class FastSCLDecoder:\n",
        "    def __init__(self, N, K_total, frozen_bits, list_size=8):\n",
        "        self.N = N\n",
        "        self.K_total = K_total\n",
        "        self.frozen_bits = frozen_bits\n",
        "        self.list_size = list_size\n",
        "\n",
        "    def decode(self, llr):\n",
        "        N = self.N\n",
        "        L = self.list_size\n",
        "\n",
        "        paths = [([], 0.0)]  # (path_bits, path_metric)\n",
        "\n",
        "        for i in range(N):\n",
        "            new_paths = []\n",
        "            for path, metric in paths:\n",
        "                if self.frozen_bits[i]:\n",
        "                    # frozen bit = 0\n",
        "                    new_paths.append((path + [0], metric))\n",
        "                else:\n",
        "                    # branch for bit=0 and bit=1\n",
        "                    for bit in [0, 1]:\n",
        "                        # path metric update using log-domain approximation\n",
        "                        pm_increment = np.log1p(np.exp(-llr[i] * (1 - 2 * bit)))\n",
        "                        new_metric = metric + pm_increment\n",
        "                        new_paths.append((path + [bit], new_metric))\n",
        "\n",
        "            # prune to list_size best paths\n",
        "            new_paths = sorted(new_paths, key=lambda x: x[1])[:L]\n",
        "            paths = new_paths\n",
        "\n",
        "        best_path = paths[0][0]\n",
        "        return np.array(best_path, dtype=int)\n",
        "\n",
        "\n",
        "# ===========================\n",
        "#RNN\n",
        "# ===========================\n",
        "# Replace DummyRNNDecoder with actual RNNDecoder\n",
        "class NNNDecoder(nn.Module):\n",
        "    def __init__(self, seq_len, hidden_size=128, num_layers=3):\n",
        "        super(NNNDecoder, self).__init__()\n",
        "        self.gru = nn.GRU(input_size=1,\n",
        "                          hidden_size=hidden_size,\n",
        "                          num_layers=num_layers,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)            # out shape: (batch, seq_len, hidden_size)\n",
        "        out = self.fc(out)              # out shape: (batch, seq_len, 1)\n",
        "        out = self.sigmoid(out)         # outputs probabilities\n",
        "        return out.squeeze(2)           # shape: (batch, seq_len)\n",
        "\n",
        "# ===========================\n",
        "# Helper Functions\n",
        "# ===========================\n",
        "def bpsk_modulation(bits):\n",
        "    return 1 - 2 * bits  # 0 -> +1, 1 -> -1\n",
        "\n",
        "def calculate_llr(y, sigma):\n",
        "    return 2 * y / (sigma ** 2)\n",
        "\n",
        "def bit_errors(true_bits, decoded_bits, info_positions):\n",
        "    decoded_info_bits = decoded_bits[info_positions]\n",
        "    return np.sum(true_bits != decoded_info_bits[:len(true_bits)])\n",
        "\n",
        "def block_error(true_bits, decoded_bits, info_positions):\n",
        "    decoded_info_bits = decoded_bits[info_positions]\n",
        "    return int(np.any(true_bits != decoded_info_bits[:len(true_bits)]))\n",
        "\n",
        "def compute_mutual_information(llr, bits):\n",
        "    p = 1 / (1 + np.exp(-llr))\n",
        "    mi = np.mean(1 - (-bits*np.log2(p + 1e-15) - (1 - bits)*np.log2(1 - p + 1e-15)))\n",
        "    return mi\n",
        "\n",
        "# ===========================\n",
        "# RNN Training Function and decoder\n",
        "\n",
        "def train_nnn(model, polar, crc, device, epochs, batch_size, learning_rate, num_train_frames=10000):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.BCELoss()\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for _ in range(num_train_frames // batch_size):\n",
        "            inputs = []\n",
        "            targets = []\n",
        "\n",
        "            for _ in range(batch_size):\n",
        "                info_bits = np.random.randint(0, 2, polar.K)\n",
        "                full_bits = crc.append_crc(info_bits) if crc else info_bits\n",
        "                codeword = polar.encode(info_bits)\n",
        "                symbols = bpsk_modulation(codeword)\n",
        "\n",
        "                noise = np.random.randn(polar.N) * np.sqrt(1 / (2 * 10**(1.0 / 10)))  # fixed 1 dB noise\n",
        "                y = symbols + noise\n",
        "                llr = calculate_llr(y, np.sqrt(1 / (2 * 10**(1.0 / 10))))\n",
        "\n",
        "                inputs.append(llr)\n",
        "                targets.append(full_bits)\n",
        "\n",
        "            inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(2).to(device)  # (batch, N, 1)\n",
        "            targets = torch.tensor(targets, dtype=torch.float32).to(device)             # (batch, K+CRC)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs[:, :targets.shape[1]]  # truncate if needed\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = epoch_loss / (num_train_frames // batch_size)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation step (simple, same method)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_inputs = []\n",
        "            val_targets = []\n",
        "            for _ in range(batch_size):\n",
        "                info_bits = np.random.randint(0, 2, polar.K)\n",
        "                full_bits = crc.append_crc(info_bits) if crc else info_bits\n",
        "                codeword = polar.encode(info_bits)\n",
        "                symbols = bpsk_modulation(codeword)\n",
        "                noise = np.random.randn(polar.N) * np.sqrt(1 / (2 * 10**(1.0 / 10)))\n",
        "                y = symbols + noise\n",
        "                llr = calculate_llr(y, np.sqrt(1 / (2 * 10**(1.0 / 10))))\n",
        "                val_inputs.append(llr)\n",
        "                val_targets.append(full_bits)\n",
        "\n",
        "            val_inputs = torch.tensor(val_inputs, dtype=torch.float32).unsqueeze(2).to(device)\n",
        "            val_targets = torch.tensor(val_targets, dtype=torch.float32).to(device)\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_outputs = val_outputs[:, :val_targets.shape[1]]\n",
        "            val_loss = criterion(val_outputs, val_targets).item()\n",
        "\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "        model.train()\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def nnn_decode(model, llr, device, K):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        llr_tensor = torch.tensor(llr, dtype=torch.float32).unsqueeze(0).unsqueeze(2).to(device)\n",
        "        output = model(llr_tensor).squeeze(0).cpu().numpy()\n",
        "        decoded_bits = (output[:K] > 0.5).astype(int)  # threshold at 0.5\n",
        "    return decoded_bits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Plotting Function\n",
        "# ===========================\n",
        "def plot_results(SNR_RANGE_DB, ber_sc, bler_sc, ber_scl, bler_scl,\n",
        "                 ber_rnn, bler_rnn, train_losses, val_losses, mi_list, epochs):\n",
        "    # SC BER/BLER\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.semilogy(SNR_RANGE_DB, ber_sc, marker='o', label='SC')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate (BER)')\n",
        "    plt.title('SC Decoder BER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.semilogy(SNR_RANGE_DB, bler_sc, marker='o', label='SC')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate (BLER)')\n",
        "    plt.title('SC Decoder BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    # SCL BER/BLER\n",
        "    plt.subplot(2, 2, 3)\n",
        "    for L in ber_scl:\n",
        "        plt.semilogy(SNR_RANGE_DB, ber_scl[L], marker='o', label=f'SCL L={L}')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('SCL Decoder BER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    for L in bler_scl:\n",
        "        plt.semilogy(SNR_RANGE_DB, bler_scl[L], marker='o', label=f'SCL L={L}')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('SCL Decoder BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # RNN BER/BLER\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogy(SNR_RANGE_DB, ber_rnn, marker='o', color='purple', label='RNN')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BER')\n",
        "    plt.title('RNN Decoder BER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogy(SNR_RANGE_DB, bler_rnn, marker='o', color='purple', label='RNN')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('BLER')\n",
        "    plt.title('RNN Decoder BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Training and Validation Loss\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('RNN Training and Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Mutual Information\n",
        "    plt.figure()\n",
        "    plt.plot(SNR_RANGE_DB, mi_list, marker='o')\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Mutual Information (bits)')\n",
        "    plt.title('Mutual Information vs SNR')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    #Training, Validation plot\n",
        "\n",
        "    # After training RNN, plot training and validation loss\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('RNN Training and Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "# ===========================\n",
        "# Main Simulation Function\n",
        "\n",
        "###################################\n",
        "def main():\n",
        "    # Config params (keep as before)\n",
        "    N = 128\n",
        "    K = 64\n",
        "    CRC_LEN = 8\n",
        "    LIST_SIZE = [1, 4, 8, 16]\n",
        "    SNR_RANGE_DB = np.arange(0.5, 4.5, 0.5)\n",
        "    NUM_FRAMES_PER_SNR = 1000\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 1e-3\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    crc = CRC(poly=0x07, length=CRC_LEN)\n",
        "    polar = PolarCodeGenerator(N, K, crc=crc)\n",
        "\n",
        "    # Use your real NNNDecoder here\n",
        "    rnn_decoder = NNNDecoder(seq_len=N, hidden_size=128, num_layers=3).to(device)\n",
        "\n",
        "    print(\"Training RNN decoder...\")\n",
        "    train_losses, val_losses = train_rnn(\n",
        "        model=rnn_decoder,\n",
        "        polar=polar,\n",
        "        crc=crc,\n",
        "        device=device,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        num_train_frames=5000\n",
        "    )\n",
        "\n",
        "    ber_sc, bler_sc = [], []\n",
        "    ber_rnn, bler_rnn = [], []\n",
        "    ber_scl = {L: [] for L in LIST_SIZE}\n",
        "    bler_scl = {L: [] for L in LIST_SIZE}\n",
        "    mi_list = []\n",
        "\n",
        "    for snr_db in SNR_RANGE_DB:\n",
        "        print(f\"Simulating at SNR = {snr_db} dB\")\n",
        "        snr_linear = 10 ** (snr_db / 10)\n",
        "        noise_variance = 1 / (2 * snr_linear)\n",
        "\n",
        "        bit_err_sc = block_err_sc = 0\n",
        "        bit_err_rnn = block_err_rnn = 0\n",
        "        bit_err_scl = {L: 0 for L in LIST_SIZE}\n",
        "        block_err_scl = {L: 0 for L in LIST_SIZE}\n",
        "\n",
        "        mi_total = 0\n",
        "\n",
        "        for frame_idx in range(NUM_FRAMES_PER_SNR):\n",
        "            info_bits = np.random.randint(0, 2, K)\n",
        "            encoded_bits = polar.encode(info_bits)\n",
        "            tx = 1 - 2 * encoded_bits  # BPSK modulation\n",
        "            noise = np.random.randn(N) * np.sqrt(noise_variance)\n",
        "            y = tx + noise\n",
        "            llr = 2 * y / noise_variance\n",
        "\n",
        "            decoded_sc, _ = polar.decode(llr, decoder_type='SC')\n",
        "            bit_err_sc += bit_errors(info_bits, decoded_sc, polar.information_bits_positions)\n",
        "            block_err_sc += block_error(info_bits, decoded_sc, polar.information_bits_positions)\n",
        "\n",
        "            for L in LIST_SIZE:\n",
        "                decoded_scl, _ = polar.decode(llr, decoder_type='SCL', list_size=L)\n",
        "                bit_err_scl[L] += bit_errors(info_bits, decoded_scl, polar.information_bits_positions)\n",
        "                block_err_scl[L] += block_error(info_bits, decoded_scl, polar.information_bits_positions)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                llr_tensor = torch.tensor(llr, dtype=torch.float32).unsqueeze(0).unsqueeze(2).to(device)  # (1, N, 1)\n",
        "                rnn_output = rnn_decoder(llr_tensor).cpu().numpy().squeeze(0)  # (N,)\n",
        "                decoded_rnn = (rnn_output[:K] > 0.5).astype(int)\n",
        "\n",
        "            bit_err_rnn += np.sum(info_bits != decoded_rnn)\n",
        "            block_err_rnn += int(np.any(info_bits != decoded_rnn))\n",
        "\n",
        "            mi_total += compute_mutual_information(llr, encoded_bits)\n",
        "\n",
        "        ber_sc.append(bit_err_sc / (NUM_FRAMES_PER_SNR * K))\n",
        "        bler_sc.append(block_err_sc / NUM_FRAMES_PER_SNR)\n",
        "\n",
        "        for L in LIST_SIZE:\n",
        "            ber_scl[L].append(bit_err_scl[L] / (NUM_FRAMES_PER_SNR * K))\n",
        "            bler_scl[L].append(block_err_scl[L] / NUM_FRAMES_PER_SNR)\n",
        "\n",
        "        ber_rnn.append(bit_err_rnn / (NUM_FRAMES_PER_SNR * K))\n",
        "        bler_rnn.append(block_err_rnn / NUM_FRAMES_PER_SNR)\n",
        "\n",
        "        mi_list.append(mi_total / NUM_FRAMES_PER_SNR)\n",
        "\n",
        "    # (Your plotting code remains the same here...)\n",
        "\n",
        "    # ...\n",
        "\n",
        "    # --- Plot SC decoder results ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogy(SNR_RANGE_DB, ber_sc, marker='o', label='SC')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate (BER)')\n",
        "    plt.title('SC Decoder BER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogy(SNR_RANGE_DB, bler_sc, marker='o', label='SC')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate (BLER)')\n",
        "    plt.title('SC Decoder BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot SCL decoder results ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for L in LIST_SIZE:\n",
        "        plt.semilogy(SNR_RANGE_DB, ber_scl[L], marker='o', label=f'SCL L={L}')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate (BER)')\n",
        "    plt.title('SCL Decoder BER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for L in LIST_SIZE:\n",
        "        plt.semilogy(SNR_RANGE_DB, bler_scl[L], marker='o', label=f'SCL L={L}')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate (BLER)')\n",
        "    plt.title('SCL Decoder BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot RNN decoder results ---\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.semilogy(SNR_RANGE_DB, ber_rnn, marker='o', color='purple', label='RNN')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Bit Error Rate (BER)')\n",
        "    plt.title('RNN Decoder BER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.semilogy(SNR_RANGE_DB, bler_rnn, marker='o', color='purple', label='RNN')\n",
        "    plt.ylim(1e-5, 1)\n",
        "    plt.xlabel('SNR (dB)')\n",
        "    plt.ylabel('Block Error Rate (BLER)')\n",
        "    plt.title('RNN Decoder BLER vs SNR')\n",
        "    plt.grid(True, which='both')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot Training and Validation Loss ---\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('RNN Training and Validation Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Plot Mutual Information ---\n",
        "    if len(mi_list) == len(SNR_RANGE_DB):\n",
        "        plt.figure()\n",
        "        plt.plot(SNR_RANGE_DB, mi_list, marker='o')\n",
        "        plt.xlabel('SNR (dB)')\n",
        "        plt.ylabel('Mutual Information (bits)')\n",
        "        plt.title('Mutual Information vs SNR')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vwGt0OgMSPT",
        "outputId": "b9986dd8-9c25-4892-d783-86473e3d5964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN decoder...\n",
            "Epoch 1/10 - Train Loss: 0.6922 | Val Loss: 0.6926\n",
            "Epoch 2/10 - Train Loss: 0.6920 | Val Loss: 0.6928\n",
            "Epoch 3/10 - Train Loss: 0.6919 | Val Loss: 0.6912\n",
            "Epoch 4/10 - Train Loss: 0.6917 | Val Loss: 0.6913\n",
            "Epoch 5/10 - Train Loss: 0.6911 | Val Loss: 0.6920\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}